<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>How to research</title>
    <link href="/2024/06/21/4_how_to_research/"/>
    <url>/2024/06/21/4_how_to_research/</url>
    
    <content type="html"><![CDATA[<p>Re + Search</p><span id="more"></span><p>假设我们的课题名为”ProjectName”</p><h2 id="1-1-检索式构建体系"><a href="#1-1-检索式构建体系" class="headerlink" title="1.1 检索式构建体系"></a>1.1 检索式构建体系</h2><p>首先百度搜”ProjectName”的概念，了解相关涉及到的外文表达。第一次检索的原则大一些，把关键词尽可能都囊括进来</p><h2 id="How-to-read-a-paper-http-nob-cs-ucdavis-edu-classes-ecs289m-2022-12-lectures-lec02-03-keyshav-pdf-http-nob-cs-ucdavis-edu-classes-ecs289m-2022-12-lectures-lec02-03-keyshav-pdf-text-Go-through-the-whole-paper-more-thoroughly-•-references-you-encounter-at-points-that-interest-you"><a href="#How-to-read-a-paper-http-nob-cs-ucdavis-edu-classes-ecs289m-2022-12-lectures-lec02-03-keyshav-pdf-http-nob-cs-ucdavis-edu-classes-ecs289m-2022-12-lectures-lec02-03-keyshav-pdf-text-Go-through-the-whole-paper-more-thoroughly-•-references-you-encounter-at-points-that-interest-you" class="headerlink" title="[How to read a paper]([http://nob.cs.ucdavis.edu/classes/ecs289m-2022-12/lectures/lec02/03-keyshav.pdf](http://nob.cs.ucdavis.edu/classes/ecs289m-2022-12/lectures/lec02/03-keyshav.pdf#:~:text=Go through the whole paper more thoroughly •,references you encounter at points that interest you))"></a>[How to read a paper]([<a href="http://nob.cs.ucdavis.edu/classes/ecs289m-2022-12/lectures/lec02/03-keyshav.pdf]">http://nob.cs.ucdavis.edu/classes/ecs289m-2022-12/lectures/lec02/03-keyshav.pdf]</a>(<a href="http://nob.cs.ucdavis.edu/classes/ecs289m-2022-12/lectures/lec02/03-keyshav.pdf#:~:text=Go">http://nob.cs.ucdavis.edu/classes/ecs289m-2022-12/lectures/lec02/03-keyshav.pdf#:~:text=Go</a> through the whole paper more thoroughly •,references you encounter at points that interest you))</h2><p><strong>第一遍</strong>：快速浏览论文，对论文的内容形成整体感知（10~20min）</p><pre><code class="hljs">- title、abstract、introduction- 阅读章节标题和章节子标题- 浏览数学公式，不用看数学证明- conclusions and contribution- 浏览一下参考文献，顺手剔除掉已经阅读过的文献，可以勾勾画画，也可以记下来</code></pre><p><strong>第二步</strong>：（如果对这篇论文感兴趣）细读论文，总结文章的主要内容(1小时)</p><pre><code class="hljs">- 在论文空白处，写下你自己认为的关键点- 把读不懂的术语，或者你的其他疑问，记在笔记本上，以备自己以后复习- 认真看结果图(figures)、流程图(diagrams)、表格(tables)，这些图表都很直观，你要看看有没有明显的错误</code></pre><p><strong>第三遍</strong>：尝试在脑子里复现或者直接动手复现这篇论文</p>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tutorial</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Image Warping</title>
    <link href="/2024/06/16/8_ImageWarping/"/>
    <url>/2024/06/16/8_ImageWarping/</url>
    
    <content type="html"><![CDATA[<p>图像扭曲是计算机视觉和图像处理中的一个重要概念，它涉及到改变图像的几何形状或位置，使其看起来像是被拉伸、压缩或旋转。</p><span id="more"></span><p>Reference materials include:</p><ul><li><input checked="" disabled="" type="checkbox"> <a href="https://www.cs.princeton.edu/courses/archive/fall00/cs426/lectures/warp/warp.pdf">C0S 426, Fall 2000</a></li><li><input checked="" disabled="" type="checkbox"> <a href="https://www.cs.princeton.edu/courses/archive/spring22/cos426/lectures/Lecture-2.pdf">C0S 426, Spring 2022</a></li><li><input checked="" disabled="" type="checkbox"> <a href="https://legacy.imagemagick.org/Usage/filter/">Resampling filters</a></li></ul><h1 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h1><p>图像预处理操作包括五大类内容。</p><ul><li>Quantization: 量化是将连续的像素值（或颜色值）转换为离散的级别。这个过程在图像压缩和图像格式转换中非常常见。<ul><li>Uniform Quantization </li><li>Random dither </li><li>Ordered dither </li><li>Floyd-Steinberg dither</li></ul></li><li>Pixel operations: 像素操作是对图像中的每个像素值进行独立处理，通常不考虑像素之间的关系。 <ul><li>Add random noise </li><li>Add luminance </li><li>Add contrast </li><li>Add saturation</li></ul></li><li>Filtering: 滤波是通过卷积操作处理图像，考虑一个像素及其邻域像素的关系，用于平滑图像或增强特定特征。 <ul><li>Blur </li><li>Detect edges</li></ul></li><li>Warping: 图像扭曲涉及对图像进行几何变换，改变其形状或位置。 <ul><li>Scale </li><li>Rotate </li><li><strong>Warp</strong></li></ul></li><li>Combining: 组合是将多幅图像或图像的不同部分合成一幅新的图像。 <ul><li>Composite </li><li>Morph<br>总之，图像预处理的目标是通过一系列的操作来改善图像质量、增强特定特征、减少噪声或校正几何畸变等，为后续的高级图像处理和计算机视觉任务奠定基础。这些技术在图像预处理阶段共同作用，确保输入到后续步骤的图像数据是最优化的。</li></ul></li></ul><p>而在图像处理过程中，空间图像的几何变换包括两个主要步骤：映射（mapping）和重采样（resampling）。这两个步骤是实现图像扭曲、变换或几何校正的关键。</p><ul><li>Move pixels of image <ul><li>Mapping ：空间坐标变换关系<ul><li>Forward </li><li>Inverse</li></ul></li><li>Resampling ：变换坐标的赋值、插值运算<ul><li>Point sampling </li><li>Triangle filter </li><li>Gaussian filter</li></ul></li></ul></li></ul><h1 id="Mapping"><a href="#Mapping" class="headerlink" title="Mapping"></a>Mapping</h1><p><strong>映射是确定图像中每个像素在变换后的新位置。</strong>它定义了从原始图像到目标图像的坐标变换关系。这个过程主要关注的是计算像素如何从原始位置移动到新的位置。</p><h2 id="Define-transformation"><a href="#Define-transformation" class="headerlink" title="Define transformation"></a>Define transformation</h2><p>Describe the destination (x,y) for every location (u,v) in the source (or vice-versa, if invertible).</p><h2 id="Example-Mappings"><a href="#Example-Mappings" class="headerlink" title="Example Mappings"></a>Example Mappings</h2><p>变换函数可以是仿射变换、透视变换或其他非线性变换。</p><ul><li><p>仿射变换（Affine Transformation）：是一类<strong>保持点、直线和线段平行性的变换</strong>。它包括：</p><ul><li>线性变换（如旋转-Rotate、缩放-Scale、剪切-Shear）</li><li>平移-Translation</li></ul></li><li><p>透视变换 （Perspective Transformation）：是用于模拟三维视角效果、图像校正和增强现实的变换。<strong>透视变换是一种保留直线性质的变换。</strong>它能够将直线投影成直线，但不保持平行性，适合处理远近关系。</p></li><li><p>将三维空间中的点投影到二维平面上</p></li><li><p>逆向透视变换 (Inverse Perspective Mapping)：将二维图像中的点映射回三维空间。常用于图像校正、如车道线检测、摄像头标定等</p></li><li><p>非线性变换 （Non-linear Transformation）：非线性变换通常没有固定的公式，可以是任意函数。</p></li><li><p>径向变换（Radial Transformation）：用于校正镜头的径向畸变，如桶形畸变和枕形畸变。</p></li><li><p>极坐标变换：常用于全景图像处理和纹理映射。</p></li><li><p>涡旋变换-Swirl：将图像进行旋转扭曲，常用于图像特效。</p></li><li><p>波浪变换-Wave Transformation：用于模拟波浪效果。</p></li><li><p>Rain：雨效应是一种通过在图像上叠加模拟雨滴的效果来生成下雨场景的技术</p></li><li><p>弹性变形（Elastic Deformation）：弹性变形常用于数据增强。通过在图像上应用随机的、局部的位移场来实现，这些位移场通常由高斯滤波器平滑处理后生成，从而生成具有更大多样性的数据集。</p></li><li><p><strong>双曲线变换（Hyperbolic Transformation）</strong>：用于创建各种扭曲和形变效果</p></li><li><p><strong>Fish-eye</strong>：模拟鱼眼镜头效果的非线性变换，使得图像中心区域被放大，而边缘区域被压缩，产生一种球形的视觉效果，属于桶形畸变。</p></li><li><p><strong>光流变换</strong>：用于追踪图像序列中的运动，用于视频处理和运动分析</p></li><li><p><strong>图像网格变形（Mesh Warp Transformation）</strong>：通过在图像上定义控制点和网格，调整控制点来实现变形</p></li><li><p><strong>Thin Plate Spline</strong>：平滑变形，特别是在图像配准和形状匹配中</p></li></ul><h2 id="Image-Warping-Implementation"><a href="#Image-Warping-Implementation" class="headerlink" title="Image Warping Implementation"></a>Image Warping Implementation</h2><p>逆向映射（inverse mapping）通常比前向映射（forward mapping）更有效，主要原因有以下几点：</p><ol><li>forward mapping从源图像空间出发，逐个像素地将其映射到目标图像中。这个过程中，可能会出现一些目标图像像素没有被任何源图像像素映射到，导致空洞问题，需要额外的插值或其他处理方法来填补这些空洞。</li><li>inverse mapping从目标图像空间出发，逐个像素地寻找其在源图像中的对应位置。这样可以确保目标图像的每个像素都被处理到，避免出现空洞（holes）或未填充的像素。</li></ol><p><img src="https://raw.githubusercontent.com/vitalemonate/Image-Warping/main/results/forward_warp_vs_inverse_warp.png" alt="Compare mapping"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Forward mapping</span><br><span class="hljs-comment">## Iterate over source image</span><br><span class="hljs-comment">### Many source pixels can map to same destination pixel</span><br><span class="hljs-comment">### Some destination pixels may not be covered</span><br><br><span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> u = <span class="hljs-number">0</span>; u &lt; umax; u++) &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> v = <span class="hljs-number">0</span>; v &lt; vmax; v++) &#123;<br>        <span class="hljs-built_in">float</span> x = fx(u,v);<br>        <span class="hljs-built_in">float</span> y = fy(u,v);<br>        dst(x,y) = src(u,v);<br>        &#125;<br>    &#125;<br><br><span class="hljs-comment"># Reverse mapping</span><br><span class="hljs-comment">## Iterate over destination image </span><br><span class="hljs-comment">### Must resample source </span><br><span class="hljs-comment">### May oversample, but much simpler!</span><br><span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> x = <span class="hljs-number">0</span>; x &lt; xmax; x++) &#123;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> y = <span class="hljs-number">0</span>; y &lt; ymax; y++) &#123;<br>        <span class="hljs-built_in">float</span> u = fx^-<span class="hljs-number">1</span>(x,y);<br>        <span class="hljs-built_in">float</span> v = fy^-<span class="hljs-number">1</span>(x,y);<br>        dst(x,y) = src(u,v);<br>        &#125;<br>    &#125;<br></code></pre></td></tr></table></figure><h1 id="Resampling"><a href="#Resampling" class="headerlink" title="Resampling"></a>Resampling</h1><p>Evaluate source image at arbitrary (u,v), (u,v) does not usually have integer coordinates.&#x3D;》<strong>重采样是将映射得到的新位置的像素值进行填充。</strong>由于映射后的坐标（x’, y’）通常不是整数坐标，因此需要对这些坐标进行插值以获得最终的像素值。</p><h2 id="Resampling-Artefacts"><a href="#Resampling-Artefacts" class="headerlink" title="Resampling Artefacts"></a>Resampling Artefacts</h2><p><img src="https://legacy.imagemagick.org/Usage/filter/storm_resized.gif" alt="Blocking"><img src="https://legacy.imagemagick.org/Usage/filter/gray_edge_ringing.gif" alt="Ringing_1"><img src="https://legacy.imagemagick.org/Usage/filter/rings_resize.png" alt="Aliasing"></p><ol><li><h3 id="Blocking"><a href="#Blocking" class="headerlink" title="Blocking"></a>Blocking</h3><p>The primary cause of ‘blocking’ is either badly anti-aliased source image, or not enough smoothing (color mixing or blurring) between pixels to improve the overall look of an image.</p></li><li><h3 id="Ringing"><a href="#Ringing" class="headerlink" title="Ringing"></a>Ringing</h3><p>Ringing is an effect you often see in very low quality JPEG images close to sharp edges. It is typically caused by an edge being over compensated for by the resize or image compression algorithm, or a high quality filter being used with a bad support size.</p></li><li><h3 id="Aliasing-and-Moire-Effects"><a href="#Aliasing-and-Moire-Effects" class="headerlink" title="Aliasing and Moiré Effects"></a>Aliasing and Moiré Effects</h3><p>Aliasing effects are generally seen as the production of ‘staircase’ like effects along edges of images. </p></li><li><h3 id="Blurring"><a href="#Blurring" class="headerlink" title="Blurring"></a>Blurring</h3></li></ol><h2 id="香农采样定理"><a href="#香农采样定理" class="headerlink" title="香农采样定理"></a>香农采样定理</h2><p>在实现移动像素的操作时，必须考虑到数字图像是连续图像的采样版本。Sampling是从连续函数中采样离散样本。Reconstrution是从离散样本恢复连续函数。</p><p>采样点太少会发生Aliasing: high frequencies masquerade as low ones。具体而言，会发生Spatial aliasing（due to limited spatial resolution）、Temporal aliasing（ due to limited temporal resolution）和Missing Image Detail.</p><p><strong>Sampling rate must be &gt; 2 bandwidth.</strong></p><ul><li>That frequency is called the bandwidth</li><li>The minimum sampling rate for a bandlimited function is called the “Nyquist rate”</li><li>Shannon: A signal can be reconstructed from its samples iff it has no content ≥ ½ the sampling frequency：只有当信号的频率成分全部低于采样频率的一半（奈奎斯特频率）时，才能从采样中完整重建信号。</li></ul><p>香农采样定理里面提到“采样频率等于有效信号频率的两倍”很关键。如果我们采样的频率过慢，就会导致有多条函数经过采样点进而导致采样点对应的时序函数结果不唯一，所以要提高采样频率。</p><p>香农采样定理跟图像的联系是自然而然的。图像本身也是现实世界在计算机中的一种离散表示，所以图像在经过傅里叶变换之后能够被表示为某种时域函数（一堆正弦或余弦函数的叠加），那图像处理中的香农采样定理，就可以表示为：当我们使用高于最高频率2倍的频率对图像进行采样时，能够正确完整的复原图像。</p><p>Resample &#x3D; Reconstruct + Transform + Filter + Sample. 重新采样过程中往往通过卷积操作平滑信号或图像，滤除高频成分，防止伪影的产生，从而实现有效的重新采样。</p><ul><li>What is done is to try to use some type of weighted average of the original source pixel values to determine a good value for the new pixel. The real pixels surrounding the location of the new pixel forms a ‘neighbourhood’ of contributing values. The larger this neighbourhood is the slower the resize. This is a technique called <a href="https://legacy.imagemagick.org/Usage/convolve/#intro">Convolution</a>.</li></ul><h2 id="Sampling-operation"><a href="#Sampling-operation" class="headerlink" title="Sampling operation"></a>Sampling operation</h2><h3 id="Point-sampling"><a href="#Point-sampling" class="headerlink" title="Point sampling"></a>Point sampling</h3><p>Interpolation is usually only used for ‘point’ sampling images, when image scaling is either not known or needed. For example, when rotating image or minor distortions, the image’s scaling or size does not change, and as such an interpolation can produce a reasonable result, though not a very accurate one.  </p><p>Take value at closest pixel: This method is simple, but it causes  Aliasing and Moiré Effects.</p><ul><li>Box: just directly averaging the nearby pixels together</li></ul><h3 id="Triangle-filter"><a href="#Triangle-filter" class="headerlink" title="Triangle filter"></a>Triangle filter</h3><p>The ‘<strong>Triangle</strong>‘ or ‘<strong>Bilinear</strong>‘ interpolation filter just takes the interpolation of the nearest neighbourhood one step further, it weights them according to how close the new pixels position is to the the original pixels within the neighbourhood (or ‘<em>support</em>‘ region). The closer the new pixel is to a source image pixel, the more color that pixel contributes.</p><p>Convolve with triangle filte: Bilinearly interpolate four closest pixels.</p><h3 id="Other-Interpolated-Filters"><a href="#Other-Interpolated-Filters" class="headerlink" title="Other Interpolated Filters"></a>Other Interpolated Filters</h3><ul><li>Hermite</li><li>Lagrange</li><li>Catrom (Catmull-Rom) filter is a well known standard <a href="https://legacy.imagemagick.org/Usage/filter/#cubics">Cubic Filter</a> often used as an interpolation function</li></ul><h3 id="Gaussian-filter"><a href="#Gaussian-filter" class="headerlink" title="Gaussian filter"></a>Gaussian filter</h3><p>Convolve with Gaussian filter: Compute weighted sum of pixel neighborhood. </p><ul><li>Gaussian Interpolator Filter Variant</li></ul><h2 id="Image-Warping-Implementation-1"><a href="#Image-Warping-Implementation-1" class="headerlink" title="Image Warping Implementation"></a>Image Warping Implementation</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">Reverse mapping:<br><span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> x = <span class="hljs-number">0</span>; x &lt; xmax; x++) &#123;<br><span class="hljs-keyword">for</span> (<span class="hljs-built_in">int</span> y = <span class="hljs-number">0</span>; y &lt; ymax; y++) &#123;<br><span class="hljs-built_in">float</span> u = fx^-<span class="hljs-number">1</span>(x,y);<br><span class="hljs-built_in">float</span> v = fy^-<span class="hljs-number">1</span>(x,y);<br>dst(x,y) = resample_src(u,v,w);<br>&#125;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Basics</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Basics</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Git</title>
    <link href="/2024/06/04/7_Git/"/>
    <url>/2024/06/04/7_Git/</url>
    
    <content type="html"><![CDATA[<p>学习linux下的超强工具链(ง •̀_•́)ง‼。</p><span id="more"></span><p>Reference materials include:</p><ul><li><input checked="" disabled="" type="checkbox"> <a href="https://learngitbranching.js.org/?locale=zh_CN">Learn Git Branching</a>: 这个网站把git知识点做成关卡一样，然后教你操作，还有图形界面帮你理解</li><li><input checked="" disabled="" type="checkbox"> <a href="https://git-scm.com/doc">Git文档</a></li></ul><h2 id="1-基础指令"><a href="#1-基础指令" class="headerlink" title="1 基础指令"></a>1 基础指令</h2><h3 id="1-1-commit"><a href="#1-1-commit" class="headerlink" title="1.1 commit"></a>1.1 commit</h3><ul><li><code>git commit</code>：新建一次提交<br>Git 仓库中的提交记录保存的是你的目录下所有文件的快照，就像是把整个目录复制，然后再粘贴一样。Git 希望提交记录尽可能地轻量，因此在你每次进行提交时，它并不会盲目地复制整个目录，条件允许的情况下，它会将当前版本与仓库中的上一个版本进行对比，并把所有的差异打包到一起作为一个提交记录。Git 还保存了提交的历史记录。这也是为什么大多数提交记录的上面都有 parent 节点的原因 —— 对于项目组的成员来说，维护提交历史对大家都有好处。</li></ul><h3 id="1-2-branch"><a href="#1-2-branch" class="headerlink" title="1.2 branch"></a>1.2 branch</h3><ul><li><code>git branch &lt;branchName&gt;</code>：创建一个到名为 branchName 的分支，指向当前的提交记录<br>早建分支！多用分支！<br>即使创建再多的分支也不会造成储存或内存上的开销，并且按逻辑分解工作到不同的分支要比维护那些特别臃肿的分支简单多了。分支其实就相当于在说：“我想基于这个提交以及它所有的 parent 提交进行新的工作。”</li></ul><h3 id="1-3-checkout"><a href="#1-3-checkout" class="headerlink" title="1.3 checkout"></a>1.3 checkout</h3><ul><li><code>git checkout &lt;branchName&gt;</code>：切换到指定分支上<ul><li><code>git checkout -b &lt;your-branch-name&gt;</code>：创建一个新的分支同时切换到新创建的分支<br>！注意，在 Git 2.23 版本中，引入了一个名为<code>git switch</code>的新命令，最终会取代<code>git checkout</code>，因为 checkout 作为单个命令有点超载（它承载了很多独立的功能）。【<a href="https://git-scm.com/docs/git-switch">更多关于新命令<code>git switch</code>的内容</a>】</li></ul></li></ul><h3 id="1-4-merge-and-rebase"><a href="#1-4-merge-and-rebase" class="headerlink" title="1.4 merge and rebase"></a>1.4 merge and rebase</h3><ul><li><code>git merge</code>：将两个分支合并到一起的方法①<ul><li><code>git merge bugFix</code>：把 bugFix 合并到 main 里，main 现在会指向了一个拥有两个 parent 节点的提交记录，这意味着 main 包含了对代码库的所有修改</li><li><code>git checkout bugFix; git merge main</code>: 因为 main 继承自 bugFix，Git 什么都不用做，只是简单地把 bugFix 移动到 main 所指向的那个提交记录。<br>这种情况发生在“新建一个分支，在其上开发某个新功能，开发完成后再合并回主线”。在 Git 中合并两个分支时会产生一个特殊的提交记录，它有两个 parent 节点。翻译成自然语言相当于：“我要把这两个 parent 节点本身及它们所有的祖先都包含进来。”</li></ul></li><li><code>git rebase</code>：将两个分支合并到一起的方法②<br>Rebase 实际上就是取出一系列的提交记录，“复制”它们，然后在另外一个地方逐个的放下去。Rebase 的优势就是可以创造更线性的提交历史，如果只允许使用 Rebase 的话，代码库的提交历史将会变得异常清晰。</li></ul><h2 id="2-高级指令（Git的一些特性）"><a href="#2-高级指令（Git的一些特性）" class="headerlink" title="2 高级指令（Git的一些特性）"></a>2 高级指令（Git的一些特性）</h2><h3 id="2-1-HEAD"><a href="#2-1-HEAD" class="headerlink" title="2.1 HEAD"></a>2.1 HEAD</h3><p>HEAD指的就是 <code>.git/HEAD</code> 文件。<code>HEAD</code> 是当前分支引用的指针，它总是指向某次commit，默认是上一次的commit。大多数修改提交树的 Git 命令都是从改变 HEAD 的指向开始的。HEAD 通常情况下是指向分支名的（如 bugFix）。在你提交时，改变了 bugFix 的状态，这一变化通过 HEAD 变得可见。如果想看 HEAD 指向，可以通过 <code>cat .git/HEAD</code> 查看， 如果 HEAD 指向的是一个引用，还可以用 <code>git symbolic-ref HEAD</code> 查看它的指向。</p><h3 id="2-2-log"><a href="#2-2-log" class="headerlink" title="2.2 log"></a>2.2 log</h3><p>通过指定提交记录哈希值的方式在 Git 中移动不太方便。在实际应用时，可以使用 <code>git log</code> 来查查看提交记录的哈希值。并且哈希值在真实的 Git 世界中也会更长（基于 SHA-1，共 40 位，比如：<code>fed2da64c0efc5293610bdd892f82a58e8cbc5d8</code>）。不过 Git 对哈希的处理很智能。你只需要提供能够唯一标识提交记录的前几个字符即可。因此我可以仅输入<code>fed2</code> 而不是上面的一长串字符。</p><h3 id="2-3-分支树上的移动和撤销"><a href="#2-3-分支树上的移动和撤销" class="headerlink" title="2.3 分支树上的移动和撤销"></a>2.3 分支树上的移动和撤销</h3><p>除此之外，Git 还引入了相对引用。你可以从一个易于记忆的地方（比如 <code>bugFix</code> 分支或 <code>HEAD</code>）开始计算。</p><ul><li>使用操作符 <code>^</code> 向上移动 1 个提交记录<ul><li>把这个符号加在引用名称的后面，表示让 Git 寻找指定提交记录的 parent 提交。比如 <code>main^</code> 相当于“<code>main</code> 的 parent 节点”；<code>main^^</code> 是 <code>main</code> 的第二个 parent 节点。<code>git checkout main^</code></li><li>或者直接使用 <code>HEAD^</code> 往上移动：<code>git checkout HEAD^</code></li></ul></li><li>使用 操作符<code>~</code> 向上移动多个提交记录，如 <code>git checkout HEAD~4</code></li><li>强制修改分支位置<ul><li>直接使用 <code>-f</code> 选项让main分支强制指向HEAD的第三级parent提交。例如：<code>git branch -f main HEAD~3</code></li></ul></li></ul><h3 id="2-4-撤销变更"><a href="#2-4-撤销变更" class="headerlink" title="2.4 撤销变更"></a>2.4 撤销变更</h3><p>主要有两种方法用来撤销变更 <code>C1 -&gt; C2</code></p><p>①  <code>git reset HEAD^ </code>：通过把分支记录回退几个提交记录来实现撤销改动，你可以将这想象成“改写历史”；但在本地分支中使用这种方法对大家一起使用的远程分支无效。</p><p>② <code>git revert HEAD^</code>：在撤销的提交记录后面提交新纪录，因为新提交记录 <code>C2&#39;</code> 引入的更改看作是撤销 <code>C2</code> 这个提交的。也就是说 <code>C2&#39;</code> 的状态与 <code>C1</code> 是相同的。revert 之后就可以重新提交。</p><h2 id="3-移动提交记录（自由修改提交树）"><a href="#3-移动提交记录（自由修改提交树）" class="headerlink" title="3 移动提交记录（自由修改提交树）"></a>3 移动提交记录（自由修改提交树）</h2><p>Git剩余的 10% 的内容在处理复杂的工作流时非常重要——“整理提交记录”。比如开发人员有时会希望把这个提交放到这里, 那个提交放到刚才那个提交的后面。</p><h3 id="3-1-git-cherry-pick"><a href="#3-1-git-cherry-pick" class="headerlink" title="3.1 git cherry-pick"></a>3.1 git cherry-pick</h3><p>当你知道你所需要的提交记录，并且还知道这些提交记录的哈希值时, 用 cherry-pick 再好不过了 —— 没有比这更简单的方式了。</p><ul><li><code>git cherry-pick &lt;提交号&gt;...</code></li></ul><p>如果你想将一些提交复制到当前所在的位置（<code>HEAD</code>）下面的话， cherry-pick 是最直接的方式。</p><h3 id="3-2-交互式的-rebase"><a href="#3-2-交互式的-rebase" class="headerlink" title="3.2 交互式的 rebase"></a>3.2 交互式的 rebase</h3><p>但是如果你不清楚你想要的提交记录的哈希值，希望从一系列的提交记录中找到想要的记录。那我们可以利用交互式的 rebase：使用带参数 <code>--interactive</code> 的 rebase 命令, 简写为 <code>-i</code>。比如<code>git rebase -i HEAD~4</code></p><p>如果你在命令后增加了这个选项, Git 会打开一个 UI 界面并列出将要被复制到目标分支的备选提交记录，它会显示每个提交记录的哈希值和提交说明，提交说明有助于你理解这个提交进行了哪些更改。在实际使用时，所谓的 UI 窗口一般会在文本编辑器（如 Vim）中打开一个文件。</p><p>当 rebase UI界面打开时, 你能做3件事:</p><ul><li>调整提交记录的顺序（通过鼠标拖放来完成）</li><li>删除你不想要的提交（通过切换 <code>pick</code> 的状态来完成，关闭就意味着你不想要这个提交记录）</li><li>合并提交</li></ul><h2 id="4-Git-技术、技巧与贴士大集合"><a href="#4-Git-技术、技巧与贴士大集合" class="headerlink" title="4 Git 技术、技巧与贴士大集合"></a>4 Git 技术、技巧与贴士大集合</h2><h3 id="4-1-只取一个提交记录：本地栈式提交"><a href="#4-1-只取一个提交记录：本地栈式提交" class="headerlink" title="4.1 只取一个提交记录：本地栈式提交"></a>4.1 只取一个提交记录：本地栈式提交</h3><p>在解决某个特别棘手的 Bug时，为了便于调试而在代码中添加了一些调试命令并向控制台打印了一些信息。这些调试和打印语句都在它们各自的提交记录里。此时你选择通过 fast-forward 快速合并到 <code>main</code> 分支上，但这样的话 <code>main</code> 分支就会包含代码中的调试语句。实际我们只要让 Git 复制解决问题的那一个提交记录就可以了。跟之前我们在“整理提交记录”中学到的一样，我们可以使用</p><ul><li><code>git rebase -i</code></li><li><code>git cherry-pick</code></li></ul><p>来达到目的。</p><h3 id="4-2-提交的技巧-1"><a href="#4-2-提交的技巧-1" class="headerlink" title="4.2 提交的技巧#1"></a>4.2 提交的技巧#1</h3><p>接下来这种情况也是很常见的：你之前在 <code>newImage</code> 分支上进行了一次提交，然后又基于它创建了 <code>caption</code> 分支，然后又提交了一次。此时你想对某个以前的提交记录进行一些小小的调整，尽管那个提交记录并不是最新的了。我们可以通过下面的方法来克服困难：</p><ul><li>先用 <code>git rebase -i</code> 将提交重新排序，然后把我们想要修改的提交记录挪到最前</li><li>然后用 <code>git commit --amend</code> 来进行一些小修改</li><li>接着再用 <code>git rebase -i</code> 来将他们调回原来的顺序</li><li>最后我们把 main 移到修改的最前端（用你自己喜欢的方法）</li></ul><h3 id="4-3-提交的技巧-2"><a href="#4-3-提交的技巧-2" class="headerlink" title="4.3 提交的技巧#2"></a>4.3 提交的技巧#2</h3><h3 id="4-4-git-tag"><a href="#4-4-git-tag" class="headerlink" title="4.4 git tag"></a>4.4 git tag</h3><h3 id="4-5-git-describe"><a href="#4-5-git-describe" class="headerlink" title="4.5 git describe"></a>4.5 git describe</h3><h2 id="5-高级话题"><a href="#5-高级话题" class="headerlink" title="5 高级话题"></a>5 高级话题</h2><h3 id="5-1-多次Rebase"><a href="#5-1-多次Rebase" class="headerlink" title="5.1 多次Rebase"></a>5.1 多次Rebase</h3><h3 id="5-2-两次parent节点"><a href="#5-2-两次parent节点" class="headerlink" title="5.2 两次parent节点"></a>5.2 两次parent节点</h3><h3 id="5-3-纠缠不清的分支"><a href="#5-3-纠缠不清的分支" class="headerlink" title="5.3 纠缠不清的分支"></a>5.3 纠缠不清的分支</h3><h2 id="6-远程"><a href="#6-远程" class="headerlink" title="6 远程"></a>6 远程</h2><h3 id="6-1-Push-Pull-——-Git-远程仓库"><a href="#6-1-Push-Pull-——-Git-远程仓库" class="headerlink" title="6.1 Push &amp; Pull —— Git 远程仓库"></a>6.1 Push &amp; Pull —— Git 远程仓库</h3><p>远程仓库并不复杂, 在如今的云计算盛行的世界你可以把远程仓库作为你的仓库在另个一台计算机上的拷贝。你可以通过因特网与这台计算机通信 —— 也就是增加或是获取提交记录。</p><h4 id="6-1-1-git-clone"><a href="#6-1-1-git-clone" class="headerlink" title="6.1.1 git clone"></a>6.1.1 git clone</h4><p>直到现在, 教程都聚焦于<strong>本地</strong>仓库的操作（branch、merge、rebase 等等）。但我们现在需要学习远程仓库的操作 —— 我们需要一个配置这种环境的命令, 它就是 <code>git clone</code>。 从技术上来讲，<code>git clone</code> 命令在真实的环境下的作用是在<strong>本地</strong>创建一个远程仓库的拷贝（比如从 github.com）。 </p><h4 id="6-1-2-远程分支"><a href="#6-1-2-远程分支" class="headerlink" title="6.1.2 远程分支"></a>6.1.2 远程分支</h4><p>执行 <code>git clone</code> 命令后，我们的本地仓库多了一个名为 <code>o/main</code> 的分支, 这种类型的分支就叫<strong>远程</strong>分支。</p><ul><li>为什么有 <code>o/</code>？<ul><li>这是由于远程分支的命名规范：<code>&lt;remote name&gt;/&lt;branch name&gt;</code></li><li>因此，如果你看到一个名为 <code>o/main</code> 的分支，那么这个分支就叫 <code>main</code>，远程仓库的名称就是 <code>o</code>。大多数的开发人员会将它们主要的远程仓库命名为 <code>origin</code>，并不是 <code>o</code>。这是因为当你用 <code>git clone</code> 某个仓库时，Git 已经帮你把远程仓库的名称设置为 <code>origin</code> 了。不过 <code>origin</code> 对于我们的 UI 来说太长了，因此不得不使用简写 <code>o</code> :) 但是要记住, 当你使用真正的 Git 时, 你的远程仓库默认为 <code>origin</code>!</li></ul></li></ul><p>说了这么多，让我们看看实例。</p><h4 id="6-1-3-git-fetch（从远程仓库获取数据）"><a href="#6-1-3-git-fetch（从远程仓库获取数据）" class="headerlink" title="6.1.3 git fetch（从远程仓库获取数据）"></a>6.1.3 git fetch（从远程仓库获取数据）</h4><p>Git 远程仓库的操作实际可以归纳为两点：向远程仓库传输数据以及从远程仓库获取数据。既然我们能与远程仓库同步，那么就可以分享任何能被 Git 管理的更新（因此可以分享代码、文件、想法、情书等等）。从远程仓库获取数据的指令 ——  <code>git fetch</code>。</p><p><strong>1.<code>git fetch</code> 完成了仅有的但是很重要的两步:</strong></p><p>​- 从远程仓库下载本地仓库中缺失的提交记录</p><p>​- 更新远程分支指针(如 <code>o/main</code>)：<code>git fetch</code> 实际上将本地仓库中的远程分支更新成了远程仓库相应分支最新的状态。</p><p>远程分支反映了远程仓库在你<strong>最后一次与它通信时</strong>的状态，<code>git fetch</code> 就是你与远程仓库通信的方式了！<code>git fetch</code> 通常通过互联网（使用 <code>http://</code> 或 <code>git://</code> 协议) 与远程仓库通信。</p><p><strong>2.<code>git fetch</code> 不会做的事</strong></p><p><code>git fetch</code> 并不会改变你本地仓库的状态。它不会更新你的 <code>main</code> 分支，也不会修改你磁盘上的文件。所以, 你可以将 <code>git fetch</code> 的理解为单纯的下载操作。</p><h4 id="6-1-4-git-pull"><a href="#6-1-4-git-pull" class="headerlink" title="6.1.4 git pull"></a>6.1.4 git pull</h4><p>既然我们已经知道了如何用 <code>git fetch</code> 获取远程的数据, 现在我们学习如何将这些变化更新到我们的工作当中。以往的一些命令也可以实现:</p><ul><li><p><code>git cherry-pick o/main</code></p></li><li><p><code>git rebase o/main</code></p></li><li><p><code>git merge o/main</code></p></li><li><p>等等</p><p>实际上，由于先抓取更新再合并到本地分支这个流程很常用，因此 Git 提供了一个专门的命令来完成这两个操作。它就是我们要讲的 <code>git pull</code>。相当于<code>git fetch; git merge o/main</code> &#x3D; <code>git pull</code>。我们用 <code>fetch</code> 下载了 <code>C3</code>, 然后通过 <code>git merge o/main</code> 合并了这一提交记录。现在我们的 <code>main</code> 分支包含了远程仓库中的更新（在本例中远程仓库名为 <code>origin</code>）。</p></li></ul><h4 id="6-1-5-模拟团队合作"><a href="#6-1-5-模拟团队合作" class="headerlink" title="6.1.5 模拟团队合作"></a>6.1.5 模拟团队合作</h4><h4 id="6-1-6-git-push"><a href="#6-1-6-git-push" class="headerlink" title="6.1.6 git push"></a>6.1.6 git push</h4><h4 id="6-1-7-偏离的提交历史"><a href="#6-1-7-偏离的提交历史" class="headerlink" title="6.1.7 偏离的提交历史"></a>6.1.7 偏离的提交历史</h4><h4 id="6-1-8-锁定的main（locked-main）"><a href="#6-1-8-锁定的main（locked-main）" class="headerlink" title="6.1.8 锁定的main（locked main）"></a>6.1.8 锁定的main（locked main）</h4><h3 id="6-2-关于-origin-和它的周边-——-Git-远程仓库高级操作"><a href="#6-2-关于-origin-和它的周边-——-Git-远程仓库高级操作" class="headerlink" title="6.2 关于 origin 和它的周边 —— Git 远程仓库高级操作"></a>6.2 关于 origin 和它的周边 —— Git 远程仓库高级操作</h3>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tools</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Manim</title>
    <link href="/2024/06/01/5_Manim/"/>
    <url>/2024/06/01/5_Manim/</url>
    
    <content type="html"><![CDATA[<p>本篇博客记录使用Manim引擎渲染好看的动画的学习过程。</p><span id="more"></span><p>在学线性代数的过程对Grant Sanderson大佬自制的manim数学引擎制作的动画秀到了，我也学了一下，python的图像处理真是太奇妙了！</p><p>Reference materials include:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/108839666">知乎关于Manim使用的解说</a></li><li><a href="https://github.com/3b1b/manim">github上的Manim源码</a></li><li><a href="https://docs.manim.community/en/stable/faq/installation.html#different-versions">Manim 英文文档</a></li><li><a href="https://docs.manim.org.cn/">Manim 中文文档</a></li><li>Todd Zimmerman教授写的针对<a href="https://talkingphysics.wordpress.com/2019/01/08/getting-started-animating-with-manim-and-python-3-7/">Manim的使用教程</a>和他整理的<a href="https://github.com/zimmermant/manim_tutorial">github代码</a></li></ul><hr><p>3B1B 动画的制作思路是：根据自己想在场景中展现的内容和效果编写一系列的类，然后通过命令行对每个类进行实例化，前面输入的测试命令其实就包含了类的实例化过程，而每个类被实例化后都将得到一个动画片段，通过视频制作软件将各个片段衔接起来并配音，就能得到大家喜闻乐见的 3B1B 教学动画了。</p><h1 id="1-弹出的窗口中会播放一个绘制正方形并变换为圆的动画"><a href="#1-弹出的窗口中会播放一个绘制正方形并变换为圆的动画" class="headerlink" title="1. 弹出的窗口中会播放一个绘制正方形并变换为圆的动画"></a>1. 弹出的窗口中会播放一个绘制正方形并变换为圆的动画</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> manimlib <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SquareToCircle</span>(<span class="hljs-title class_ inherited__">Scene</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">construct</span>(<span class="hljs-params">self</span>):<br>        circle = Circle()<br>        circle.set_fill(BLUE, opacity=<span class="hljs-number">0.5</span>)<br>        circle.set_stroke(BLUE_E, width=<span class="hljs-number">4</span>)<br>        square = Square()<br><br>        self.play(ShowCreation(square))<br>        self.wait()<br>        self.play(ReplacementTransform(square, circle))<br>        self.wait()<br>        <br>        <span class="hljs-comment"># 在代码的末尾加上如下一行来启用交互</span><br>        <span class="hljs-comment"># self.embed()</span><br>        <span class="hljs-comment">## # 在水平方向上拉伸到四倍</span><br>        <span class="hljs-comment">## play(circle.animate.stretch(4, dim=0))</span><br>        <span class="hljs-comment"># # 旋转90°</span><br>        <span class="hljs-comment"># play(Rotate(circle, TAU / 4))</span><br>        <span class="hljs-comment"># # 在向右移动2单位同时缩小为原来的1/4</span><br>        <span class="hljs-comment"># play(circle.animate.shift(2 * RIGHT), circle.animate.scale(0.25))</span><br>        <span class="hljs-comment"># # 为了非线性变换，给circle增加10段曲线（不会播放动画）</span><br>        <span class="hljs-comment"># circle.insert_n_curves(10)</span><br>        <span class="hljs-comment"># # 给circle上的所有点施加f(z)=z^2的复变换</span><br>        <span class="hljs-comment"># play(circle.animate.apply_complex_function(lambda z: z**2))</span><br>        <span class="hljs-comment"># # 关闭窗口并退出程序</span><br>        <span class="hljs-comment"># exit()</span><br><br><span class="hljs-comment"># 运行指令</span><br>manimgl start.py SquareToCircle <br><span class="hljs-comment"># 运行并保存</span><br>manimgl start.py SquareToCircle -ow<br></code></pre></td></tr></table></figure><h1 id="2-命令行参数和配置"><a href="#2-命令行参数和配置" class="headerlink" title="2 命令行参数和配置"></a>2 命令行参数和配置</h1><p><code>manimgl &lt;code&gt;.py &lt;Scene&gt; &lt;flags&gt;</code><br>    - <code>.py : 你写的 python 文件<br>    - <Scene> : 你想要渲染的场景'<br>    - <flags> : 传入的选项<br>        - <code>-w</code> 把场景写入文件<br>        - <code>-o</code> 把场景写入文件并打开<br>        - <code>-s</code> 跳到最后只展示最后一帧<br>            - <code>-so</code> 保存最后一帧并打开<br>        - <code>-n</code> <number> 跳到场景中第 n 个动画<br>        - <code>-f</code> 打开窗口全屏</p><p>local configuration file <code>D:\Manim\manim-master\custom_config.yml</code>. You can manually modify it.也可以对于不同目录使用不同的 custom_config.yml,子文件夹下的配置文件会在运行时覆盖根目录下的总局配置文件。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">manim/<br>├── manimlib/<br>│   ├── <span class="hljs-attribute">animation</span>/<br>│   ├── ...<br>│   ├── default_config<span class="hljs-selector-class">.yml</span><br>│   └── window<span class="hljs-selector-class">.py</span><br>├── project/<br>│   ├── <span class="hljs-selector-tag">code</span><span class="hljs-selector-class">.py</span><br>│   └── custom_config<span class="hljs-selector-class">.yml</span><br>└── custom_config.yml<br></code></pre></td></tr></table></figure><h1 id="3-样例学习"><a href="#3-样例学习" class="headerlink" title="3 样例学习"></a>3 样例学习</h1><h2 id="3-1-Animating-Methods"><a href="#3-1-Animating-Methods" class="headerlink" title="3.1  Animating Methods"></a>3.1  Animating Methods</h2><p>这个动画会出现一个Π的方阵，然后方阵集体向左移动，再变为渐变色，再旋转变为一个万花筒视角</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AnimatingMethods</span>(<span class="hljs-title class_ inherited__">Scene</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">construct</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># .get_grid() 方法会返回一个由该物体复制得到的阵列</span><br>        grid = Tex(<span class="hljs-string">r&quot;\pi&quot;</span>).get_grid(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, height=<span class="hljs-number">4</span>)<br>        self.add(grid)<br><br>        <span class="hljs-comment"># 你可以通过.animate语法来动画化物件变换方法</span><br>        self.play(grid.animate.shift(LEFT))<br><br>        <span class="hljs-comment"># 或者你可以使用旧的语法，把方法和参数同时传给Scene.play</span><br>        self.play(grid.shift, LEFT)<br><br>        <span class="hljs-comment"># 这两种方法都会在mobject的初始状态和应用该方法后的状态间进行插值</span><br>        <span class="hljs-comment"># 在本例中，调用grid.shift(LEFT)会将grid向左移动一个单位</span><br><br>        <span class="hljs-comment"># 这种用法可以用在任何方法上，包括设置颜色</span><br>        self.play(grid.animate.set_color(YELLOW))<br>        self.wait()<br>        self.play(grid.animate.set_submobject_colors_by_gradient(BLUE, GREEN))<br>        self.wait()<br>        self.play(grid.animate.set_height(TAU - MED_SMALL_BUFF))<br>        self.wait()<br><br>        <span class="hljs-comment"># 方法Mobject.apply_complex_function允许应用任意的复函数</span><br>        <span class="hljs-comment"># 将把Mobject的所有点的坐标看作复数</span><br><br>        self.play(grid.animate.apply_complex_function(np.exp), run_time=<span class="hljs-number">5</span>)<br>        self.wait()<br><br>        <span class="hljs-comment"># 更一般地说，你可以应用Mobject.apply方法，它接受从R^3到R^3的一个函数</span><br>        self.play(<br>            grid.animate.apply_function(<br>                <span class="hljs-keyword">lambda</span> p: [<br>                    p[<span class="hljs-number">0</span>] + <span class="hljs-number">0.5</span> * math.sin(p[<span class="hljs-number">1</span>]),<br>                    p[<span class="hljs-number">1</span>] + <span class="hljs-number">0.5</span> * math.sin(p[<span class="hljs-number">0</span>]),<br>                    p[<span class="hljs-number">2</span>]<br>                ]<br>            ),<br>            run_time=<span class="hljs-number">5</span>,<br>        )<br>        self.wait()<br></code></pre></td></tr></table></figure><h2 id="3-2-TextExample"><a href="#3-2-TextExample" class="headerlink" title="3.2 TextExample"></a>3.2 TextExample</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">TextExample</span>(<span class="hljs-title class_ inherited__">Scene</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">construct</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 想要正确运行这个场景，你需要确保你的计算机中安装了Consolas字体</span><br>        <span class="hljs-comment"># 关于Text全部用法，请见https://github.com/3b1b/manim/pull/680</span><br>        <span class="hljs-comment"># Text 可以创建文字，定义字体等</span><br>        text = Text(<span class="hljs-string">&quot;Here is a text&quot;</span>, font=<span class="hljs-string">&quot;Consolas&quot;</span>, font_size=<span class="hljs-number">90</span>)<br>        difference = Text(<br>            <span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">            The most important difference between Text and TexText is that\n</span><br><span class="hljs-string">            you can change the font more easily, but can&#x27;t use the LaTeX grammar</span><br><span class="hljs-string">            &quot;&quot;&quot;</span>,<br>            font=<span class="hljs-string">&quot;Arial&quot;</span>, font_size=<span class="hljs-number">24</span>,<br>            <span class="hljs-comment"># t2c是一个由 文本-颜色 键值对组成的字典</span><br>            t2c=&#123;<span class="hljs-string">&quot;Text&quot;</span>: BLUE, <span class="hljs-string">&quot;TexText&quot;</span>: BLUE, <span class="hljs-string">&quot;LaTeX&quot;</span>: ORANGE&#125;<br>        )<br>        <span class="hljs-comment"># VGroup 可以将多个 VMobject 放在一起看做一个整体。</span><br>        <span class="hljs-comment"># 例子中调用了 arrange() 方法来将其中子物体依次向下排列（DOWN），且间距为 buff</span><br>        VGroup(text, difference).arrange(DOWN, buff=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># Write 是显示类似书写效果的动画</span><br>        self.play(Write(text))<br>        <span class="hljs-comment"># FadeIn 将物体淡入，第二个参数表示淡入的方向</span><br>        self.play(FadeIn(difference, UP))<br>        self.wait(<span class="hljs-number">3</span>)<br><br>        fonts = Text(<br>            <span class="hljs-string">&quot;And you can also set the font according to different words&quot;</span>,<br>            font=<span class="hljs-string">&quot;Arial&quot;</span>,<br>            t2f=&#123;<span class="hljs-string">&quot;font&quot;</span>: <span class="hljs-string">&quot;Consolas&quot;</span>, <span class="hljs-string">&quot;words&quot;</span>: <span class="hljs-string">&quot;Consolas&quot;</span>&#125;,<br>            t2c=&#123;<span class="hljs-string">&quot;font&quot;</span>: BLUE, <span class="hljs-string">&quot;words&quot;</span>: GREEN&#125;<br>        )<br>        fonts.set_width(FRAME_WIDTH - <span class="hljs-number">1</span>)<br>        slant = Text(<br>            <span class="hljs-string">&quot;And the same as slant and weight&quot;</span>,<br>            font=<span class="hljs-string">&quot;Consolas&quot;</span>,<br>            t2s=&#123;<span class="hljs-string">&quot;slant&quot;</span>: ITALIC&#125;,<br>            t2w=&#123;<span class="hljs-string">&quot;weight&quot;</span>: BOLD&#125;,<br>            t2c=&#123;<span class="hljs-string">&quot;slant&quot;</span>: ORANGE, <span class="hljs-string">&quot;weight&quot;</span>: RED&#125;<br>        )<br>        VGroup(fonts, slant).arrange(DOWN, buff=<span class="hljs-number">0.8</span>)<br>        <span class="hljs-comment"># FadeOut 将物体淡出，第二个参数表示淡出的方向</span><br>        self.play(FadeOut(text), FadeOut(difference, shift=DOWN))<br>        self.play(Write(fonts))<br>        self.wait()<br>        self.play(Write(slant))<br>        self.wait()<br></code></pre></td></tr></table></figure><h2 id="3-3-TexTransformExample"><a href="#3-3-TexTransformExample" class="headerlink" title="3.3 TexTransformExample"></a>3.3 TexTransformExample</h2><p><a href="https://docs.manim.org.cn/getting_started/example_scenes.html">https://docs.manim.org.cn/getting_started/example_scenes.html</a></p>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tools</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linear Algebra</title>
    <link href="/2024/06/01/20_Linear-Algebra/"/>
    <url>/2024/06/01/20_Linear-Algebra/</url>
    
    <content type="html"><![CDATA[<p>本篇博客记录线性代数相关的学习。</p><span id="more"></span><p>到现在，感觉世界的本质就是物理，物理学真是永远的白月光。而数学是将世界进行抽象化表示的工具，本科对于线代的学习还是囫囵吞枣了，所以打算集合多个收罗到的资料重新过一遍，加深对线代几何意义的理解。</p><p>Reference materials include:</p><ul><li><a href="https://intuitive-math.club/linear-algebra/spaces">A visual intuition</a></li><li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3Blue1Brown：Essence of linear algebra（youtube）</a><!-- - [3Blue1Brown：Essence of linear algebra（bilibili）](https://www.bilibili.com/video/BV1Ys411k7yQ/) --></li></ul><h1 id="1-向量"><a href="#1-向量" class="headerlink" title="1 向量"></a>1 向量</h1><p>什么是向量？</p><ol><li>（物理学）向量是空间中的箭头，关键在于箭头的长度和方向</li><li>（计算机）向量是有序的数字列表</li><li>（数学）向量可以是任何东西，只需要保证两个向量相加以及数字与向量相乘是有意义的即可</li></ol><p>原点可以看做空间的中心和所有向量的起点。向量的两个基础运算分别是向量加法和向量数乘。线性代数为数据分析提供了一条将大量数据列表概念化、可视化的渠道，，它让数据样式变得非常明晰，并让你大致了解特定运算的意义；另一方面线性代数可以是一种通过计算机能处理的数字来描述并操作空间的语言。</p><h1 id="2-线性组合、跨度和基向量"><a href="#2-线性组合、跨度和基向量" class="headerlink" title="2 线性组合、跨度和基向量"></a>2 线性组合、跨度和基向量</h1><p>xy坐标有两个非常特殊的向量：</p><ul><li>一个指向正右方，长度为1，通常被称为<code>i帽</code>或者x方向的单位向量</li><li>一个指向正上方，长度为1，通常被称为<code>j帽</code>或者y方向的单位向量</li><li><strong>i帽和j帽合起来被称为坐标系的基</strong></li></ul><p>两个数乘向量的和被称为这两个向量的线性组合。如果固定其中一个标量，而让另一个标量自由变化，所产生的向量的终点会描出一条直线（这就是“线性的”的由来）。而两个向量张成的空间是指仅通过向量加法与向量数乘这两种基础运算所能获得的集合。</p><h2 id="Vectors-vs-Points"><a href="#Vectors-vs-Points" class="headerlink" title="Vectors vs. Points"></a>Vectors vs. Points</h2><p>我们通常用向量的终点代表该向量，他的起点位于原点。</p><p>当你有多个向量，并且可以移除其中一个而不减小向量张成的空间大小，我们称它们<strong>线性相关</strong>。另一种表述是说，其中一个向量可以表示为其他向量的线性组合，因为这个向量已经落在其它向量张成的空间之中。另一方面，如果所有向量都给张成的空间增添了新的维度，他们被称为<strong>线性无关</strong>的。<br><strong>The basis of a vector space is a set of linearly independent vectors that span the full space</strong></p><h1 id="3-矩阵与空间变换"><a href="#3-矩阵与空间变换" class="headerlink" title="3 矩阵与空间变换"></a>3 矩阵与空间变换</h1><p>我们称满足以下的变换为线性变换（保持网格线平行且等距分布的变换）：</p><ul><li>直线在变换后仍保持为直线，没有弯曲</li><li>原点固定</li></ul><p>线性变换是操作空间的一种手段。</p><h1 id="4-矩阵乘法作为复合"><a href="#4-矩阵乘法作为复合" class="headerlink" title="4 矩阵乘法作为复合"></a>4 矩阵乘法作为复合</h1><p>矩阵代表的变换乘积应该从右往左读（首先应用右侧矩阵所描述的变换），这是因为它起源于函数的记号，因为我们总是将函数写在变量的左侧。所以对于复合函数而言，你总是从右往左读。<br>矩阵乘法具有结合性但不具有交换性。</p><h1 id="5-三维线性变换"><a href="#5-三维线性变换" class="headerlink" title="5 三维线性变换"></a>5 三维线性变换</h1><p>三维空间中得旋转可以分解为简单分立得旋转的复合。</p><h1 id="6-行列式"><a href="#6-行列式" class="headerlink" title="6 行列式"></a>6 行列式</h1><p>二维空间中，一个线性变换的行列式的值，可以直观理解为把单位面积的正方形的面积放大多少倍。行列式的正负跟正方向有关。<br>在三维空间中，使用右手定则来表示基向量的定向：</p><ul><li>食指指向i帽</li><li>中指指向j帽</li><li>大拇指指向k帽</li></ul><p><code>det(M1M2)=det(M1)det(M2)</code></p><h1 id="7-逆矩阵、列空间、秩和零空间"><a href="#7-逆矩阵、列空间、秩和零空间" class="headerlink" title="7 逆矩阵、列空间、秩和零空间"></a>7 逆矩阵、列空间、秩和零空间</h1><p>Ax &#x3D; v</p><ul><li>A是线性表换的矩阵</li><li>x与v是变换前后的向量</li></ul><p>如果A的行列式不为0，那么A存在A逆（Inverse matrices），使得A逆乘以A等于一个“什么都不做”的矩阵（恒等变换）<br>如果A的行列式为0，那么这个方程组相关的变换将空间压缩到更低的维度上（因为变换后的单位面积正方形变成了点），此时没有逆变换，因为你不能把一条线解压缩为一个平面。当变换的结果为一条直线时，说明结果是一维的，我们称这个变换的秩（rank）为1。当变换的结果为一个平面时，结果是2维的，我们称这个变换的秩（rank）为2。</p><p><strong>所以rank代表变换后空间的维数。</strong><br><strong>而不管是一条直线，一个平面还是一个三维空间等，所有可能的变换结果的集合都被称为矩阵的“列空间”（Column space）。</strong>矩阵的列会告诉你基向量变换后的位置，这些变换后的基向量张成的空间就是所有可能的变换结果。换句话说，列空间就是矩阵的列所张成的空间。所以更精确的rank的定义是：<strong>秩（Rank）表示列空间的维数。</strong>当rank达到最大值时，意味着rank与列数相等，我们称为满秩。<br>(!!!注意，零向量一定会被包含在列空间中，因为线性变换必须保持原点位置不变。)<br>对一个满秩变换来说，唯一能变换后落在原点的就是零向量自身。但对一个非满秩的矩阵来说，它往往会把空间压缩到一个更低的维度上，也就是说会有一系列向量在变换后成为零向量。<strong>落在原点的向量的集合被称为“零空间”（Null space）或“核”</strong></p><h1 id="8-非方矩阵作为维度之间的变换"><a href="#8-非方矩阵作为维度之间的变换" class="headerlink" title="8 非方矩阵作为维度之间的变换"></a>8 非方矩阵作为维度之间的变换</h1><p>非方矩阵（Nonsquare matrices）。用矩阵代表变换。比如变换的矩阵是3x2，三行两列，我们之前说过每一列表示基向量被变换后在当前空间的位置，有三行，意味着现在的空间是三维的，有两列，代表之前有两个基向量，所以3x2大小的变换矩阵它代表的是将二维空间隐射到三维空空间上。</p><h1 id="9-点积和对偶性"><a href="#9-点积和对偶性" class="headerlink" title="9 点积和对偶性"></a>9 点积和对偶性</h1><p>点积与顺序无关。点积是理解投影的有力工具，并且方便检验两个向量的指向是否相同。<br>两个向量点乘，就是将其中一个向量转化为线性变换。比如，由第八节可知[1 -2]的一行2列的行向量可以看作是把二维平面压缩到1维数轴的线性变换，相当于原先的x-(1，0)和y-(0,1)现在变成了x-(1，0)和y-(-2,0)。因此对于一个[1 -2]的一行2列的行向量乘上一个2行一列的[4 3]的列向量。我们可以看作是二维坐标中原本<code>x = 4</code>，<code>j=3</code>的向量其基向量x和j分别被变换为<code>x-(1，0)</code>和<code>y-(-2,0)</code>，然后再统计倍数4和3，也就是<code>1*4+(-2)*3 = -2</code>.</p><p>对偶性的思想在于：每当你看到一个多维空间到数轴的线性变换时，该线性变换都与那个空间中的唯一一个向量对应。也就是说向量点积可以与一个线性变换等价。数值上说，这是因为这类线性变换可以用一个只有一行的矩阵描述，而它的每一列给出了变换后基向量的位置，将这个矩阵与某个向量v相乘，在计算上与将矩阵转置得到的向量和v点乘的结果相同。因此每当你看到一个从空间到数轴的线性变换，你都能找到一个向量被称为这个变换的对偶向量（Daul vector），使得应用线性变换和与对偶向量点乘等价。</p><h1 id="10-以线性变换的目光看叉积"><a href="#10-以线性变换的目光看叉积" class="headerlink" title="10 以线性变换的目光看叉积"></a>10 以线性变换的目光看叉积</h1><p>叉乘跟顺序有关。叉乘的结果不是一个数，而是一个向量，这个向量的长度就是两个向量所围成的平行四边形的面积，该向量的方向与平行四边形垂直，符合右手定则。</p><h1 id="11-叉乘与对偶性的关系"><a href="#11-叉乘与对偶性的关系" class="headerlink" title="11 叉乘与对偶性的关系"></a>11 叉乘与对偶性的关系</h1><p>首先我们定义一个从三维空间到数轴的特定线性变换，并且它是根据向量v和w来定义的，然后当我们将这个变换与三维空间中的对偶向量关联时，这个对偶向量就会是v和w的叉积。</p><h1 id="12-克莱默法则的几何解释"><a href="#12-克莱默法则的几何解释" class="headerlink" title="12 克莱默法则的几何解释"></a>12 克莱默法则的几何解释</h1><p><strong>正交变换-Orthonormal</strong>：保留点积的变换，它是使所有基本向量彼此垂直并且仍然具有单位长度的向量，它们通常被看作旋转矩阵，对应刚性运动，没有拉伸、挤压或变形。<br>克莱默法则用于求解线性方程组的解。</p><h1 id="13-基础变更"><a href="#13-基础变更" class="headerlink" title="13 基础变更"></a>13 基础变更</h1><p>总的来说，当你看到这样的一个表达式：A逆乘以M乘以A，它暗示这是一种数学上的转移作用，中间的矩阵M代表一种你可见的变换，而外侧的两个矩阵代表视角上的转化，矩阵的乘积结果代表他人视角中的转化。</p><h1 id="14-特征向量和特征值"><a href="#14-特征向量和特征值" class="headerlink" title="14 特征向量和特征值"></a>14 特征向量和特征值</h1><p>特征向量的意义是针对向量V，存在变换A，使得V经过变换A之后仍然停留在V所张成的空间中，特征值表示压缩&#x2F;拉伸的标量。<br>若基向量恰好是特征向量，则矩阵的对角元是它们所属的特征值。</p><h1 id="15-计算特征值的快速技巧"><a href="#15-计算特征值的快速技巧" class="headerlink" title="15 计算特征值的快速技巧"></a>15 计算特征值的快速技巧</h1><p>设一个向量为[[a b],[c d]]，它的特征值对应为λ1和λ2，则有:</p><ul><li>mean(λ1+λ2)&#x3D;λ(a+d)  &#x3D;&gt; m</li><li>λ1λ2 &#x3D; ad-bc  &#x3D;&gt; p</li><li>λ1, λ2 &#x3D; m +&#x2F;- √(m^2 - p)</li></ul><h1 id="16-抽象矢量空间"><a href="#16-抽象矢量空间" class="headerlink" title="16 抽象矢量空间"></a>16 抽象矢量空间</h1><p>满足可加性和成比例这两个性质的变换是线性变换。只要你处理的对象集具有合理的数乘和相加的概念，不管是空间中的箭头、一组数、函数的集合还是你定义的其他奇怪东西的集合，线性代数中所有关于向量、线性变换和其他的概念都应该适用于它。而这些类似向量的事物，比如箭头、一组数、函数等，它们构成的集合被称为“向量空间”</p>]]></content>
    
    
    <categories>
      
      <category>Math</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch_《Tutorials》</title>
    <link href="/2024/05/29/1_Pytorch_1/"/>
    <url>/2024/05/29/1_Pytorch_1/</url>
    
    <content type="html"><![CDATA[<p>本篇章主要记录的是官网提供的<a href="https://pytorch.org/tutorials/">Tutorials</a>和<a href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch官方文档(中文版)</a>的重要内容，是Pytorch探索之旅的第2篇章啦！</p><span id="more"></span><p>Reference materials include:</p><ul><li><a href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch官方文档(中文版)</a>: 我感觉这个文档的好处在于非常详尽的给出相关函数和模块的细致化定义介绍</li><li><a href="https://pytorch.org/tutorials/">Tutorials</a>：感觉官网的这个文档是从Pytorch的使用框架去简要介绍</li><li>Andrew W. Traska撰写的<a href="https://www.manning.com/books/grokking-deep-learning">《Grokking Deep Learning》</a>是开发强大模型和理解深度神经网络基础机制的重要资源</li><li>Ian Goodfellow, Yoshua Bengio和Aaron Courville的<a href="https://www.deeplearningbook.org/">《Deep Learning》</a></li><li>清华翻译的<a href="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/">《Deep learning with PyTorch》</a></li></ul><h1 id="1-Quickstart"><a href="#1-Quickstart" class="headerlink" title="1 Quickstart"></a>1 Quickstart</h1><h2 id="1-1-Working-with-data"><a href="#1-1-Working-with-data" class="headerlink" title="1.1 Working with data"></a>1.1 Working with data</h2><p>PyTorch has two primitives to work with data: <code>torch.utils.data.DataLoader</code> and <code>torch.utils.data.Dataset</code>. <strong>Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset.</strong> PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO (<a href="https://pytorch.org/vision/stable/datasets.html">full list here</a>).<br>Every TorchVision Dataset includes two arguments: <code>transform</code> and <code>target_transform</code> to modify the samples and labels respectively.</p><h2 id="1-2-Creating-Models"><a href="#1-2-Creating-Models" class="headerlink" title="1.2 Creating Models"></a>1.2 Creating Models</h2><p>To define a neural network in PyTorch, we create a class that inherits from nn.Module. </p><ul><li>We define the layers of the network in the <code>__init__</code> function</li><li>Specify how data will pass through the network in the <code>forward</code> function</li><li>To accelerate operations in the neural network, we move it to the GPU or MPS if available.</li></ul><h2 id="1-3-Optimizing-the-Model-Parameters"><a href="#1-3-Optimizing-the-Model-Parameters" class="headerlink" title="1.3 Optimizing the Model Parameters"></a>1.3 Optimizing the Model Parameters</h2><p>To train a model, we need a loss function and an optimizer.</p><h2 id="1-4-Saving-Models"><a href="#1-4-Saving-Models" class="headerlink" title="1.4 Saving Models"></a>1.4 Saving Models</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(model.state_dict(), <span class="hljs-string">&quot;model.pth&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Saved PyTorch Model State to model.pth&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="1-5-Loading-Models"><a href="#1-5-Loading-Models" class="headerlink" title="1.5 Loading Models"></a>1.5 Loading Models</h2><p>The process for loading a model includes re-creating the model structure and loading the state dictionary into it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = NeuralNetwork().to(device)<br>model.load_state_dict(torch.load(<span class="hljs-string">&quot;model.pth&quot;</span>))<br></code></pre></td></tr></table></figure><h1 id="2-Tensors"><a href="#2-Tensors" class="headerlink" title="2 Tensors"></a>2 Tensors</h1><p>Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators.</p><h2 id="2-1-Initializing-a-Tensor"><a href="#2-1-Initializing-a-Tensor" class="headerlink" title="2.1 Initializing a Tensor"></a>2.1 Initializing a Tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. Directly from data</span><br>data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br>x_data = torch.tensor(data)<br><br><span class="hljs-comment"># 2. From a NumPy array</span><br>np_array = np.array(data)<br>x_np = torch.from_numpy(np_array)<br><br><span class="hljs-comment"># 3. From another tensor</span><br>x_ones = torch.ones_like(x_data) <span class="hljs-comment"># retains the properties of x_data</span><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># overrides the datatype of x_data</span><br><br><span class="hljs-comment"># 4. With random or constant values</span><br>shape = (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br></code></pre></td></tr></table></figure><h2 id="2-2-Attributes-of-a-Tensor"><a href="#2-2-Attributes-of-a-Tensor" class="headerlink" title="2.2 Attributes of a Tensor"></a>2.2 Attributes of a Tensor</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pythohn">print(f&quot;Shape of tensor: &#123;tensor.shape&#125;&quot;)<br>print(f&quot;Datatype of tensor: &#123;tensor.dtype&#125;&quot;)<br>print(f&quot;Device tensor is stored on: &#123;tensor.device&#125;&quot;)<br></code></pre></td></tr></table></figure><h2 id="2-3-Operations-on-Tensor"><a href="#2-3-Operations-on-Tensor" class="headerlink" title="2.3 Operations on Tensor"></a>2.3 Operations on Tensor</h2><p>Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described <a href="https://pytorch.org/docs/stable/torch.html">here</a>.<br>By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># We move our tensor to the GPU if available</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    tensor = tensor.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br><br><span class="hljs-comment"># 1 Standard numpy-like indexing and slicing</span><br>tensor = torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First row: <span class="hljs-subst">&#123;tensor[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First column: <span class="hljs-subst">&#123;tensor[:, <span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Last column: <span class="hljs-subst">&#123;tensor[..., -<span class="hljs-number">1</span>]&#125;</span>&quot;</span>)<br>tensor[:,<span class="hljs-number">1</span>] = <span class="hljs-number">0</span><br><span class="hljs-built_in">print</span>(tensor)<br><span class="hljs-comment">## 输出</span><br>First row: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>First column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>Last column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br><br><span class="hljs-comment"># 2 Joining tensors</span><br><span class="hljs-comment"># - `torch.cat`: Concatenates the given sequence along an existing dimension.</span><br><span class="hljs-comment"># - `torch.stack`: Concatenates a sequence of tensors along a new dimension.</span><br><br><span class="hljs-comment"># 3 Single-element tensors </span><br><span class="hljs-comment"># If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using `item()`</span><br></code></pre></td></tr></table></figure><h2 id="2-4-Bridge-with-NumPy"><a href="#2-4-Bridge-with-NumPy" class="headerlink" title="2.4 Bridge with NumPy"></a>2.4 Bridge with NumPy</h2><p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Tensor to NumPy array</span><br>n = t.numpy()<br><span class="hljs-comment"># NumPy array to Tensor</span><br>t = torch.from_numpy(n)<br><br><span class="hljs-comment">## Changes in the NumPy array reflects in the tensor.</span><br>n = np.ones(<span class="hljs-number">5</span>)<br>t = torch.from_numpy(n)<br>np.add(n, <span class="hljs-number">1</span>, out=n)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br><span class="hljs-comment">### 输出</span><br>t: tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>], dtype=torch.float64)<br>n: [<span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span>]<br></code></pre></td></tr></table></figure><h1 id="3-Datasets-and-DataLoaders"><a href="#3-Datasets-and-DataLoaders" class="headerlink" title="3 Datasets and DataLoaders"></a>3 Datasets and DataLoaders</h1><h2 id="3-1-Loading-a-Dataset"><a href="#3-1-Loading-a-Dataset" class="headerlink" title="3.1 Loading a Dataset"></a>3.1 Loading a Dataset</h2><p>We can load the FashionMNIST Dataset with the following parameters:</p><ul><li>root is the path where the train&#x2F;test data is stored,</li><li>train specifies training or test dataset,</li><li>download&#x3D;True downloads the data from the internet if it’s not available at root.</li><li>transform and target_transform specify the feature and label transformations<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><br>training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br><br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></table></figure></li></ul><h2 id="3-2-Iterating-and-Visualizing-the-Dataset"><a href="#3-2-Iterating-and-Visualizing-the-Dataset" class="headerlink" title="3.2 Iterating and Visualizing the Dataset"></a>3.2 Iterating and Visualizing the Dataset</h2><p>We can index Datasets manually like a list: training_data[index]. We use matplotlib to visualize some samples in our training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">labels_map = &#123;<br>    <span class="hljs-number">0</span>: <span class="hljs-string">&quot;T-Shirt&quot;</span>,<br>    <span class="hljs-number">1</span>: <span class="hljs-string">&quot;Trouser&quot;</span>,<br>    <span class="hljs-number">2</span>: <span class="hljs-string">&quot;Pullover&quot;</span>,<br>    <span class="hljs-number">3</span>: <span class="hljs-string">&quot;Dress&quot;</span>,<br>    <span class="hljs-number">4</span>: <span class="hljs-string">&quot;Coat&quot;</span>,<br>    <span class="hljs-number">5</span>: <span class="hljs-string">&quot;Sandal&quot;</span>,<br>    <span class="hljs-number">6</span>: <span class="hljs-string">&quot;Shirt&quot;</span>,<br>    <span class="hljs-number">7</span>: <span class="hljs-string">&quot;Sneaker&quot;</span>,<br>    <span class="hljs-number">8</span>: <span class="hljs-string">&quot;Bag&quot;</span>,<br>    <span class="hljs-number">9</span>: <span class="hljs-string">&quot;Ankle Boot&quot;</span>,<br>&#125;<br>figure = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>cols, rows = <span class="hljs-number">3</span>, <span class="hljs-number">3</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, cols * rows + <span class="hljs-number">1</span>):<br>    sample_idx = torch.randint(<span class="hljs-built_in">len</span>(training_data), size=(<span class="hljs-number">1</span>,)).item()<br>    img, label = training_data[sample_idx]<br>    figure.add_subplot(rows, cols, i)<br>    plt.title(labels_map[label])<br>    plt.axis(<span class="hljs-string">&quot;off&quot;</span>)<br>    plt.imshow(img.squeeze(), cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><h2 id="3-3-Creating-a-Custom-Dataset-for-your-files"><a href="#3-3-Creating-a-Custom-Dataset-for-your-files" class="headerlink" title="3.3 Creating a Custom Dataset for your files"></a>3.3 Creating a Custom Dataset for your files</h2><p>A custom Dataset class must implement three functions: <code>__init__</code>, <code>__len__</code>, and <code>__getitem__</code>.<br>The FashionMNIST images are stored in a directory <code>img_dir</code>, and their labels are stored separately in a CSV file <code>annotations_file</code>.<br>The labels.csv file looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tshirt1.jpg, <span class="hljs-number">0</span><br>tshirt2.jpg, <span class="hljs-number">0</span><br>......<br>ankleboot999.jpg, <span class="hljs-number">9</span><br></code></pre></td></tr></table></figure><p>Code as fellow:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomImageDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-comment"># The __init__ function is run once when instantiating the Dataset object.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, annotations_file, img_dir, transform=<span class="hljs-literal">None</span>, target_transform=<span class="hljs-literal">None</span></span>):<br>        self.img_labels = pd.read_csv(annotations_file)<br>        self.img_dir = img_dir<br>        self.transform = transform<br>        self.target_transform = target_transform<br><br>    <span class="hljs-comment"># The __len__ function returns the number of samples in our dataset.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_labels)<br><br>    <span class="hljs-comment"># The __getitem__ function loads and returns a sample from the dataset at the given index idx.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-comment"># Based on the index, it identifies the image’s location on disk</span><br>        <span class="hljs-comment"># converts that to a tensor using read_image</span><br>        <span class="hljs-comment"># retrieves the corresponding label from the csv data in self.img_labels</span><br>        <span class="hljs-comment"># calls the transform functions on them (if applicable)</span><br>        <span class="hljs-comment"># and returns the tensor image and corresponding label in a tuple.</span><br>        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="hljs-number">0</span>])<br>        image = read_image(img_path)<br>        label = self.img_labels.iloc[idx, <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> self.transform:<br>            image = self.transform(image)<br>        <span class="hljs-keyword">if</span> self.target_transform:<br>            label = self.target_transform(label)<br>        <span class="hljs-keyword">return</span> image, label<br></code></pre></td></tr></table></figure><h2 id="3-4-Preparing-your-data-for-training-with-DataLoaders"><a href="#3-4-Preparing-your-data-for-training-with-DataLoaders" class="headerlink" title="3.4 Preparing your data for training with DataLoaders"></a>3.4 Preparing your data for training with DataLoaders</h2><p>The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h2 id="3-5-Iterate-through-the-DataLoader"><a href="#3-5-Iterate-through-the-DataLoader" class="headerlink" title="3.5 Iterate through the DataLoader"></a>3.5 Iterate through the DataLoader</h2><p>We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns <strong>a batch of train_features and train_labels</strong> (containing batch_size&#x3D;64 features and labels respectively). Because we specified shuffle&#x3D;True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Display image and label.</span><br>train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Feature batch shape: <span class="hljs-subst">&#123;train_features.size()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Labels batch shape: <span class="hljs-subst">&#123;train_labels.size()&#125;</span>&quot;</span>)<br>img = train_features[<span class="hljs-number">0</span>].squeeze()<br>label = train_labels[<span class="hljs-number">0</span>]<br>plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Label: <span class="hljs-subst">&#123;label&#125;</span>&quot;</span>)<br><br><span class="hljs-comment">## 输出</span><br><span class="hljs-comment">## 一张图示</span><br>Feature batch shape: torch.Size([<span class="hljs-number">64</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Labels batch shape: torch.Size([<span class="hljs-number">64</span>])<br>Label: <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><h1 id="4-Transformers"><a href="#4-Transformers" class="headerlink" title="4 Transformers"></a>4 Transformers</h1><p>We use transforms to perform some manipulation of the data and make it suitable for training. All TorchVision datasets have two parameters <code>transform</code> to modify the features and <code>target_transform</code> to modify the labels that accept callables containing the transformation logic. <a href="https://pytorch.org/vision/stable/transforms.html">More resource in torchvision.transforms module </a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor, Lambda<br><br>ds = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor(),<br>    <span class="hljs-comment"># Lambda transforms apply any user-defined lambda function. </span><br>    target_transform=Lambda(<span class="hljs-keyword">lambda</span> y: torch.zeros(<span class="hljs-number">10</span>, dtype=torch.<span class="hljs-built_in">float</span>).scatter_(<span class="hljs-number">0</span>, torch.tensor(y), value=<span class="hljs-number">1</span>))<br>)<br></code></pre></td></tr></table></figure><h1 id="5-Build-Model"><a href="#5-Build-Model" class="headerlink" title="5 Build Model"></a>5 Build Model</h1><ul><li>The <code>nn.Flatten</code> layer converts each 2D 28x28 image into a contiguous array of 784 pixel values.</li><li>The <code>nn.Linear</code> layer is a module that applies a linear transformation on the input using its stored weights and biases.</li><li>The <code>nn.ReLU</code> creates the complex mappings between the model’s inputs and outputs.</li><li><code>nn.Sequential</code> is an ordered container of modules.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">seq_modules = nn.Sequential(<br>    flatten,<br>    layer1,<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">10</span>)<br>)<br>input_image = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br>logits = seq_modules(input_image)<br></code></pre></td></tr></table></figure></li><li><code>nn.Softmax</code> : The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class.</li></ul><h1 id="6-Automatic-Differentiation"><a href="#6-Automatic-Differentiation" class="headerlink" title="6 Automatic Differentiation"></a>6 Automatic Differentiation</h1><p><a href="https://pytorch.org/docs/stable/notes/autograd.html">Autograd mechanics</a></p><h1 id="7-Optimization-Loop"><a href="#7-Optimization-Loop" class="headerlink" title="7 Optimization Loop"></a>7 Optimization Loop</h1><p>Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters (as we saw in the previous section), and optimizes these parameters using gradient descent.<br>We define the following hyperparameters for training:</p><ul><li><strong>Number of Epochs</strong>: the number times to iterate over the dataset</li><li><strong>Batch Size</strong>: the number of data samples propagated through the network before the parameters are updated</li><li><strong>Learning Rate</strong>: how much to update models parameters at each batch&#x2F;epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training</li></ul><p>Each iteration of the optimization loop is called an epoch.And it  consists of two main parts: </p><ul><li>The Train Loop：iterate over the training dataset and try to converge to optimal parameters.</li><li>The Validation&#x2F;Test Loop：iterate over the test dataset to check if model performance is improving.</li></ul><p>Common loss functions include <code>nn.MSELoss</code> (Mean Square Error) for regression tasks, and <code>nn.NLLLoss</code> (Negative Log Likelihood) for classification. <code>nn.CrossEntropyLoss</code> combines <code>nn.LogSoftmax</code> and <code>nn.NLLLoss</code>.</p><p>Optimization is the process of adjusting model parameters to reduce model error in each training step. </p><iframe width="560" height="315" src="https://youtu.be/tIeHLnjs5U8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><h1 id="8-Save-Load-and-Use-Model"><a href="#8-Save-Load-and-Use-Model" class="headerlink" title="8 Save, Load and Use Model"></a>8 Save, Load and Use Model</h1><p>PyTorch models store the learned parameters in an internal state dictionary, called <code>state_dict</code>. These can be persisted via the <code>torch.save</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.vgg16(weights=<span class="hljs-string">&#x27;IMAGENET1K_V1&#x27;</span>)<br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;model_weights.pth&#x27;</span>)<br><br><span class="hljs-comment"># To load model weights, you need to create an instance of the same model first, and then load the parameters </span><br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;model_weights.pth&#x27;</span>))<br><span class="hljs-comment"># be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode.</span><br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><h1 id="9-PACKAGE参考"><a href="#9-PACKAGE参考" class="headerlink" title="9 PACKAGE参考"></a>9 PACKAGE参考</h1><h2 id="9-1-torch"><a href="#9-1-torch" class="headerlink" title="9.1 torch"></a>9.1 torch</h2><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/">torch</a>包含了多维张量的数据结构以及基于其上的多种数学操作。另外，它也提供了多种工具，其中一些可以更有效地对张量和任意类型进行序列化。</p><h3 id="9-1-1-Tensor"><a href="#9-1-1-Tensor" class="headerlink" title="9.1.1 Tensor"></a>9.1.1 Tensor</h3><ul><li><code>torch.is_tensor(obj)</code></li><li><code>torch.is_storage(obj)</code></li><li><code>torch.numel(input)</code>: 返回input张量中的元素个数</li><li><code>torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)</code><ul><li>设置打印选项。</li><li>precision：浮点数输出的精度位数 (默认为8)</li><li>threshold：阈值，触发汇总显示而不是完全显示(repr)的数组元素的总数 （默认为1000）</li><li>edgeitems：汇总显示中，每维（轴）两端显示的项数（默认值为3）</li><li>linewidth：用于插入行间隔的每行字符数（默认为80）。Thresholded matricies will ignore this parameter.</li><li>profile：pretty打印的完全默认值。 可以覆盖上述所有选项 (默认为short, full)</li></ul></li></ul><h3 id="9-1-2-创建操作"><a href="#9-1-2-创建操作" class="headerlink" title="9.1.2 创建操作"></a>9.1.2 创建操作</h3><ul><li><code>torch.eye(n, m=None, out=None)</code>: 返回一个nxn的2维张量，对角线位置全1，其它位置全0</li><li><code>torch.from_numpy(ndarray)</code></li><li><code>torch.linspace(start, end, steps=100, out=None)</code>: 返回一个1维张量，包含在区间start和end上均匀间隔的steps个点。 输出1维张量的长度为steps</li><li><code>torch.logspace(start, end, steps=100, out=None)</code></li><li><code>torch.ones(*sizes, out=None)</code></li><li><code>torch.rand(*sizes, out=None)</code>: 返回一个张量，包含了从区间<code>[0,1)</code>的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义</li><li><code>torch.randn(*sizes, out=None)</code>: 返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数sizes定义</li><li><code>torch.randperm(n, out=None)</code>: 给定参数n，返回一个从0 到n -1 的随机整数排列</li><li><code>torch.arange(start, end, step=1, out=None)</code>: 包含从start到end，以step为步长的一组序列值(默认步长为1)</li><li><code>torch.zeros(*sizes, out=None)</code></li></ul><h3 id="9-1-3-索引-切片-连接-换位"><a href="#9-1-3-索引-切片-连接-换位" class="headerlink" title="9.1.3 索引,切片,连接,换位"></a>9.1.3 索引,切片,连接,换位</h3><ul><li><code>torch.cat(inputs, dimension=0)</code></li><li><code>torch.chunk(tensor, chunks, dim=0)</code>: 在给定维度(轴)上将输入张量进行分块儿</li><li><code>torch.gather(input, dim, index, out=None)</code>: 沿给定轴dim，将输入索引张量index指定位置的值进行聚合</li><li><code>torch.index_select(input, dim, index, out=None)</code>: 沿着指定维度对输入进行切片，取index中指定的相应项(index为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量_Tensor_有相同的维度(在指定轴上)。<strong>注意： 返回的张量不与原始张量共享内存空间</strong></li><li><code>torch.masked_select(input, mask, out=None)</code></li><li><code>torch.nonzero(input, out=None)</code>: 返回一个包含输入input中非零元素索引的张量</li><li><code>torch.split(tensor, split_size, dim=0)</code>: 将输入张量分割成相等形状的chunks（如果可分）</li><li><code>torch.squeeze(input, dim=None, out=None)</code>: 将输入张量形状中的1去除并返回</li><li><code>torch.stack(sequence, dim=0)</code>: 沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状</li><li><code>torch.t</code>: 输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数<code>transpose(input, 0, 1)</code>的简写函数</li><li><code>torch.transpose(input, dim0, dim1, out=None)</code>: 返回输入矩阵input的转置。交换维度dim0和dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改</li><li><code>torch.unbind</code>: 移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片</li><li><code>torch.unsqueeze(input, dim, out=None)</code>: 返回一个新的张量，对输入的制定位置插入维度 1</li></ul><h3 id="9-1-4-随机抽样"><a href="#9-1-4-随机抽样" class="headerlink" title="9.1.4 随机抽样"></a>9.1.4 随机抽样</h3><ul><li><code>torch.manual_seed(seed)</code></li><li><code>torch.initial_seed()</code></li><li><code>torch.get_rng_state()</code></li><li><code>torch.set_rng_state(new_state)</code></li><li><code>torch.default_generator = &lt;torch._C.Generator object&gt;</code></li><li><code>torch.bernoulli(input, out=None)</code>: 从伯努利分布中抽取二元随机数(0 或者 1)。输入张量须包含用于抽取上述二元随机值的概率</li><li><code>torch.multinomial(input, num_samples,replacement=False, out=None)</code>: 返回一个张量，每行包含从input相应行中定义的多项分布中抽取的num_samples个样本</li><li><code>torch.normal(means, std, out=None)</code>: 返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数</li></ul><h3 id="9-1-5-序列化"><a href="#9-1-5-序列化" class="headerlink" title="9.1.5 序列化"></a>9.1.5 序列化</h3><ul><li><code>torch.save(obj, f, pickle_module=&lt;module &#39;pickle&#39; from &#39;/home/jenkins/miniconda/lib/python3.5/pickle.py&#39;&gt;, pickle_protocol=2)</code>: 保存一个对象到一个硬盘文件上</li><li><code>torch.load</code></li></ul><h3 id="9-1-6-并行化"><a href="#9-1-6-并行化" class="headerlink" title="9.1.6 并行化"></a>9.1.6 并行化</h3><ul><li><code>torch.get_num_threads()</code>：获得用于并行化CPU操作的OpenMP线程数</li><li><code>torch.set_num_threads()</code>: 设定用于并行化CPU操作的OpenMP线程数</li></ul><h3 id="9-1-7-数学操作"><a href="#9-1-7-数学操作" class="headerlink" title="9.1.7 数学操作"></a>9.1.7 数学操作</h3><ul><li><code>torch.abs(input, out=None)</code>: 计算输入张量的每个元素绝对值</li><li><code>torch.sin(input, out=None)</code></li><li><code>torch.sinh(input, out=None)</code>: 双曲正弦</li><li><code>torch.cos(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的余弦</li><li><code>torch.cosh(input, out=None)</code>: 双曲余弦</li><li><code>torch.tan(input, out=None)</code></li><li><code>torch.tanh(input, out=None)</code>:双曲正切</li><li><code>torch.acos(input, out=None)</code>: 返回一个新张量，包含输入张量每个元素的反余弦</li><li><code>torch.asin(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的反正弦函数</li><li><code>torch.atan(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的反正切函数</li><li><code>torch.atan2(input1, input2, out=None)</code>: 返回一个新张量，包含两个输入张量input1和input2的反正切函数</li><li><code>torch.add(input, value, out=None)</code></li><li><code>torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None)</code>: 用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor</li><li><code>torch.addcmul(tensor, value=1, tensor1, tensor2, out=None)</code>: 用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor</li><li><code>torch.ceil(input, out=None)</code>: 天井函数，对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数，并返回结果到输出</li><li><code>torch.floor(input, out=None)</code>: 床函数,返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数</li><li><code>torch.round(input, out=None)</code>: 返回一个新张量，将输入input张量每个元素舍入到最近的整数</li><li><code>torch.sign(input, out=None)</code>: 符号函数：返回一个新张量，包含输入input张量每个元素的正负</li><li><code>torch.trunc(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数，其比x更接近零。简而言之，有符号数的小数部分被舍弃)</li><li><code>torch.clamp(input, min, max, out=None)</code>: 将输入input张量每个元素的夹紧到区间 [min,max]，并返回结果到一个新张量</li><li><code>torch.div(input, value, out=None)</code>: 将input逐元素除以标量值value，并返回结果到输出张量out</li><li><code>torch.exp(tensor, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的指数</li><li><code>torch.fmod(input, divisor, out=None)</code>: 计算除法余数，其结果的符号与 input 相同</li><li><code>torch.remainder(input, divisor, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的除法余数，其结果与 divisor 相同</li><li><code>torch.frac(tensor, out=None)</code>: 返回每个元素的分数部分</li><li><code>torch.lerp(start, end, weight, out=None)</code>: 对两个张量以start，end做线性插值， 将结果返回到输出张量。out &#x3D; start +weight * (end - start)</li><li><code>torch.log(input, out=None)</code>: 计算input的自然对数</li><li><code>torch.log1p(input, out=None) </code>: 计算input+1的自然对数</li><li><code>torch.mul(input, value, out=None)</code>: 用标量值value乘以输入input的每个元素，并返回一个新的结果张量</li><li><code>torch.neg(input, out=None)</code>: 返回一个新张量，包含输入input张量按元素取负</li><li><code>torch.pow(input, exponent, out=None)</code>: 对输入input的按元素求exponent次幂值，并返回结果张量</li><li><code>torch.reciprocal(input, out=None)</code>: 倒数</li><li><code>torch.rsqrt(input, out=None)</code>: 平方根倒数</li><li><code>torch.sqrt(input, out=None)</code>: 平方根</li><li><code>torch.sigmoid(input, out=None)</code></li></ul><h3 id="9-1-8-Reduction-Ops-缩减操作"><a href="#9-1-8-Reduction-Ops-缩减操作" class="headerlink" title="9.1.8 Reduction Ops(缩减操作)"></a>9.1.8 Reduction Ops(缩减操作)</h3><ul><li><code>torch.cumprod(input, dim, out=None)</code>: 返回输入沿指定维度的累积</li><li><code>torch.prod(input)</code>: 返回输入张量input 所有元素的积</li><li><code>torch.cumsum(input, dim, out=None)</code>: 返回输入沿指定维度的累和</li><li><code>torch.sum(input)</code>: 返回输入张量input 所有元素的和</li><li><code>torch.dist(input, other, p=2, out=None)</code>: 返回 (input - other) 的 p范数</li><li><code>torch.mean(input)</code></li><li><code>torch.median(input, dim=-1, values=None, indices=None)</code>: 返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor</li><li><code>torch.mode(input, dim=-1, values=None, indices=None)</code>: 返回给定维dim上，每行的众数值。 同时返回一个LongTensor，包含众数职的索引</li><li><code>torch.norm(input, p=2)</code>: 返回输入张量input 的p 范数</li><li><code>torch.std(input)</code>: 返回输入张量input 所有元素的标准差</li><li><code>torch.var(input)</code>: 返回输入张量所有元素的方差</li></ul><h3 id="9-1-8-Comparison-Ops-比较操作"><a href="#9-1-8-Comparison-Ops-比较操作" class="headerlink" title="9.1.8 Comparison Ops(比较操作)"></a>9.1.8 Comparison Ops(比较操作)</h3><ul><li><code>torch.eq(input, other, out=None)</code>：比较元素相等性, 返回一个<code>torch.ByteTensor</code>张量，包含了每个位置的比较结果(相等为1，不等为0 )</li><li><code>torch.equal(tensor1, tensor2)</code>: 如果两个张量有相同的形状和元素值，则返回True ，否则 False</li><li><code>torch.ge(input, other, out=None)</code>: 逐元素比较input和other，即是否 input&gt;&#x3D;other. 返回一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;&#x3D; other )</li><li><code>torch.gt(input, other, out=None)</code>: 逐元素比较input和other ， 即是否input&gt;other. 如果两个张量有相同的形状和元素值，则返回True ，否则 False</li><li><code>torch.le(input, other, out=None)</code>: 逐元素比较input和other ， 即是否input&lt;&#x3D;other</li><li><code>torch.lt(input, other, out=None)</code>: 逐元素比较input和other ， 即是否 input &lt; other</li><li><code>torch.ne(input, other, out=None)</code>: 逐元素比较input和other ， 即是否 input!&#x3D;other</li><li><code>torch.kthvalue(input, k, dim=None, out=None)</code>: 取输入张量input指定维上第k 个最小值</li><li><code>torch.max()</code></li><li><code>torch.min(input)</code></li><li><code>torch.sort(input, dim=None, descending=False, out=None)</code>: 对输入张量input沿着指定维按升序排序</li><li><code>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None)</code>: 沿给定dim维度返回输入张量input中 k 个最大值</li></ul><h3 id="9-1-9-Other-Operations-其他操作"><a href="#9-1-9-Other-Operations-其他操作" class="headerlink" title="9.1.9 Other Operations(其他操作)"></a>9.1.9 Other Operations(其他操作)</h3><ul><li><code>torch.cross(input, other, dim=-1, out=None)</code>: 返回沿着维度dim上，两个张量input和other的向量积（叉积）</li><li><code>torch.diag(input, diagonal=0, out=None)</code><ul><li>如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵</li><li>如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量</li></ul></li><li><code>torch.histc(input, bins=100, min=0, max=0, out=None)</code>: 计算输入张量的直方图. 如果min和max都为0, 则利用数据中的最大最小值作为边界</li><li><code>torch.renorm(input, p, dim, maxnorm, out=None)</code>: 返回一个张量，包含规范化后的各个子张量，使得沿着dim维划分的各子张量的p范数小于maxnorm</li><li><code>torch.trace(input)</code>: 返回输入2维矩阵对角线元素的和</li><li><code>torch.tril(input, k=0, out=None)</code>: 返回一个张量out，包含输入矩阵(2D张量)的下三角部分，out其余部分被设为0</li><li><code>torch.triu(input, k=0, out=None)</code>: 返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为0</li></ul><h3 id="9-1-10-BLAS-and-LAPACK-Operations"><a href="#9-1-10-BLAS-and-LAPACK-Operations" class="headerlink" title="9.1.10 BLAS and LAPACK Operations"></a>9.1.10 BLAS and LAPACK Operations</h3><ul><li><code>torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None)</code>: 执行mat &#x3D; beta * mat + alpha * batch1.bmm(batch2) 操作。<ul><li>mat：二维张量（n, p）</li><li>batch1：三维张量（batch_size, n, m）</li><li>batch2：三维张量（batch_size, m, p）</li></ul></li><li><code>torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None)</code>: 执行 mat &#x3D; beta * mat + alpha * torch.bmm(batch1, batch2) 操作<ul><li>mat：三维张量（batch_size, n, p）</li><li>batch1：三维张量（batch_size, n, m）</li><li>batch2：三维张量（batch_size, m, p）</li></ul></li><li><code>torch.bmm(batch1, batch2, out=None)</code>: 对存储在两个批batch1和batch2内的矩阵进行批矩阵乘操作<ul><li>batch1：三维张量（batch_size, n, m）</li><li>batch2：三维张量（batch_size, m, p）</li></ul></li><li><code>torch.mm(mat1, mat2, out=None)</code>: 执行二维矩阵乘法</li><li><code>torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None)</code>: 适用于将批量矩阵乘法的结果与一个二维矩阵按比例相加</li><li><code>torch.mv(mat, vec, out=None)</code>: 对矩阵mat和向量vec进行相乘</li><li><code>torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None)</code>: 向量相乘</li><li><code>torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None)</code>：张量积</li><li><code>torch.btrifact(A, info=None)</code>: 返回一个元组，包含LU 分解和pivots</li><li><code>torch.btrisolve(b, LU_data, LU_pivots)</code>: 返回线性方程组Ax&#x3D;b的LU解</li><li><code>torch.dot(tensor1, tensor2)</code></li><li><code>torch.eig(a, eigenvectors=False, out=None)</code>: 计算<strong>实方阵a</strong>的特征值和特征向量</li><li><code>torch.gels(B, A, out=None)</code>: 对形如m×n的满秩矩阵a计算其最小二乘和最小范数问题的解</li><li><code>torch.geqrf(input, out=None)</code>: 直接调用LAPACK的底层函数</li><li><code>torch.ger(vec1, vec2, out=None)</code>: 计算两向量vec1,vec2的张量积</li><li><code>torch.gesv(B, A, out=None)</code>: 返回线性方程组AX&#x3D;B的解</li><li><code>torch.inverse(input, out=None)</code>: 对方阵输入input取逆</li><li><code>torch.svd(input, some=True, out=None)</code>:  返回对形如<code>n×m</code>的实矩阵<code>A</code>进行奇异值分解的结果</li><li><code>torch.symeig(input, eigenvectors=False, upper=True, out=None)</code>: 返回<strong>实对称矩阵</strong>input的特征值和特征向量</li></ul><h2 id="9-2-torch-Tensor"><a href="#9-2-torch-Tensor" class="headerlink" title="9.2 torch.Tensor"></a>9.2 torch.Tensor</h2><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Tensor/">torch.Tensor</a>是一种包含单一数据类型元素的多维矩阵。</p><p>！注意： 会改变tensor的函数操作会用一个下划线后缀来标示。比如，torch.FloatTensor.abs_()会在原地计算绝对值，并返回改变后的tensor，而tensor.FloatTensor.abs()将会在一个新的tensor中计算结果。</p><h2 id="9-3-torch-Storage"><a href="#9-3-torch-Storage" class="headerlink" title="9.3 torch.Storage"></a>9.3 torch.Storage</h2><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Storage/">torch.Storage</a>是一个单一数据类型的连续一维数组。</p><h2 id="9-4-torch-nn"><a href="#9-4-torch-nn" class="headerlink" title="9.4 torch.nn"></a>9.4 torch.nn</h2><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#_1">torch.nn</a></p>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>PyTorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Userful tools</title>
    <link href="/2024/05/22/3_Userful-tools/"/>
    <url>/2024/05/22/3_Userful-tools/</url>
    
    <content type="html"><![CDATA[<p>工欲善其事必先利其器。</p><span id="more"></span><h1 id="1-笔记管理"><a href="#1-笔记管理" class="headerlink" title="1 笔记管理"></a>1 笔记管理</h1><p><a href="https://www.notion.so/">Notion</a><br><a href="https://mirrors.tuna.tsinghua.edu.cn/CTAN/info/lshort/chinese/lshort-zh-cn.pdf">111分钟了解Latex</a></p><h1 id="2-作图"><a href="#2-作图" class="headerlink" title="2 作图"></a>2 作图</h1><p>大家好像主要都是Visio，PPT为辅（我觉得PhotoShop也很方便，嘻嘻嘻感觉冥冥之中当年钻研摄影帮了我好多好多）</p><h2 id="2-1-首先是工具推荐"><a href="#2-1-首先是工具推荐" class="headerlink" title="2.1 首先是工具推荐"></a>2.1 首先是工具推荐</h2><ul><li><a href="https://app.diagrams.net/">draw.io</a></li><li><a href="https://github.com/guanyingc/latex_paper_writing_tips">Tips for Writing a Research Paper using LaTeX</a>:由陈冠英老师维护，给LaTeX初学者提供多个图表排版的例子，方便用到自己的论文当中。还有种会议poster的例子，可以参考。</li><li><a href="https://github.com/dair-ai/ml-visuals">ML Visuals</a>:里面包含100多个常用的神经网络的图，是google在线PPT的形式.<a href="https://docs.google.com/presentation/d/11mR1nkIR9fbHegFkcFq8z9oDQ5sjv8E3JJp1LfLGKuk/edit#slide=id.p">PPT直接下载</a></li><li><a href="https://www.iconfont.cn/search/index?searchType=icon&q=%E9%87%91%E5%B8%81&page=1&fromCollection=-1&fills=&tag=">阿里巴巴矢量图标素材库</a></li><li><a href="http://alexlenail.me/NN-SVG/index.html">NN SVG</a>:画神经网络结构用的。左侧是一些设置选项，可以自己设置节点，层数，还有网络类型等等，设置好以后可以把这个神经网络图直接下载下来，然后插入到visio或者PPT中就可以用了！</li><li><code>ChatHub</code>：是一个基于谷歌Chrome浏览器插件的chatbot聚合客户端，你可以用它在一个应用中使用多种AI聊天服务。</li><li><code>Code Interpreter</code>：OpenAI 的官方插件，通过设置中的Beta面板向所有ChatGPT Plus 用户提供。可以数据分析、创建图表、编辑文件、执行数学运算等。可惜要花钱，但它有以下优点：<ul><li>Code Interpreter 允许 AI 做数学题（非常复杂的数学题）和做更精确的文字工作（比如实际计算段落中的字数），因为它可以编写 Python 代码来解决大语言模型在数学和语言方面的固有弱点。</li><li>Code Interpreter 降低了幻觉和迷惑的概率。</li><li>Code Interpreter 让人工智能的用途更加广泛。</li><li>用户不必编程，因为 Code Interpreter 可以代替做所有的工作。</li><li>它给了你更多的 AI Moment。</li></ul></li><li><a href="https://github.com/OpenBMB/MiniCPM-V">视觉问答领域开源项目</a></li><li><a href="https://github.com/timqian/chinese-independent-blogs">中文独立博客</a></li></ul><h2 id="2-2-简单聊聊目前对作图的认知"><a href="#2-2-简单聊聊目前对作图的认知" class="headerlink" title="2.2 简单聊聊目前对作图的认知"></a>2.2 简单聊聊目前对作图的认知</h2><p>我感觉审稿人看论文是（就连我自己看论文的时候都是）挑重点看，如果图能把你想表达的观点说清楚，文字写得不好其实问题不大，毕竟文字表达一个个磨非常耗费心力和时间。而且人的注意力天生就是喜欢看图。而且画图其实蛮享受的，它是一个把你的思路显现出来的过程，喜欢。</p><p>但无论是哪种图，配色一般是淡蓝、淡红、淡黄、淡绿四种（彩虹一共才7种颜色，要体现对比就只能是这四种颜色了，而且你需要考虑一下有人红绿色盲 … … ），然后有时会用紫色、灰色补充（紫色多用于模块，灰色多用于小区域或者大面积打底）。整个论文的配色风格需要一致，比如说后面的折线图、柱状图、散点图最好还是以前面框架的颜色一致。</p><p>经过一次投稿，CV感觉只需要两种图，框架图和数据展示图。审美相关，这里安利一下，感觉Yongming Rao老师的图好好看：<br><img src="https://pic4.zhimg.com/80/v2-917d709c464f873d5876ab9665435543_720w.webp"></p><h1 id="3-论文"><a href="#3-论文" class="headerlink" title="3 论文"></a>3 论文</h1><ul><li><a href="http://www.researchrabbitapp.com/">Research Rabbit</a>: 文献检索及可视化工具，它使用 AI 帮助我们发现相关且有趣的研究文章, 这些文章是根据我们的研究方向来关联的。<a href="https://zhuanlan.zhihu.com/p/394423702">使用教程</a></li><li><a href="https://www.storkapp.me/?ref=1003">文献鸟Stork</a>:可以自己定义关键词，然后它每天会发邮件给你推送相关的文献</li><li><a href="https://www.researcher-app.com/">ResearchApp</a>:类似朋友圈一样，可以选择关注特定的学术期刊，只要对方有更新，app就会同步更新</li><li><a href="https://www.connectedpapers.com/">connectedpapers</a>: 相似论文查找</li><li><a href="https://www.scholar-inbox.com/">scholar-inbox</a>: 论文推荐</li><li><a href="https://kimi.moonshot.cn/">Kimi AI</a>: 论文问答</li><li><a href="https://help.jianguoyun.com/?p=4190">坚果云 + Zotero</a>: 文献管理</li></ul><h1 id="4-汇报"><a href="#4-汇报" class="headerlink" title="4 汇报"></a>4 汇报</h1><ul><li><a href="https://zhiwen.xfyun.cn/">讯飞智文-AI在线生成PPT、Word</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tools</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Configure online demo</title>
    <link href="/2024/05/19/6_onlineDemo/"/>
    <url>/2024/05/19/6_onlineDemo/</url>
    
    <content type="html"><![CDATA[<p>这篇文章总共纪录了学习Replicate和HuggingFace在线搭demo的过程，最终我选的是HuggingFace，所以HuggingFace的内容详尽一些，Replicate只有一点点。</p><span id="more"></span><h1 id="Replicate"><a href="#Replicate" class="headerlink" title="Replicate"></a>Replicate</h1><p>参考：<a href="https://replicate.com/docs">Replicate文档</a><br>Replicate是一个云端的机器学习模型运行平台，它允许用户使用云端API（在Python或Jupyter Notebook中）直接运行模型，并在云端进行模型的部署和调优。<br>其优势在于：<br>    - 无需下载、安装或配置<br>    - 快速轻松运行机器学习模型<br>    - 提供大量的预训练模型和数据集</p><h2 id="1-1-大致流程"><a href="#1-1-大致流程" class="headerlink" title="1.1 大致流程"></a>1.1 大致流程</h2><p>Replicate的HTTP API 可与任何编程语言配合使用。使用 Python 客户端，首先需要安装Python库：<code>pip install replicate</code>。</p><p>然后去<a href="https://replicate.com/account/api-tokens">account settings</a>中找到你的API token，并把它设置（声明）到你当前的代码环境：<code>export REPLICATE_API_TOKEN=&lt;paste-your-token-here&gt;</code>。</p><p>安全一点的方式为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># get a token: https://replicate.com/account</span><br><span class="hljs-keyword">from</span> getpass <span class="hljs-keyword">import</span> getpass<br><span class="hljs-keyword">import</span> os<br><br>REPLICATE_API_TOKEN = getpass()<br>os.environ[<span class="hljs-string">&quot;REPLICATE_API_TOKEN&quot;</span>] = REPLICATE_API_TOKEN<br><br><span class="hljs-comment">## 2.1 在Python代码中导入Replicate库，以便使用Replicate的功能</span><br><span class="hljs-keyword">import</span> replicate<br><span class="hljs-comment">### receive images as inputs. Use a file handle or URL:</span><br>image = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;mystery.jpg&quot;</span>, <span class="hljs-string">&quot;rb&quot;</span>)<br><span class="hljs-comment">### or...</span><br>image = <span class="hljs-string">&quot;https://example.com/mystery.jpg&quot;</span><br><br><span class="hljs-comment">## 2.2 调用模型，按需求返回</span><br>replicate.run(<br>  <span class="hljs-string">&quot;replicate/resnet:dd782a3d531b61af491d1026434392e8afb40bfb53b8af35f727e80661489767&quot;</span>,<br>  <span class="hljs-built_in">input</span>=&#123;<span class="hljs-string">&quot;image&quot;</span>: image&#125;<br>)<br></code></pre></td></tr></table></figure><p><a href="https://juejin.cn/post/7298642789078974515">有个跟我很像的用了controlnet的工作在Replicate上的部署教程</a><br>大概了解了一下，感觉Replicate挺简单的，但是好像不如Huggingface的使用者多以及GPU算力要贵一点。决定临阵倒戈，还是去学HuggingFace吧，谁知道以后遇到什么bug了能不能解决。</p><p>好多人用啊，让我看看这个最大的开源平台有多牛(ง๑ •̀_•́)ง</p><h1 id="Gradio-HuggingFace"><a href="#Gradio-HuggingFace" class="headerlink" title="Gradio + HuggingFace"></a>Gradio + HuggingFace</h1><p>计算机视觉和图像处理的算法一般都具有直观的实用性。为了推广自己工作的影响力，大家会选择把自己算法的实现效果部署到网页端的UI接口供大家使用。</p><h1 id="1-界面设计库Gradio"><a href="#1-界面设计库Gradio" class="headerlink" title="1 界面设计库Gradio"></a>1 界面设计库Gradio</h1><p>Gradio是MIT的开源项目，它允许我们快速建立demo或者web application。使用时可理解为一个Python包，Prerequisite: Gradio requires Python 3.8 or higher，它的安装命令：<code>pip install gradio</code>。<br><a href="https://www.gradio.app/guides">Gradio教程</a></p><h2 id="1-1-Quickstart"><a href="#1-1-Quickstart" class="headerlink" title="1.1 Quickstart"></a>1.1 Quickstart</h2><h3 id="1-1-1-示例，实现一个本地静态交互页面"><a href="#1-1-1-示例，实现一个本地静态交互页面" class="headerlink" title="1.1.1 示例，实现一个本地静态交互页面"></a>1.1.1 示例，实现一个本地静态交互页面</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>(<span class="hljs-params">name, intensity</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Hello, &quot;</span> + name + <span class="hljs-string">&quot;!&quot;</span> * <span class="hljs-built_in">int</span>(intensity)<br><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;slider&quot;</span>],<br>    outputs=[<span class="hljs-string">&quot;text&quot;</span>],<br>)<br><br>demo.launch()<br><br></code></pre></td></tr></table></figure><p>我用的anaconda命令行运行，它显示：</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Running <span class="hljs-keyword">on</span> <span class="hljs-built_in">local</span> URL:  http:<span class="hljs-comment">//127.0.0.1:7860</span><br><br><span class="hljs-keyword">To</span> create a <span class="hljs-keyword">public</span> <span class="hljs-keyword">link</span>, <span class="hljs-built_in">set</span> <span class="hljs-string">`share=True`</span> <span class="hljs-keyword">in</span> <span class="hljs-string">`launch()`</span>.<br></code></pre></td></tr></table></figure><p>然后我在本地浏览器输入<code>http://127.0.0.1:7788/</code>就看到了对应的页面。</p><h3 id="1-1-2-hot-reload-mode"><a href="#1-1-2-hot-reload-mode" class="headerlink" title="1.1.2 hot reload mode"></a>1.1.2 hot reload mode</h3><p>Automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in gradio before the name of the file instead of python. In the example:<code>gradio app.py</code>(Type this in terminal).<br><a href="https://www.gradio.app/guides/developing-faster-with-reload-mode">Hot Reloading Guide</a></p><h3 id="1-1-3-Understanding-the-Interface-Class"><a href="#1-1-3-Understanding-the-Interface-Class" class="headerlink" title="1.1.3 Understanding the Interface Class"></a>1.1.3 Understanding the Interface Class</h3><p>gradio的核心是它的<code>gr.Interface</code>函数，用来构建可视化界面。The Interface class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs.<br>它主要有三个核心属性：</p><ul><li><code>fn</code>: the function to wrap a user interface (UI) around</li><li><code>inputs</code>：the number of components should match the number of arguments in your function</li><li><code>outputs</code>：the number of components should match the number of return values from your function.</li></ul><p>所以对于任何图像处理类的ML代码来说，基本流程就是<strong>图像输入&gt;&gt;模型推理&gt;&gt;返回图片</strong>。<a href="https://www.gradio.app/main/guides/the-interface-class">building Interfaces</a></p><h3 id="1-1-4-Sharing-Your-Demo"><a href="#1-1-4-Sharing-Your-Demo" class="headerlink" title="1.1.4 Sharing Your Demo"></a>1.1.4 Sharing Your Demo</h3><p>Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set <code>share=True</code> in launch() like <code>demo.launch(share=True)</code>, and a publicly accessible URL will be created for your demo.</p><p>所以实际上他跑起来用的是我本地的算力资源，但是类似网络通信可以将输入输出在不同电脑之间进行联络。Share links expire after 72 hours. (it is also possible to <a href="https://github.com/huggingface/frp/">set up your own Share Server</a> on your own cloud server to overcome this restriction.)</p><h2 id="1-2-the-Interface-Class"><a href="#1-2-the-Interface-Class" class="headerlink" title="1.2 the Interface Class"></a>1.2 the Interface Class</h2><h3 id="1-2-1-Components"><a href="#1-2-1-Components" class="headerlink" title="1.2.1 Components"></a>1.2.1 Components</h3><p>提供了超过30种components(e.g. the gr.Image component is designed to handle input or output images, the gr.Label component displays classification labels and probabilities, the gr.Plot component displays various kinds of plots, and so on).<br>而且Gradio可以自动处理输入输出与函数fn之间的类型转换(Preprocessing and Postprocessing)，需要在对应的控件里加一个属性type：<code>img = gr.Image(type=&quot;pil&quot;)</code></p><h3 id="1-2-2-Components-Attributes"><a href="#1-2-2-Components-Attributes" class="headerlink" title="1.2.2 Components Attributes"></a>1.2.2 Components Attributes</h3><p>比如加一个滑轮效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 之前的</span><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;slider&quot;</span>],<br>    outputs=[<span class="hljs-string">&quot;text&quot;</span>],<br>)<br><span class="hljs-comment">######################################################################</span><br><span class="hljs-comment"># 现在</span><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, gr.Slider(value=<span class="hljs-number">2</span>, minimum=<span class="hljs-number">1</span>, maximum=<span class="hljs-number">10</span>, step=<span class="hljs-number">1</span>)],<br>    outputs=[gr.Textbox(label=<span class="hljs-string">&quot;greeting&quot;</span>, lines=<span class="hljs-number">3</span>)],<br>)<br><br></code></pre></td></tr></table></figure><p>Suppose you had a more complex function, with multiple outputs as well. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 之前的</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>(<span class="hljs-params">name, intensity</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Hello, &quot;</span> + name + <span class="hljs-string">&quot;!&quot;</span> * intensity<br><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, gr.Slider(value=<span class="hljs-number">2</span>, minimum=<span class="hljs-number">1</span>, maximum=<span class="hljs-number">10</span>, step=<span class="hljs-number">1</span>)],<br>    outputs=[gr.Textbox(label=<span class="hljs-string">&quot;greeting&quot;</span>, lines=<span class="hljs-number">3</span>)],<br>)<br><span class="hljs-comment"># lines=3 参数用于设置文本框的默认显示行数</span><br><span class="hljs-comment"># 具体来说，它控制了文本框的高度，使其在显示时能够容纳 3 行文本。</span><br><br><span class="hljs-comment">######################################################################</span><br><span class="hljs-comment"># 现在</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>(<span class="hljs-params">name, is_morning, temperature</span>):<br>    salutation = <span class="hljs-string">&quot;Good morning&quot;</span> <span class="hljs-keyword">if</span> is_morning <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;Good evening&quot;</span><br>    greeting = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;salutation&#125;</span> <span class="hljs-subst">&#123;name&#125;</span>. It is <span class="hljs-subst">&#123;temperature&#125;</span> degrees today&quot;</span><br>    celsius = (temperature - <span class="hljs-number">32</span>) * <span class="hljs-number">5</span> / <span class="hljs-number">9</span><br>    <span class="hljs-keyword">return</span> greeting, <span class="hljs-built_in">round</span>(celsius, <span class="hljs-number">2</span>)<br><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;checkbox&quot;</span>, gr.Slider(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>)],<br>    outputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;number&quot;</span>],<br>)<br></code></pre></td></tr></table></figure><h3 id="1-2-3-Reactive-Interfaces"><a href="#1-2-3-Reactive-Interfaces" class="headerlink" title="1.2.3 Reactive Interfaces"></a>1.2.3 Reactive Interfaces</h3><p>提供Live Interfaces模式，不需要submit button，会自动计算提交结果。暂时我不太需要这个功能，之后遇到再学。</p><h3 id="1-2-4-The-4-Kinds-of-Gradio-Interfaces"><a href="#1-2-4-The-4-Kinds-of-Gradio-Interfaces" class="headerlink" title="1.2.4 The 4 Kinds of Gradio Interfaces"></a>1.2.4 The 4 Kinds of Gradio Interfaces</h3><ol><li>Standard demos: which have both separate inputs and outputs</li><li>Output-only demos: which don’t take any input but produce on output</li><li>Input-only demos: which don’t produce any output but do take in some sort of input</li><li>Unified demos: which have both input and output components, but the input and output components are the same. This means that the output produced overrides the input (e.g. a text autocomplete model)</li></ol><h3 id="1-2-5-Queuing"><a href="#1-2-5-Queuing" class="headerlink" title="1.2.5 Queuing"></a>1.2.5 Queuing</h3><p>Every Gradio app comes with a built-in queuing system that can scale to thousands of concurrent users. You can configure the queue by using <code>queue()</code> method. 例如通过设置queue()的<code>default_concurrency_limit</code>参数来控制单次处理的请求数(默认是1):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">demo = gr.Interface(...).queue(default_concurrency_limit=<span class="hljs-number">5</span>)<br>demo.launch()<br></code></pre></td></tr></table></figure><h3 id="1-2-6-Streaming-outputs"><a href="#1-2-6-Streaming-outputs" class="headerlink" title="1.2.6 Streaming outputs"></a>1.2.6 Streaming outputs</h3><p>In some cases, you may want to stream a sequence of outputs rather than show a single output at once.官网说的场景是“图像生成，显示到结果图像中的每一时刻图像”，感觉我暂时不需要，之后再学。</p><h3 id="1-2-7-Alerts"><a href="#1-2-7-Alerts" class="headerlink" title="1.2.7 Alerts"></a>1.2.7 Alerts</h3><p>跟用户显示警告提醒他们某些输入输出的错误。主要使用<code>gr.Error(&quot;custom message&quot;)</code>、<code>gr.Warning(&quot;custom message&quot;)</code>和<code>gr.Info(&quot;custom message&quot;) </code>。The only difference between gr.Info() and gr.Warning() is the color of the alert.</p><h3 id="1-2-8-Styling"><a href="#1-2-8-Styling" class="headerlink" title="1.2.8 Styling"></a>1.2.8 Styling</h3><p><a href="https://www.gradio.app/guides/theming-guide">关于Theming的介绍</a><br>可以直接调用别人目前设计的主题模板，也可以自己上传CSS文件自定义。Gradio 具有内置主题引擎，可自定义应用的外观和感觉。只需要在代码中写上这一句<code>demo = gr.Interface(..., theme=gr.themes.Monochrome())</code>。<br>现有的一系列主题可以通过<code>gr.themes.*</code>进行调用，有：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">gr.themes.Base()<br>gr.themes.Default()<br>gr.themes.Glass()<br>gr.themes.Monochrome()<br>gr.themes.Soft()<br></code></pre></td></tr></table></figure><h4 id="1-2-8-1-Using-the-Theme-Builder"><a href="#1-2-8-1-Using-the-Theme-Builder" class="headerlink" title="1.2.8.1 Using the Theme Builder"></a>1.2.8.1 Using the Theme Builder</h4><p>然后基于这些现有的模板，你可以任意调整其中的一些类的属性</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">## 原先</span><br><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br>gr.themes.builder()<br><br><span class="hljs-comment">## 修改后</span><br><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br>theme = gr.themes.Base().<span class="hljs-built_in">set</span>(<br>    button_secondary_text_color_dark=<span class="hljs-string">&#x27;*secondary_900&#x27;</span><br>)<br><br><span class="hljs-keyword">with</span> gr.Blocks(theme=theme) <span class="hljs-keyword">as</span> demo:<br>    ...<br></code></pre></td></tr></table></figure><h4 id="1-2-8-1-Extending-Themes-via-the-Constructor"><a href="#1-2-8-1-Extending-Themes-via-the-Constructor" class="headerlink" title="1.2.8.1 Extending Themes via the Constructor"></a>1.2.8.1 Extending Themes via the Constructor</h4><p>Although each theme has hundreds of CSS variables, the values for most these variables are drawn from <strong>8 core variables</strong> which can be set through the constructor of each prebuilt theme. Modifying these 8 arguments allows you to quickly change the look and feel of your app.</p><ul><li>① Core Colors：<code>gradio.themes.Color</code> <ul><li>these Color objects hold brightness values for the palette of a single hue, ranging from 50, 100, 200…, 800, 900, 950.</li><li><strong>primary_hue</strong>: This is the color draws attention in your theme. In the default theme, this is set to <code>gradio.themes.colors.orange</code>.</li><li><strong>secondary_hue</strong>: This is the color that is used for secondary elements in your theme. In the default theme, this is set to <code>gradio.themes.colors.blue</code>.</li><li><strong>neutral_hue</strong>: This is the color that is used for text and other neutral elements in your theme. In the default theme, this is set to <code>gradio.themes.colors.gray</code>.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> gr.Blocks(theme=gr.themes.Default(primary_hue=<span class="hljs-string">&quot;red&quot;</span>, secondary_hue=<span class="hljs-string">&quot;pink&quot;</span>)) <span class="hljs-keyword">as</span> demo:<br><br><span class="hljs-comment">## 或者</span><br><span class="hljs-keyword">with</span> gr.Blocks(theme=gr.themes.Default(primary_hue=gr.themes.colors.red, secondary_hue=gr.themes.colors.pink)) <span class="hljs-keyword">as</span> demo:<br>    ...<br><br><span class="hljs-comment">## 或者modify the values of CSS variables after the theme has been loaded</span><br>theme = gr.themes.Default(primary_hue=<span class="hljs-string">&quot;blue&quot;</span>).<span class="hljs-built_in">set</span>(<br>    loader_color=<span class="hljs-string">&quot;#FF0000&quot;</span>,<br>    slider_color=<span class="hljs-string">&quot;#FF0000&quot;</span>,<br>)<br><br><span class="hljs-keyword">with</span> gr.Blocks(theme=theme) <span class="hljs-keyword">as</span> demo:<br>    ...<br><br><span class="hljs-comment">## 支持的颜色字符有</span><br>slate<br>gray<br>zinc<br>neutral<br>stone<br>red<br>orange<br>amber<br>yellow<br>lime<br>green<br>emerald<br>teal<br>cyan<br>sky<br>blue<br>indigo<br>violet<br>purple<br>fuchsia<br>pink<br>rose<br><br></code></pre></td></tr></table></figure><ul><li>② Core Sizing：<code>gradio.themes.Size</code> <ul><li>These Size objects hold pixel size values that range from xxs to xxl.</li><li><strong>spacing_size</strong>: <code>gradio.themes.sizes.spacing_md</code>.</li><li><strong>radius_size</strong>: <code>gradio.themes.sizes.radius_md</code>.</li><li><strong>text_size</strong>: <code>gradio.themes.sizes.text_md</code>.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> gr.Blocks(theme=gr.themes.Default(spacing_size=<span class="hljs-string">&quot;sm&quot;</span>, radius_size=<span class="hljs-string">&quot;none&quot;</span>)) <span class="hljs-keyword">as</span> demo:<br>    ...<br><span class="hljs-keyword">with</span> gr.Blocks(theme=gr.themes.Default(spacing_size=gr.themes.sizes.spacing_sm, radius_size=gr.themes.sizes.radius_none)) <span class="hljs-keyword">as</span> demo:<br>    ...<br><br><span class="hljs-comment">## 支持的size object有</span><br>radius_none<br>radius_sm<br>radius_md<br>radius_lg<br>spacing_sm<br>spacing_md<br>spacing_lg<br>text_sm<br>text_md<br>text_lg<br></code></pre></td></tr></table></figure><ul><li>③ Core Fonts：<code>gradio.themes.xxxFont</code> <ul><li>set the fonts of the theme.</li><li><strong>font</strong>: <code>gradio.themes.GoogleFont(&quot;Source Sans Pro&quot;)</code>.</li><li><strong>font_mono</strong>: sets the monospace font of the theme <code>gradio.themes.GoogleFont(&quot;IBM Plex Mono&quot;)</code>.</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> gr.Blocks(theme=gr.themes.Default(font=[gr.themes.GoogleFont(<span class="hljs-string">&quot;Inconsolata&quot;</span>), <span class="hljs-string">&quot;Arial&quot;</span>, <span class="hljs-string">&quot;sans-serif&quot;</span>])) <span class="hljs-keyword">as</span> demo:<br>    ...<br><span class="hljs-comment">## 支持的size object有</span><br>radius_none<br>radius_sm<br>radius_md<br>radius_lg<br>spacing_sm<br>spacing_md<br>spacing_lg<br>text_sm<br>text_md<br>text_lg<br></code></pre></td></tr></table></figure><h3 id="1-2-9-Progress-bars"><a href="#1-2-9-Progress-bars" class="headerlink" title="1.2.9 Progress bars"></a>1.2.9 Progress bars</h3><p>挺人性化的设计的，到哪都是进度条是吧( ´◔︎ ‸◔︎&#96;)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">slowly_reverse</span>(<span class="hljs-params">word, progress=gr.Progress(<span class="hljs-params"></span>)</span>):<br>    progress(<span class="hljs-number">0</span>, desc=<span class="hljs-string">&quot;Starting&quot;</span>)<br>    time.sleep(<span class="hljs-number">1</span>)<br>    progress(<span class="hljs-number">0.05</span>)<br>    new_string = <span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> letter <span class="hljs-keyword">in</span> progress.tqdm(word, desc=<span class="hljs-string">&quot;Reversing&quot;</span>):<br>        time.sleep(<span class="hljs-number">0.25</span>)<br>        new_string = letter + new_string<br>    <span class="hljs-keyword">return</span> new_string<br><br>demo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())<br><br>demo.launch()<br><br></code></pre></td></tr></table></figure><p> you can even report progress updates automatically from any tqdm.tqdm that already exists within your function by setting the default argument as gr.Progress(track_tqdm&#x3D;True)!</p><h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><p> add example<br>我们可以在页面下方添加供用户选择的测试样例。比如做了一个图像去噪算法，但是用户手头并没有躁点照片，example能让他更快的体验到效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>(<span class="hljs-params">name, intensity</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Hello, &quot;</span> + name + <span class="hljs-string">&quot;!&quot;</span> * <span class="hljs-built_in">int</span>(intensity)<br><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;slider&quot;</span>],<br>    outputs=[<span class="hljs-string">&quot;text&quot;</span>],<br>)<br><br>demo.launch()<br><br></code></pre></td></tr></table></figure><h1 id="2-在线托管平台-HuggingFace"><a href="#2-在线托管平台-HuggingFace" class="headerlink" title="2 在线托管平台 HuggingFace"></a>2 在线托管平台 HuggingFace</h1><p>前文提到过通过在代码中设置<code>demo.launch(share=True)</code>就能生成一个共享链接形如：<a href="https://07ff8706ab.gradio.live.虽然该链接是通过/">https://07ff8706ab.gradio.live。虽然该链接是通过</a> Gradio 共享服务器提供的免费公共 URL，但实际上跑代码的算力是我本地的服务器，同时共享链接将在 72 小时后过期。所以接下来的问题就是怎么在自己的云服务器上设置自己的共享服务器，这样才能突破链接的时间限制。而这就是HuggingFace可以干的事！Hugging Face Spaces 提供基础设施，进而可以免费永久托管机器学习模型！<br><a href="https://huggingface.co/welcome">使用服务前，先注册账号（官网界面右上角”Sign Up”）</a></p><h2 id="2-1-Hugging-Face-Spaces"><a href="#2-1-Hugging-Face-Spaces" class="headerlink" title="2.1 Hugging Face Spaces"></a>2.1 Hugging Face Spaces</h2><p><a href="https://huggingface.co/docs/hub/spaces">Spaces文档</a><br>在HuggingFace官网登录账号后，切换到Spaces创建一个新的空间假设叫“DemoName”。先选”Gradio”和”Private”，以生成一个可使用的Gradio在线应用：<code>https://huggingface.co/spaces/YourUserName/DemoName</code>。每个项目空间默认免费配备8个CPU核和16GB 运行内存，GPU资源需要单独付费。Each Spaces environment is limited to 16GB RAM, 2 CPU cores and 50GB of (not persistent) disk space by default.<del>感觉这个比github还适合进行代码备份</del></p><p>创建完Space之后，把本地预先构建好的项目文件（UI构建文件必须得命名成app.py且位于根目录）上传到该空间。具体方法与Github项目的上传和版本维护方式完全一样:</p><ul><li>克隆space项目到本地：<code>git clone https://huggingface.co/spaces/your_account/proj_name/tree/main</code></li><li>将本地已跑通的项目文件复制到刚才克隆的space项目文件夹</li><li>新建描述运行环境依赖的文件：<code>requirements.txt</code></li><li>指定Python依赖的包:<code>packages.txt</code></li><li>指定特殊的系统依赖配置</li><li><a href="https://huggingface.co/docs/hub/spaces-dependencies">详情参考</a></li><li>将此更新同步到远程仓库：<ul><li>git add -A .</li><li>git commit -m “add project files”</li><li>git push</li></ul></li></ul><p>完成以上步骤后（等待1~2分钟系统刷新），进入Space项目的App选项卡即可查看部署到web端的应用。</p><!-- （~~学了一阵发现这种方法不适合我，呜呜呜呜呜因为CPU好慢啊，跑一张图要1个小时~~）。那接下来需要实现的效果应该是本地运行 Gradio 应用程序，然后通过某个服务对每次运行的程序生成一个序列号，这个序列号最好是一个公网 URL 地址，这样服务器就会通过反向代理的方式将用户的请求转发到我本地的 Gradio 应用程序上。这种实现方式可以使用户在不知道具体本地部署的情况下，通过公共网址轻松地访问 Gradio 应用程序。而目前我的电脑相当于私有地址，那么外部网络中的其他计算机是无法访问我显卡上运行的 Gradio 应用程序。在这种情况下，要做的是端口转发技术：**将服务器的私有地址映射到一定的公开地址上，从而可以在外部网络中访问 Gradio 应用程序。**# 3 内外穿透https://blog.csdn.net/qq_15821487/article/details/131494881常用的一种端口转发技术是 NAT 网络地址转换（Network Address Translation），它可以将服务器的内部 IP 地址映射到一个公共 IP 地址上，并将访问该公共 IP 地址和端口的流量转发到服务器的内部 IP 地址和端口。在 Linux 系统中，可以使用 iptables 工具来进行端口转发和 NAT 配置。https://blog.csdn.net/wtyuong/article/details/137524656https://blog.csdn.net/xyl295528322/article/details/131995889https://blog.csdn.net/supperman_009/article/details/130432290https://blog.csdn.net/qq_42531954/article/details/133901919 -->]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tools</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch_《Deep learning with PyTorch》</title>
    <link href="/2024/04/22/2_Pytorch/"/>
    <url>/2024/04/22/2_Pytorch/</url>
    
    <content type="html"><![CDATA[<p>路漫漫其修远兮。本篇章主要记录的是清华翻译的《Deep learning with PyTorch》这本书中的重要内容，是Pytorch探索之旅的第1篇章！</p><span id="more"></span><p>Reference materials include:</p><ul><li><a href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch官方文档(中文版)</a>: 我感觉这个文档的好处在于非常详尽的给出相关函数和模块的细致化定义介绍</li><li><a href="https://pytorch.org/tutorials/">Tutorials</a>：感觉官网的这个文档是从Pytorch的使用框架去简要介绍</li><li>Andrew W. Traska撰写的<a href="https://www.manning.com/books/grokking-deep-learning">《Grokking Deep Learning》</a>是开发强大模型和理解深度神经网络基础机制的重要资源</li><li>Ian Goodfellow, Yoshua Bengio和Aaron Courville的<a href="https://www.deeplearningbook.org/">《Deep Learning》</a></li><li>清华翻译的<a href="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/">《Deep learning with PyTorch》</a></li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><strong>PyTorch是一个使用Python格式实现深度学习模型的库</strong>。它的核心数据结构Tensor，是一个多维数组。Pytorch就像是一个能在GPU上运行并且自带自动求导功能的Numpy数组。它配备了高性能的C++运行引擎使得他不必依赖Python的运行机制：</p><ul><li><strong>Pytorch</strong>的很大部分使用C++和CUDA（NVIDIA提供的类似C++的语言）编写。</li><li><strong>PyTorch</strong>核心是提供多维数组（tensor）的库， torch模块提供了对其进行扩展操作的库。</li><li>同时<strong>PyTorch</strong>的第二个核心功能是允许Tensor跟踪对其所执行的操作，并通过反向传播来计算输出相对于其任何输入的导数。</li></ul><p><strong>PyTorch</strong>最先实现了深度学习架构，深度学习模型强大的原因是因为它可以自动的学习样本输出与所期望输出之间的关系。</p><h2 id="1-1-PyTorch提供了构建和训练神经网络所需的所有模块："><a href="#1-1-PyTorch提供了构建和训练神经网络所需的所有模块：" class="headerlink" title="1.1 PyTorch提供了构建和训练神经网络所需的所有模块："></a>1.1 PyTorch提供了构建和训练神经网络所需的所有模块：</h2><ul><li>构建神经网络的核心模块位于<code>troch.nn</code>中。包括全连接层、卷积层、激活函数和损失函数。</li><li><code>torch.util.data</code>能找到适用于数据加载和处理的工具，相关的两个主要的类为Dataset和DataLoader。<br>  -<code>Dataset</code>承担了自定义的数据格式与标准的Pytorch张量之间的转换任务<br>  -<code>DataLoader</code>可以在后台生成子进程来从Dataset中加载数据，使数据准备就绪并在循环可以使用后立即等待训练循环<br>  -除此之外还可以使用专用的硬件（多个GPU）来训练模型，在这些情况下，可以通过<code>torch.nn.DataParallel</code>和<code>torch.distributed</code>来使用其他的可用硬件</li></ul><p>Pytorch使用Caffe2作为后端，增加了对ONNX的支持（定义了一种与深度学习库无关的模型描述和转换格式），增加了称为<strong>TorchScript</strong>的延迟执行图模式运行引擎（这个模块避开了Python解释器所带来的成本，我们可以将这个模型看作是具有针对张量操作的有限指令集的虚拟机，它的调用不会增加Python的开销，还能使PyTorch可以实时地将已知序列转换为更有效的混合操作）。默认的运行方式是即使执行（eager mode）。</p><p><strong>关于PyTorch的安装，windows建议使用Anaconda，Linux建议使用Pip。</strong></p><h1 id="2-从张量开始"><a href="#2-从张量开始" class="headerlink" title="2. 从张量开始"></a>2. 从张量开始</h1><p><strong>- Tensor是Pytorch最基本的数据结构</strong></p><p>深度学习的应用往往是将某种形式获取的数据（图像或文本）转换为另一种形式的数据（标签、数字或文本），因此从这个角度看，深度学习就像是构建一个将数据从一种表示转换为另一种表示的系统，这种转换是通过从一系列样本中提取共性来驱动的额，这些共性能够反映期望的映射关系。<br>为了实现上述过程，首先需要让网路理解输入数据，因此输入需要被转换为浮点数的集合。这些浮点数的集合及其操作是现代AI的核心。而网络层次之间的数据被视为中间表示（intermediate representation）。中间表示是将输入与前一层神经元权重相结合的结果，每个中间表示对于之前的输入都是唯一的。<br>为此，PyTorch引入了一个基本的数据结构：张量（tensor）。张量是指将向量（vector）和矩阵（matrix）推广到任意维度，与张量相同概念的另一个名称是多维数组（multidimensional array）。</p><h2 id="2-1-张量基础"><a href="#2-1-张量基础" class="headerlink" title="2.1 张量基础"></a>2.1 张量基础</h2><p>Python列表或数字元组（tuple）是在内存中单独分配的Python对象的集合；而PyTorch张量或NumPy数组（通常）是连续内存块上的视图（view），这些内存块存有未封装（unboxed）的C数值类型。<br><strong>获取一个张量的形状：tensor.shape</strong></p><h2 id="2-2-张量与存储"><a href="#2-2-张量与存储" class="headerlink" title="2.2 张量与存储"></a>2.2 张量与存储</h2><p>基础内存只分配一次，由torch.Storage实例管理，<strong>Storage是一个一维的数值数据数组</strong>，例如一块包含了指定类型（可能是float或int32）数字的连续内存块。<strong>而tensor可以看作是某个Storage实例的视图。</strong>因此多个tensor可以索引到同一Storage。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>points = torch.tensor([[<span class="hljs-number">1.0</span>, <span class="hljs-number">4.0</span>], [<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>], [<span class="hljs-number">3.0</span>, <span class="hljs-number">5.0</span>]])<br>points.storage()<br><br><span class="hljs-comment"># 输出</span><br> <span class="hljs-number">1.0</span><br> <span class="hljs-number">4.0</span><br> <span class="hljs-number">2.0</span><br> <span class="hljs-number">1.0</span><br> <span class="hljs-number">3.0</span><br> <span class="hljs-number">5.0</span><br>[torch.FloatStorage of size <span class="hljs-number">6</span>]<br></code></pre></td></tr></table></figure><p> <strong>无法使用两个索引来索引二维tensor的存储，因为Storage始终是一维的，与引用它的任何张量的维数无关。</strong><br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter2/2.4.png" alt="张量是一个存储实例的视图"></p><h2 id="2-3-尺寸、存储偏移与步长"><a href="#2-3-尺寸、存储偏移与步长" class="headerlink" title="2.3 尺寸、存储偏移与步长"></a>2.3 尺寸、存储偏移与步长</h2><p><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter2/2.5.png" alt="张量的尺寸、偏移与步长之间的关系"><br>tensor可以看作是某个Storage实例的视图。为了索引Storage，张量依赖于几条明确定义它们的信息：尺寸（size）、存储偏移（storage offset）和步长（stride）<br>    - 尺寸是一个元组，表示tensor每个维度上有多少个元素。<br>    - 存储偏移是Storage中与张量中的第一个元素相对应的索引。<br>        - 步长是在Storage中为了沿每个维度获取下一个元素而需要跳过的元素数量。<br>        - 步长是一个元组，表示当索引在每个维度上增加1时必须跳过的存储中元素的数量。<br>    - shape是属性，size()是类，这俩包含的信息相同。</p><p>用下标<code>i</code>和<code>j</code>访问二维张量等价于访问存储中的<code>storage_offset + stride[0] * i + stride[1] * j</code>元素。<br>张量Tensor和和存储Storage之间的这种间接操作会使某些操作（例如转置或提取子张量）的代价很小，因为<strong>它们不会导致内存重新分配</strong>；相反，它们（仅仅）分配一个新的张量对象，该对象具有不同的尺寸、存储偏移或步长。更改子张量同时也会对原始张量产生影响。所以我们可以克隆子张量得到新的张量（以避免这种影响）：<code>tensor.clone()</code>.Tensor的转置只改变尺寸和步长。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">some_tensor = torch.ones(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br>some_tensor.shape, some_tensor.stride()<br><span class="hljs-comment"># 输出</span><br>(torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]), (<span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>))<br><br>some_tensor_t = some_tensor.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>some_tensor_t.shape, some_tensor_t.stride()<br><span class="hljs-comment"># 输出</span><br>(torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>]), (<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">20</span>))<br><br></code></pre></td></tr></table></figure><h2 id="2-4-数据类型"><a href="#2-4-数据类型" class="headerlink" title="2.4 数据类型"></a>2.4 数据类型</h2><ul><li>torch.float32&#x2F;torch.float —— 32位浮点数:torch.FloatTensor &#x3D; torch.Tensor</li><li>torch.float64&#x2F;torch.double —— 64位双精度浮点数:torch.DoubleTensor</li><li>torch.float16&#x2F;torch.half —— 16位半精度浮点数</li><li>torch.int8 —— 带符号8位整数:torch.CharTensor</li><li>torch.uint8 —— 无符号8位整数:torch.ByteTensor</li><li>torch.int16&#x2F;torch.short —— 带符号16位整数</li><li>torch.int32&#x2F;torch.int —— 带符号32位整数</li><li>torch.int64&#x2F;torch.long —— 带符号64位整数</li></ul><p>通过访问<code>dtype</code>属性来获得张量的数据类型。而数据类型之间的转换可以通过<code>type</code>和<code>to</code>实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">short_points = torch.ones(<span class="hljs-number">10</span>, <span class="hljs-number">2</span>).short()<br>short_points = torch.ones(<span class="hljs-number">10</span>, <span class="hljs-number">2</span>).to(dtype=torch.short)<br></code></pre></td></tr></table></figure><h2 id="2-5-索引张量"><a href="#2-5-索引张量" class="headerlink" title="2.5 索引张量"></a>2.5 索引张量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">some_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>))<br>some_list[:]     <span class="hljs-comment"># 所有元素</span><br>some_list[<span class="hljs-number">1</span>:<span class="hljs-number">4</span>]   <span class="hljs-comment"># 第1（含）到第4（不含）个元素</span><br>some_list[<span class="hljs-number">1</span>:]    <span class="hljs-comment"># 第1（含）个之后所有元素</span><br>some_list[:<span class="hljs-number">4</span>]    <span class="hljs-comment"># 第4（不含）个之前所有元素</span><br>some_list[:-<span class="hljs-number">1</span>]   <span class="hljs-comment"># 最末尾（不含）元素之前所有元素</span><br>some_list[<span class="hljs-number">1</span>:<span class="hljs-number">4</span>:<span class="hljs-number">2</span>] <span class="hljs-comment"># 范围1（含）到4（不含），步长为2的元素</span><br><br>points[<span class="hljs-number">1</span>:]    <span class="hljs-comment"># 第1行及之后所有行，（默认）所有列</span><br>points[<span class="hljs-number">1</span>:, :] <span class="hljs-comment"># 第1行及之后所有行，所有列</span><br>points[<span class="hljs-number">1</span>:, <span class="hljs-number">0</span>] <span class="hljs-comment"># 第1行及之后所有行，仅第0列</span><br><br></code></pre></td></tr></table></figure><h2 id="2-6-Pytorch与NumPy的互通性"><a href="#2-6-Pytorch与NumPy的互通性" class="headerlink" title="2.6 Pytorch与NumPy的互通性"></a>2.6 Pytorch与NumPy的互通性</h2><p>从PyTorch张量创建NumPy数组：<code>points_np = points.numpy()</code><br>从NumPy数组创建PyTorch张量：<code>points = torch.from_numpy(points_np)</code></p><h2 id="2-7-序列化张量"><a href="#2-7-序列化张量" class="headerlink" title="2.7 序列化张量"></a>2.7 序列化张量</h2><p>PyTorch内部使用<code>pickle</code>来序列化张量对象和实现用于存储的专用序列化代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将points张量保存到ourpoints.t文件中</span><br>torch.save(points, <span class="hljs-string">&#x27;../../data/chapter2/ourpoints.t&#x27;</span>)<br><span class="hljs-comment"># 将points加载回来</span><br>points = torch.load(<span class="hljs-string">&#x27;../../data/chapter2/ourpoints.t&#x27;</span>)<br><br><span class="hljs-comment">## 上述例子可让你快速保存张量，但这个文件格式本身是不互通的，你只能用PyTorch读取它</span><br><span class="hljs-comment">## 对于需要（互通）的情况，你可以使用HDF5格式和库</span><br><span class="hljs-comment">## HDF5是一种可移植的、广泛支持的格式，用于表示以嵌套键值字典形式组织的序列化多维数组。</span><br><span class="hljs-comment">## Python通过h5py库支持HDF5，该库以NumPy数组的形式接收和返回数据。</span><br><br><span class="hljs-keyword">import</span> h5py<br><br>f = h5py.File(<span class="hljs-string">&#x27;../../data/chapter2/ourpoints.hdf5&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>dset = f.create_dataset(<span class="hljs-string">&#x27;coords&#x27;</span>, data=points.numpy())<br>f.close()<br><span class="hljs-comment"># coords是传入HDF5文件的键值</span><br><span class="hljs-comment"># 你可以索引在磁盘的数据并且仅访问你感兴趣的元素，例如你只想加载数据集中的最后两个点数据：</span><br>f = h5py.File(<span class="hljs-string">&#x27;../../data/chapter2/ourpoints.hdf5&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>)<br>dset = f[<span class="hljs-string">&#x27;coords&#x27;</span>]<br>last_points = dset[<span class="hljs-number">1</span>:]<br><br></code></pre></td></tr></table></figure><h2 id="2-8-将张量转移到GPU上运行"><a href="#2-8-将张量转移到GPU上运行" class="headerlink" title="2.8 将张量转移到GPU上运行"></a>2.8 将张量转移到GPU上运行</h2><p>PyTorch张量还具有设备（device）的概念，这是在设置计算机上放张量（tensor）数据的位置。</p><ul><li>通过为构造函数指定相应的参数，可以在GPU上创建张量：<code>points_gpu = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 4.0]], device=&#39;cuda&#39;)</code>；同时可以通过提供device和dtype参数来同时更改位置和数据类型</li><li>也可以使用<code>to</code>方法将在CPU上创建的张量（tensor）复制到GPU：<code>points_gpu = points.to(device=&#39;cuda&#39;)</code></li><li>使用速记方法<code>cpu</code>和<code>cuda</code>代替to方法来实现相同的目标：<code>points_gpu = points.cuda(0)</code>和<code>points_cpu = points_gpu.cpu()</code></li></ul><h1 id="3-使用张量表示真实数据"><a href="#3-使用张量表示真实数据" class="headerlink" title="3 使用张量表示真实数据"></a>3 使用张量表示真实数据</h1><p>将异构的现实世界数据编码成浮点数张量以供神经网络使用。</p><ul><li>表格数据：表格中的每一行都独立于其他行，他们的顺序页没有任何关系</li><li>时间序列：存在严格的排序其他类型的数据。比如文本和音频。</li><li>文本数据：将文本信息编码为张量形式的技术为独热编码。<br>每个字符将由一个长度等于编码中字符数的向量表示。该向量除了有一个元素是1外其他全为0，这个1的索引对应该字符在字符集中的位置。接下来，在编码中建立单词到索引的映射，一般单词作为键，而整数作为值。独热编码时，你将使用此词典来有效地找到单词的索引。但由于独热编码不支持长篇文本，后续出现了嵌入的方式处理文本。<br><strong>【关于文本嵌入】</strong><br>用于同一上下文的单词映射到嵌入空间的邻近区域。生成的嵌入的一个有趣的方面是，相似的词不仅会聚在一起，还会与其他词保持一致的空间关系。如果你要使用“苹果”的嵌入向量，并加上和减去其他词的嵌入向量，就可以进行类比，例如<code>苹果 - 红色 - 甜 + 酸</code>，最后可能得到一个类似<code>柠檬</code>的向量。</li><li>图像数据：图像表示为按规则网格排列的标量集合。PyTorch模块处理图像数据需要将张量设置为C x H x W（分别为通道、高度和宽度）</li><li>体积数据：在CT（Computed Tomography）扫描等医学成像应用程序的情况下，通常需要处理从头到脚方向堆叠的图像序列，每个序列对应于整个身体的横截面。在CT扫描中，强度代表身体不同部位的密度：肺、脂肪、水、肌肉、骨骼，以密度递增的顺序排列，当在临床工作站上显示CT扫描时，会从暗到亮映射。根据穿过人体后到达检测器的X射线量计算每个点的密度，并使用一些复杂的数学运算将原始传感器数据反卷积（deconvolve）为完整体积数据。CT具有单个的强度通道，这类似于灰度图像。通过将单个2D切片堆叠到3D张量中，你可以构建表示对象的3D解剖结构的体积数据。</li></ul><h1 id="4-学习机制"><a href="#4-学习机制" class="headerlink" title="4 学习机制"></a>4 学习机制</h1><p>数据科学的流程</p><ul><li>得到很多好的数据</li><li>试图将这些数据可视化</li><li>选择有可能拟合数据的最简单的模型</li><li>划分数据，以便处理部分数据并保留独立的数据集用来验证</li><li>从试探性的偏心率和大小开始，然后进行迭代直到模型拟合观察结果为止</li><li>根据独立的数据集验证他的模型</li></ul><p><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter4/4.2.png" alt="模型的学习过程"></p><h2 id="4-1-一个反向传播的简单示例"><a href="#4-1-一个反向传播的简单示例" class="headerlink" title="4.1 一个反向传播的简单示例"></a>4.1 一个反向传播的简单示例</h2><p>通过链式法则向后传播导数，可以计算复合函数（模型函数和损失函数）相对于它们的最内层参数<code>w</code>和<code>b</code>的梯度。基本的要求是涉及到的函数都是可微分的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">t_u, w, b</span>):<br>    <span class="hljs-keyword">return</span> w * t_u + b<br><br><span class="hljs-comment"># 定义损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss_fn</span>(<span class="hljs-params">t_p, t_c</span>):<br>    squared_diffs = (t_p - t_c)**<span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> squared_diffs.mean()<br><br><span class="hljs-comment"># 导数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dmodel_dw</span>(<span class="hljs-params">t_u, w, b</span>):<br>    <span class="hljs-keyword">return</span> t_u<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dmodel_db</span>(<span class="hljs-params">t_u, w, b</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span><br><br><span class="hljs-comment"># 返回相对于 w 和 b 的梯度的函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad_fn</span>(<span class="hljs-params">t_u, t_c, t_p, w, b</span>):<br>    dloss_dw = dloss_fn(t_p, t_c) * dmodel_dw(t_u, w, b)<br>    dloss_db = dloss_fn(t_p, t_c) * dmodel_db(t_u, w, b)<br>    <span class="hljs-keyword">return</span> torch.stack([dloss_dw.mean(), dloss_db.mean()])<br><br><span class="hljs-comment"># 循环训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">training_loop</span>(<span class="hljs-params">n_epochs, learning_rate, params, t_u, t_c, </span><br><span class="hljs-params">                    print_params = <span class="hljs-literal">True</span>, verbose=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_epochs + <span class="hljs-number">1</span>):<br>        w, b = params<br><br>        t_p = model(t_u, w, b) <span class="hljs-comment"># 前向传播</span><br>        loss = loss_fn(t_p, t_c)<br>        grad = grad_fn(t_u, t_c, t_p, w, b) <span class="hljs-comment"># 反向传播</span><br><br>        params = params - learning_rate * grad<br><br>        <span class="hljs-keyword">if</span> epoch % verbose == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch %d, Loss %f&#x27;</span> % (epoch, <span class="hljs-built_in">float</span>(loss)))<br>            <span class="hljs-keyword">if</span> print_params:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;    Params: &#x27;</span>, params)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;    Grad  : &#x27;</span>, grad)<br>    <span class="hljs-keyword">return</span> params<br><br></code></pre></td></tr></table></figure><h2 id="4-2-Pytorch自动求导"><a href="#4-2-Pytorch自动求导" class="headerlink" title="4.2 Pytorch自动求导"></a>4.2 Pytorch自动求导</h2><p>一般来讲，所有PyTorch张量都有一个初始为空的名为<code>grad</code>的属性:<code>params.grad is None # True</code>。你可以将包含任意数量的张量的<code>require_grad</code>设置为<code>True</code>以及组合任何函数。在这种情况下，PyTorch会在沿着整个函数链（即计算图）计算损失的导数，并在这些张量（即计算图的叶节点）的grad属性中将这些导数值累积（accumulate）起来。<br><strong>!!!!【Attention】</strong><br>重复调用backward会导致导数在叶节点处累积。因此，如果提前调用了backward，然后再次计算损失并再次调用backward（如在训练循环中一样），那么在每个叶节点上的梯度会被累积（即求和）在前一次迭代计算出的那个叶节点上，导致梯度值不正确。为防止这种情况发生，你需要在每次迭代时将梯度显式清零:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> params.grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    params.grad.zero_()<br></code></pre></td></tr></table></figure><p>torch模块有一个optim子模块，你可以在其中找到实现不同优化算法的类。这里有一个简短的清单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-built_in">dir</span>(optim)<br><br><span class="hljs-comment"># 输出</span><br>[<span class="hljs-string">&#x27;ASGD&#x27;</span>,<br> <span class="hljs-string">&#x27;Adadelta&#x27;</span>,<br> <span class="hljs-string">&#x27;Adagrad&#x27;</span>,<br> <span class="hljs-string">&#x27;Adam&#x27;</span>,<br> <span class="hljs-string">&#x27;AdamW&#x27;</span>,<br> <span class="hljs-string">&#x27;Adamax&#x27;</span>,<br> <span class="hljs-string">&#x27;LBFGS&#x27;</span>,<br> <span class="hljs-string">&#x27;Optimizer&#x27;</span>,<br> <span class="hljs-string">&#x27;RMSprop&#x27;</span>,<br> <span class="hljs-string">&#x27;Rprop&#x27;</span>,<br> <span class="hljs-string">&#x27;SGD&#x27;</span>,<br> <span class="hljs-string">&#x27;SparseAdam&#x27;</span>,<br> ...<br>]<br></code></pre></td></tr></table></figure><p>深度神经网络可以近似复杂的函数，前提是神经元的数量（即参数量）足够高。参数越少，网络能够近似的函数越简单。因此，这里有一条规律：如果训练损失没有减少，则该模型对于数据来说太简单了。另一种可能性是训练数据中不包含有意义的信息以用于预测输出。<br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter4/4.12.png" alt="关于过拟合的举例"></p><h1 id="5-使用神经网络拟合数据"><a href="#5-使用神经网络拟合数据" class="headerlink" title="5 使用神经网络拟合数据"></a>5 使用神经网络拟合数据</h1><p>不管具体模型是什么，参数的更新方式都是一样的：反向传播误差然后通过计算损失关于参数的梯度来更新这些参数。神经网络具有非凸误差曲面主要是因为激活函数。组合神经元来逼近各种复杂函数的能力取决于每个神经元固有的线性和非线性行为的组合。深度神经网络可让你近似高度非线性的过程，而无需为它们建立明确的模型。</p><h2 id="5-1-神经元"><a href="#5-1-神经元" class="headerlink" title="5.1 神经元"></a>5.1 神经元</h2><p>深度学习的核心是神经网络，即能够通过简单函数的组合来表示复杂函数的数学实体。这些复杂函数的基本组成单元是神经元，如图5.2所示。从本质上讲，神经元不过是输入的线性变换（例如，输入乘以一个数[weight，权重]，再加上一个常数[偏置，bias]），然后再经过一个固定的非线性函数（称为激活函数）。<br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter5/5.2.png" alt="神经元：线性变换后再经过一个非线性函数"><br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter5/5.3.png" alt="一个三层的神经网络"></p><h2 id="5-2-激活函数"><a href="#5-2-激活函数" class="headerlink" title="5.2 激活函数"></a>5.2 激活函数</h2><p>激活函数的作用是将先前线性运算的输出聚集到给定范围内。一系列线性变换紧跟可微激活函数中可以构建出能近似高度非线性过程的模型，且可以通过梯度下降很好地估计出其参数。<br>激活函数的要求有：</p><ul><li>非线性</li><li>可微<br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter5/5.5.png" alt="常用以及不是很常用的激活函数"></li></ul><h2 id="5-3-PyTorch的nn模块"><a href="#5-3-PyTorch的nn模块" class="headerlink" title="5.3 PyTorch的nn模块"></a>5.3 PyTorch的nn模块</h2><p>PyTorch有一个专门用于神经网络的完整子模块：<code>torch.nn</code>。该子模块包含创建各种神经网络体系结构所需的构建块。这些构建块在PyTorch术语中称为module（模块），在其他框架中称为layer（层）。nn中的任何模块都被编写成同时产生一个批次（即多个输入）的输出。这样进行批处理的主要原因是希望可以充分利用执行计算的计算资源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. nn提供了一种通过nn.Sequential容器串联模块的简单方法</span><br>seq_model = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">1</span>, <span class="hljs-number">13</span>),<br>            nn.Tanh(),<br>            nn.Linear(<span class="hljs-number">13</span>, <span class="hljs-number">1</span>))<br>seq_model<br><span class="hljs-comment">## 输出</span><br>Sequential(<br>  (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">1</span>, out_features=<span class="hljs-number">13</span>, bias=<span class="hljs-literal">True</span>)<br>  (<span class="hljs-number">1</span>): Tanh()<br>  (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">13</span>, out_features=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>)<br>)<br><span class="hljs-comment"># ——————————————————————————————————————————————————————————————————</span><br><br><span class="hljs-comment">## 1.1 Sequential中每个模块的名称都是该模块在参数中出现的顺序。</span><br><span class="hljs-comment">##      有趣的是，Sequential还可以接受OrderedDict作为参数</span><br><span class="hljs-comment">#       这样就可以给Sequential的每个模块命名</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br><br>seq_model = nn.Sequential(OrderedDict([<br>    (<span class="hljs-string">&#x27;hidden_linear&#x27;</span>, nn.Linear(<span class="hljs-number">1</span>, <span class="hljs-number">8</span>)),<br>    (<span class="hljs-string">&#x27;hidden_activation&#x27;</span>, nn.Tanh()),<br>    (<span class="hljs-string">&#x27;output_linear&#x27;</span>, nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>))<br>]))<br><br>seq_model<br><span class="hljs-comment">## 输出</span><br>Sequential(<br>  (hidden_linear): Linear(in_features=<span class="hljs-number">1</span>, out_features=<span class="hljs-number">8</span>, bias=<span class="hljs-literal">True</span>)<br>  (hidden_activation): Tanh()<br>  (output_linear): Linear(in_features=<span class="hljs-number">8</span>, out_features=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>)<br>)<br><span class="hljs-comment"># ——————————————————————————————————————————————————————————————————</span><br><br><br><span class="hljs-comment"># 2. 调用model.parameters()可以得到第一线性模块和第二线性模块中的权重和偏差。</span><br><span class="hljs-comment">## 在本例中，我们可以通过打印形状来检查参数：</span><br>[param.shape <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> seq_model.parameters()]<br><br><span class="hljs-comment">## 输出</span><br>[torch.Size([<span class="hljs-number">13</span>, <span class="hljs-number">1</span>]), torch.Size([<span class="hljs-number">13</span>]), torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">13</span>]), torch.Size([<span class="hljs-number">1</span>])]<br><span class="hljs-comment"># ——————————————————————————————————————————————————————————————————</span><br><br><span class="hljs-comment"># 3. 当你检查由几个子模块组成的模型的参数时，可以方便地通过其名称识别参数。这个方法叫做named_parameters</span><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> seq_model.named_parameters():<br>    <span class="hljs-built_in">print</span>(name, param.shape)<br><br><span class="hljs-comment">##  输出</span><br><span class="hljs-number">0.</span>weight torch.Size([<span class="hljs-number">13</span>, <span class="hljs-number">1</span>])<br><span class="hljs-number">0.</span>bias torch.Size([<span class="hljs-number">13</span>])<br><span class="hljs-number">2.</span>weight torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">13</span>])<br><span class="hljs-number">2.</span>bias torch.Size([<span class="hljs-number">1</span>])<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>PyTorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
