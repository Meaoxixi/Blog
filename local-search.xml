<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Manim</title>
    <link href="/2024/06/01/5_Manim/"/>
    <url>/2024/06/01/5_Manim/</url>
    
    <content type="html"><![CDATA[<p>本篇博客记录使用Manim引擎渲染好看的动画的学习过程。</p><span id="more"></span><p>在学线性代数的过程对Grant Sanderson大佬自制的manim数学引擎制作的动画秀到了，我也学了一下，python的图像处理真是太奇妙了！</p><p>Reference materials include:</p><ul><li><a href="https://zhuanlan.zhihu.com/p/108839666">知乎关于Manim使用的解说</a></li><li><a href="https://github.com/3b1b/manim">github上的Manim源码</a></li><li><a href="https://docs.manim.community/en/stable/faq/installation.html#different-versions">Manim 英文文档</a></li><li><a href="https://docs.manim.org.cn/">Manim 中文文档</a></li><li>Todd Zimmerman教授写的针对<a href="https://talkingphysics.wordpress.com/2019/01/08/getting-started-animating-with-manim-and-python-3-7/">Manim的使用教程</a>和他整理的<a href="https://github.com/zimmermant/manim_tutorial">github代码</a></li></ul><hr><p>3B1B 动画的制作思路是：根据自己想在场景中展现的内容和效果编写一系列的类，然后通过命令行对每个类进行实例化，前面输入的测试命令其实就包含了类的实例化过程，而每个类被实例化后都将得到一个动画片段，通过视频制作软件将各个片段衔接起来并配音，就能得到大家喜闻乐见的 3B1B 教学动画了。</p><h1 id="1-弹出的窗口中会播放一个绘制正方形并变换为圆的动画"><a href="#1-弹出的窗口中会播放一个绘制正方形并变换为圆的动画" class="headerlink" title="1. 弹出的窗口中会播放一个绘制正方形并变换为圆的动画"></a>1. 弹出的窗口中会播放一个绘制正方形并变换为圆的动画</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> manimlib <span class="hljs-keyword">import</span> *<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">SquareToCircle</span>(<span class="hljs-title class_ inherited__">Scene</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">construct</span>(<span class="hljs-params">self</span>):<br>        circle = Circle()<br>        circle.set_fill(BLUE, opacity=<span class="hljs-number">0.5</span>)<br>        circle.set_stroke(BLUE_E, width=<span class="hljs-number">4</span>)<br>        square = Square()<br><br>        self.play(ShowCreation(square))<br>        self.wait()<br>        self.play(ReplacementTransform(square, circle))<br>        self.wait()<br>        <br>        <span class="hljs-comment"># 在代码的末尾加上如下一行来启用交互</span><br>        <span class="hljs-comment"># self.embed()</span><br>        <span class="hljs-comment">## # 在水平方向上拉伸到四倍</span><br>        <span class="hljs-comment">## play(circle.animate.stretch(4, dim=0))</span><br>        <span class="hljs-comment"># # 旋转90°</span><br>        <span class="hljs-comment"># play(Rotate(circle, TAU / 4))</span><br>        <span class="hljs-comment"># # 在向右移动2单位同时缩小为原来的1/4</span><br>        <span class="hljs-comment"># play(circle.animate.shift(2 * RIGHT), circle.animate.scale(0.25))</span><br>        <span class="hljs-comment"># # 为了非线性变换，给circle增加10段曲线（不会播放动画）</span><br>        <span class="hljs-comment"># circle.insert_n_curves(10)</span><br>        <span class="hljs-comment"># # 给circle上的所有点施加f(z)=z^2的复变换</span><br>        <span class="hljs-comment"># play(circle.animate.apply_complex_function(lambda z: z**2))</span><br>        <span class="hljs-comment"># # 关闭窗口并退出程序</span><br>        <span class="hljs-comment"># exit()</span><br><br><span class="hljs-comment"># 运行指令</span><br>manimgl start.py SquareToCircle <br><span class="hljs-comment"># 运行并保存</span><br>manimgl start.py SquareToCircle -ow<br></code></pre></td></tr></table></figure><h1 id="2-命令行参数和配置"><a href="#2-命令行参数和配置" class="headerlink" title="2 命令行参数和配置"></a>2 命令行参数和配置</h1><p><code>manimgl &lt;code&gt;.py &lt;Scene&gt; &lt;flags&gt;</code><br>    - <code>.py : 你写的 python 文件<br>    - <Scene> : 你想要渲染的场景'<br>    - <flags> : 传入的选项<br>        - <code>-w</code> 把场景写入文件<br>        - <code>-o</code> 把场景写入文件并打开<br>        - <code>-s</code> 跳到最后只展示最后一帧<br>            - <code>-so</code> 保存最后一帧并打开<br>        - <code>-n</code> <number> 跳到场景中第 n 个动画<br>        - <code>-f</code> 打开窗口全屏</p><p>local configuration file <code>D:\Manim\manim-master\custom_config.yml</code>. You can manually modify it.也可以对于不同目录使用不同的 custom_config.yml,子文件夹下的配置文件会在运行时覆盖根目录下的总局配置文件。</p><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs stylus">manim/<br>├── manimlib/<br>│   ├── <span class="hljs-attribute">animation</span>/<br>│   ├── ...<br>│   ├── default_config<span class="hljs-selector-class">.yml</span><br>│   └── window<span class="hljs-selector-class">.py</span><br>├── project/<br>│   ├── <span class="hljs-selector-tag">code</span><span class="hljs-selector-class">.py</span><br>│   └── custom_config<span class="hljs-selector-class">.yml</span><br>└── custom_config.yml<br></code></pre></td></tr></table></figure><h1 id="3-样例学习"><a href="#3-样例学习" class="headerlink" title="3 样例学习"></a>3 样例学习</h1><h2 id="3-1-Animating-Methods"><a href="#3-1-Animating-Methods" class="headerlink" title="3.1  Animating Methods"></a>3.1  Animating Methods</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">AnimatingMethods</span>(<span class="hljs-title class_ inherited__">Scene</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">construct</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># .get_grid() 方法会返回一个由该物体复制得到的阵列</span><br>        grid = Tex(<span class="hljs-string">r&quot;\pi&quot;</span>).get_grid(<span class="hljs-number">10</span>, <span class="hljs-number">10</span>, height=<span class="hljs-number">4</span>)<br>        self.add(grid)<br><br>        <span class="hljs-comment"># 你可以通过.animate语法来动画化物件变换方法</span><br>        self.play(grid.animate.shift(LEFT))<br><br>        <span class="hljs-comment"># 或者你可以使用旧的语法，把方法和参数同时传给Scene.play</span><br>        self.play(grid.shift, LEFT)<br><br>        <span class="hljs-comment"># 这两种方法都会在mobject的初始状态和应用该方法后的状态间进行插值</span><br>        <span class="hljs-comment"># 在本例中，调用grid.shift(LEFT)会将grid向左移动一个单位</span><br><br>        <span class="hljs-comment"># 这种用法可以用在任何方法上，包括设置颜色</span><br>        self.play(grid.animate.set_color(YELLOW))<br>        self.wait()<br>        self.play(grid.animate.set_submobject_colors_by_gradient(BLUE, GREEN))<br>        self.wait()<br>        self.play(grid.animate.set_height(TAU - MED_SMALL_BUFF))<br>        self.wait()<br><br>        <span class="hljs-comment"># 方法Mobject.apply_complex_function允许应用任意的复函数</span><br>        <span class="hljs-comment"># 将把Mobject的所有点的坐标看作复数</span><br><br>        self.play(grid.animate.apply_complex_function(np.exp), run_time=<span class="hljs-number">5</span>)<br>        self.wait()<br><br>        <span class="hljs-comment"># 更一般地说，你可以应用Mobject.apply方法，它接受从R^3到R^3的一个函数</span><br>        self.play(<br>            grid.animate.apply_function(<br>                <span class="hljs-keyword">lambda</span> p: [<br>                    p[<span class="hljs-number">0</span>] + <span class="hljs-number">0.5</span> * math.sin(p[<span class="hljs-number">1</span>]),<br>                    p[<span class="hljs-number">1</span>] + <span class="hljs-number">0.5</span> * math.sin(p[<span class="hljs-number">0</span>]),<br>                    p[<span class="hljs-number">2</span>]<br>                ]<br>            ),<br>            run_time=<span class="hljs-number">5</span>,<br>        )<br>        self.wait()<br></code></pre></td></tr></table></figure><p><a href="https://docs.manim.org.cn/getting_started/example_scenes.html">https://docs.manim.org.cn/getting_started/example_scenes.html</a></p>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tools</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Linear Algebra</title>
    <link href="/2024/06/01/4_Linear-Algebra/"/>
    <url>/2024/06/01/4_Linear-Algebra/</url>
    
    <content type="html"><![CDATA[<p>本篇博客记录线性代数相关的学习。</p><span id="more"></span><p>到现在，感觉世界的本质就是物理，物理学真是永远的白月光。而数学是将世界进行抽象化表示的工具，本科对于线代的学习还是囫囵吞枣了，所以打算集合多个收罗到的资料重新过一遍，加深对线代几何意义的理解。</p><p>Reference materials include:</p><ul><li><a href="https://intuitive-math.club/linear-algebra/spaces">A visual intuition</a></li><li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">3Blue1Brown：Essence of linear algebra（youtube）</a></li><li><a href="https://www.bilibili.com/video/BV1Ys411k7yQ/">3Blue1Brown：Essence of linear algebra（bilibili）</a></li></ul><h1 id="1-向量"><a href="#1-向量" class="headerlink" title="1 向量"></a>1 向量</h1><p>什么是向量？</p><ol><li>（物理学）向量是空间中的箭头，关键在于箭头的长度和方向</li><li>（计算机）向量是有序的数字列表</li><li>（数学）向量可以是任何东西，只需要保证两个向量相加以及数字与向量相乘是有意义的即可</li></ol><p>原点可以看做空间的中心和所有向量的起点。向量的两个基础运算分别是向量加法和向量数乘。线性代数为数据分析提供了一条将大量数据列表概念化、可视化的渠道，，它让数据样式变得非常明晰，并让你大致了解特定运算的意义；另一方面线性代数可以是一种通过计算机能处理的数字来描述并操作空间的语言。</p><h1 id="2-线性组合、跨度和基向量"><a href="#2-线性组合、跨度和基向量" class="headerlink" title="2 线性组合、跨度和基向量"></a>2 线性组合、跨度和基向量</h1><p>xy坐标有两个非常特殊的向量：</p><ul><li>一个指向正右方，长度为1，通常被称为<code>i帽</code>或者x方向的单位向量</li><li>一个指向正上方，长度为1，通常被称为<code>j帽</code>或者y方向的单位向量</li><li><strong>i帽和j帽合起来被称为坐标系的基</strong></li></ul><p>两个数乘向量的和被称为这两个向量的线性组合。如果固定其中一个标量，而让另一个标量自由变化，所产生的向量的终点会描出一条直线（这就是“线性的”的由来）。而两个向量张成的空间是指仅通过向量加法与向量数乘这两种基础运算所能获得的集合。</p><h2 id="Vectors-vs-Points"><a href="#Vectors-vs-Points" class="headerlink" title="Vectors vs. Points"></a>Vectors vs. Points</h2><p>我们通常用向量的终点代表该向量，他的起点位于原点。</p><p>当你有多个向量，并且可以移除其中一个而不减小向量张成的空间大小，我们称它们<strong>线性相关</strong>。另一种表述是说，其中一个向量可以表示为其他向量的线性组合，因为这个向量已经落在其它向量张成的空间之中。另一方面，如果所有向量都给张成的空间增添了新的维度，他们被称为<strong>线性无关</strong>的。<br><strong>The basis of a vector space is a set of linearly independent vectors that span the full space</strong></p><h1 id="3-矩阵与空间变换"><a href="#3-矩阵与空间变换" class="headerlink" title="3 矩阵与空间变换"></a>3 矩阵与空间变换</h1><p>我们称满足以下的变换为线性变换（保持网格线平行且等距分布的变换）：</p><ul><li>直线在变换后仍保持为直线，没有弯曲</li><li>原点固定</li></ul><p>线性变换是操作空间的一种手段。</p><h1 id="4-矩阵乘法作为复合"><a href="#4-矩阵乘法作为复合" class="headerlink" title="4 矩阵乘法作为复合"></a>4 矩阵乘法作为复合</h1>]]></content>
    
    
    <categories>
      
      <category>Math</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Math</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch_《Tutorials》</title>
    <link href="/2024/05/29/1_Pytorch_1/"/>
    <url>/2024/05/29/1_Pytorch_1/</url>
    
    <content type="html"><![CDATA[<p>本篇章主要记录的是官网提供的<a href="https://pytorch.org/tutorials/">Tutorials</a>和<a href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch官方文档(中文版)</a>的重要内容，是Pytorch探索之旅的第2篇章啦！</p><span id="more"></span><p>Reference materials include:</p><ul><li><a href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch官方文档(中文版)</a>: 我感觉这个文档的好处在于非常详尽的给出相关函数和模块的细致化定义介绍</li><li><a href="https://pytorch.org/tutorials/">Tutorials</a>：感觉官网的这个文档是从Pytorch的使用框架去简要介绍</li><li>Andrew W. Traska撰写的<a href="https://www.manning.com/books/grokking-deep-learning">《Grokking Deep Learning》</a>是开发强大模型和理解深度神经网络基础机制的重要资源</li><li>Ian Goodfellow, Yoshua Bengio和Aaron Courville的<a href="https://www.deeplearningbook.org/">《Deep Learning》</a></li><li>清华翻译的<a href="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/">《Deep learning with PyTorch》</a></li></ul><h1 id="1-Quickstart"><a href="#1-Quickstart" class="headerlink" title="1 Quickstart"></a>1 Quickstart</h1><h2 id="1-1-Working-with-data"><a href="#1-1-Working-with-data" class="headerlink" title="1.1 Working with data"></a>1.1 Working with data</h2><p>PyTorch has two primitives to work with data: <code>torch.utils.data.DataLoader</code> and <code>torch.utils.data.Dataset</code>. <strong>Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset.</strong> PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO (<a href="https://pytorch.org/vision/stable/datasets.html">full list here</a>).<br>Every TorchVision Dataset includes two arguments: <code>transform</code> and <code>target_transform</code> to modify the samples and labels respectively.</p><h2 id="1-2-Creating-Models"><a href="#1-2-Creating-Models" class="headerlink" title="1.2 Creating Models"></a>1.2 Creating Models</h2><p>To define a neural network in PyTorch, we create a class that inherits from nn.Module. </p><ul><li>We define the layers of the network in the <code>__init__</code> function</li><li>Specify how data will pass through the network in the <code>forward</code> function</li><li>To accelerate operations in the neural network, we move it to the GPU or MPS if available.</li></ul><h2 id="1-3-Optimizing-the-Model-Parameters"><a href="#1-3-Optimizing-the-Model-Parameters" class="headerlink" title="1.3 Optimizing the Model Parameters"></a>1.3 Optimizing the Model Parameters</h2><p>To train a model, we need a loss function and an optimizer.</p><h2 id="1-4-Saving-Models"><a href="#1-4-Saving-Models" class="headerlink" title="1.4 Saving Models"></a>1.4 Saving Models</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(model.state_dict(), <span class="hljs-string">&quot;model.pth&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Saved PyTorch Model State to model.pth&quot;</span>)<br></code></pre></td></tr></table></figure><h2 id="1-5-Loading-Models"><a href="#1-5-Loading-Models" class="headerlink" title="1.5 Loading Models"></a>1.5 Loading Models</h2><p>The process for loading a model includes re-creating the model structure and loading the state dictionary into it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = NeuralNetwork().to(device)<br>model.load_state_dict(torch.load(<span class="hljs-string">&quot;model.pth&quot;</span>))<br></code></pre></td></tr></table></figure><h1 id="2-Tensors"><a href="#2-Tensors" class="headerlink" title="2 Tensors"></a>2 Tensors</h1><p>Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators.</p><h2 id="2-1-Initializing-a-Tensor"><a href="#2-1-Initializing-a-Tensor" class="headerlink" title="2.1 Initializing a Tensor"></a>2.1 Initializing a Tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. Directly from data</span><br>data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br>x_data = torch.tensor(data)<br><br><span class="hljs-comment"># 2. From a NumPy array</span><br>np_array = np.array(data)<br>x_np = torch.from_numpy(np_array)<br><br><span class="hljs-comment"># 3. From another tensor</span><br>x_ones = torch.ones_like(x_data) <span class="hljs-comment"># retains the properties of x_data</span><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># overrides the datatype of x_data</span><br><br><span class="hljs-comment"># 4. With random or constant values</span><br>shape = (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br></code></pre></td></tr></table></figure><h2 id="2-2-Attributes-of-a-Tensor"><a href="#2-2-Attributes-of-a-Tensor" class="headerlink" title="2.2 Attributes of a Tensor"></a>2.2 Attributes of a Tensor</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pythohn">print(f&quot;Shape of tensor: &#123;tensor.shape&#125;&quot;)<br>print(f&quot;Datatype of tensor: &#123;tensor.dtype&#125;&quot;)<br>print(f&quot;Device tensor is stored on: &#123;tensor.device&#125;&quot;)<br></code></pre></td></tr></table></figure><h2 id="2-3-Operations-on-Tensor"><a href="#2-3-Operations-on-Tensor" class="headerlink" title="2.3 Operations on Tensor"></a>2.3 Operations on Tensor</h2><p>Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described <a href="https://pytorch.org/docs/stable/torch.html">here</a>.<br>By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># We move our tensor to the GPU if available</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    tensor = tensor.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br><br><span class="hljs-comment"># 1 Standard numpy-like indexing and slicing</span><br>tensor = torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First row: <span class="hljs-subst">&#123;tensor[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First column: <span class="hljs-subst">&#123;tensor[:, <span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Last column: <span class="hljs-subst">&#123;tensor[..., -<span class="hljs-number">1</span>]&#125;</span>&quot;</span>)<br>tensor[:,<span class="hljs-number">1</span>] = <span class="hljs-number">0</span><br><span class="hljs-built_in">print</span>(tensor)<br><span class="hljs-comment">## 输出</span><br>First row: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>First column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>Last column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br><br><span class="hljs-comment"># 2 Joining tensors</span><br><span class="hljs-comment"># - `torch.cat`: Concatenates the given sequence along an existing dimension.</span><br><span class="hljs-comment"># - `torch.stack`: Concatenates a sequence of tensors along a new dimension.</span><br><br><span class="hljs-comment"># 3 Single-element tensors </span><br><span class="hljs-comment"># If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using `item()`</span><br></code></pre></td></tr></table></figure><h2 id="2-4-Bridge-with-NumPy"><a href="#2-4-Bridge-with-NumPy" class="headerlink" title="2.4 Bridge with NumPy"></a>2.4 Bridge with NumPy</h2><p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Tensor to NumPy array</span><br>n = t.numpy()<br><span class="hljs-comment"># NumPy array to Tensor</span><br>t = torch.from_numpy(n)<br><br><span class="hljs-comment">## Changes in the NumPy array reflects in the tensor.</span><br>n = np.ones(<span class="hljs-number">5</span>)<br>t = torch.from_numpy(n)<br>np.add(n, <span class="hljs-number">1</span>, out=n)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br><span class="hljs-comment">### 输出</span><br>t: tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>], dtype=torch.float64)<br>n: [<span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span>]<br></code></pre></td></tr></table></figure><h1 id="3-Datasets-and-DataLoaders"><a href="#3-Datasets-and-DataLoaders" class="headerlink" title="3 Datasets and DataLoaders"></a>3 Datasets and DataLoaders</h1><h2 id="3-1-Loading-a-Dataset"><a href="#3-1-Loading-a-Dataset" class="headerlink" title="3.1 Loading a Dataset"></a>3.1 Loading a Dataset</h2><p>We can load the FashionMNIST Dataset with the following parameters:</p><ul><li>root is the path where the train&#x2F;test data is stored,</li><li>train specifies training or test dataset,</li><li>download&#x3D;True downloads the data from the internet if it’s not available at root.</li><li>transform and target_transform specify the feature and label transformations<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><br>training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br><br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></table></figure></li></ul><h2 id="3-2-Iterating-and-Visualizing-the-Dataset"><a href="#3-2-Iterating-and-Visualizing-the-Dataset" class="headerlink" title="3.2 Iterating and Visualizing the Dataset"></a>3.2 Iterating and Visualizing the Dataset</h2><p>We can index Datasets manually like a list: training_data[index]. We use matplotlib to visualize some samples in our training data.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">labels_map = &#123;<br>    <span class="hljs-number">0</span>: <span class="hljs-string">&quot;T-Shirt&quot;</span>,<br>    <span class="hljs-number">1</span>: <span class="hljs-string">&quot;Trouser&quot;</span>,<br>    <span class="hljs-number">2</span>: <span class="hljs-string">&quot;Pullover&quot;</span>,<br>    <span class="hljs-number">3</span>: <span class="hljs-string">&quot;Dress&quot;</span>,<br>    <span class="hljs-number">4</span>: <span class="hljs-string">&quot;Coat&quot;</span>,<br>    <span class="hljs-number">5</span>: <span class="hljs-string">&quot;Sandal&quot;</span>,<br>    <span class="hljs-number">6</span>: <span class="hljs-string">&quot;Shirt&quot;</span>,<br>    <span class="hljs-number">7</span>: <span class="hljs-string">&quot;Sneaker&quot;</span>,<br>    <span class="hljs-number">8</span>: <span class="hljs-string">&quot;Bag&quot;</span>,<br>    <span class="hljs-number">9</span>: <span class="hljs-string">&quot;Ankle Boot&quot;</span>,<br>&#125;<br>figure = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>cols, rows = <span class="hljs-number">3</span>, <span class="hljs-number">3</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, cols * rows + <span class="hljs-number">1</span>):<br>    sample_idx = torch.randint(<span class="hljs-built_in">len</span>(training_data), size=(<span class="hljs-number">1</span>,)).item()<br>    img, label = training_data[sample_idx]<br>    figure.add_subplot(rows, cols, i)<br>    plt.title(labels_map[label])<br>    plt.axis(<span class="hljs-string">&quot;off&quot;</span>)<br>    plt.imshow(img.squeeze(), cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure><h2 id="3-3-Creating-a-Custom-Dataset-for-your-files"><a href="#3-3-Creating-a-Custom-Dataset-for-your-files" class="headerlink" title="3.3 Creating a Custom Dataset for your files"></a>3.3 Creating a Custom Dataset for your files</h2><p>A custom Dataset class must implement three functions: <code>__init__</code>, <code>__len__</code>, and <code>__getitem__</code>.<br>The FashionMNIST images are stored in a directory <code>img_dir</code>, and their labels are stored separately in a CSV file <code>annotations_file</code>.<br>The labels.csv file looks like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tshirt1.jpg, <span class="hljs-number">0</span><br>tshirt2.jpg, <span class="hljs-number">0</span><br>......<br>ankleboot999.jpg, <span class="hljs-number">9</span><br></code></pre></td></tr></table></figure><p>Code as fellow:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomImageDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-comment"># The __init__ function is run once when instantiating the Dataset object.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, annotations_file, img_dir, transform=<span class="hljs-literal">None</span>, target_transform=<span class="hljs-literal">None</span></span>):<br>        self.img_labels = pd.read_csv(annotations_file)<br>        self.img_dir = img_dir<br>        self.transform = transform<br>        self.target_transform = target_transform<br><br>    <span class="hljs-comment"># The __len__ function returns the number of samples in our dataset.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_labels)<br><br>    <span class="hljs-comment"># The __getitem__ function loads and returns a sample from the dataset at the given index idx.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-comment"># Based on the index, it identifies the image’s location on disk</span><br>        <span class="hljs-comment"># converts that to a tensor using read_image</span><br>        <span class="hljs-comment"># retrieves the corresponding label from the csv data in self.img_labels</span><br>        <span class="hljs-comment"># calls the transform functions on them (if applicable)</span><br>        <span class="hljs-comment"># and returns the tensor image and corresponding label in a tuple.</span><br>        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="hljs-number">0</span>])<br>        image = read_image(img_path)<br>        label = self.img_labels.iloc[idx, <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> self.transform:<br>            image = self.transform(image)<br>        <span class="hljs-keyword">if</span> self.target_transform:<br>            label = self.target_transform(label)<br>        <span class="hljs-keyword">return</span> image, label<br></code></pre></td></tr></table></figure><h2 id="3-4-Preparing-your-data-for-training-with-DataLoaders"><a href="#3-4-Preparing-your-data-for-training-with-DataLoaders" class="headerlink" title="3.4 Preparing your data for training with DataLoaders"></a>3.4 Preparing your data for training with DataLoaders</h2><p>The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><h2 id="3-5-Iterate-through-the-DataLoader"><a href="#3-5-Iterate-through-the-DataLoader" class="headerlink" title="3.5 Iterate through the DataLoader"></a>3.5 Iterate through the DataLoader</h2><p>We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns <strong>a batch of train_features and train_labels</strong> (containing batch_size&#x3D;64 features and labels respectively). Because we specified shuffle&#x3D;True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Display image and label.</span><br>train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Feature batch shape: <span class="hljs-subst">&#123;train_features.size()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Labels batch shape: <span class="hljs-subst">&#123;train_labels.size()&#125;</span>&quot;</span>)<br>img = train_features[<span class="hljs-number">0</span>].squeeze()<br>label = train_labels[<span class="hljs-number">0</span>]<br>plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Label: <span class="hljs-subst">&#123;label&#125;</span>&quot;</span>)<br><br><span class="hljs-comment">## 输出</span><br><span class="hljs-comment">## 一张图示</span><br>Feature batch shape: torch.Size([<span class="hljs-number">64</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Labels batch shape: torch.Size([<span class="hljs-number">64</span>])<br>Label: <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure><h1 id="4-Transformers"><a href="#4-Transformers" class="headerlink" title="4 Transformers"></a>4 Transformers</h1><p>We use transforms to perform some manipulation of the data and make it suitable for training. All TorchVision datasets have two parameters <code>transform</code> to modify the features and <code>target_transform</code> to modify the labels that accept callables containing the transformation logic. <a href="https://pytorch.org/vision/stable/transforms.html">More resource in torchvision.transforms module </a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor, Lambda<br><br>ds = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor(),<br>    <span class="hljs-comment"># Lambda transforms apply any user-defined lambda function. </span><br>    target_transform=Lambda(<span class="hljs-keyword">lambda</span> y: torch.zeros(<span class="hljs-number">10</span>, dtype=torch.<span class="hljs-built_in">float</span>).scatter_(<span class="hljs-number">0</span>, torch.tensor(y), value=<span class="hljs-number">1</span>))<br>)<br></code></pre></td></tr></table></figure><h1 id="5-Build-Model"><a href="#5-Build-Model" class="headerlink" title="5 Build Model"></a>5 Build Model</h1><ul><li>The <code>nn.Flatten</code> layer converts each 2D 28x28 image into a contiguous array of 784 pixel values.</li><li>The <code>nn.Linear</code> layer is a module that applies a linear transformation on the input using its stored weights and biases.</li><li>The <code>nn.ReLU</code> creates the complex mappings between the model’s inputs and outputs.</li><li><code>nn.Sequential</code> is an ordered container of modules.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">seq_modules = nn.Sequential(<br>    flatten,<br>    layer1,<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">10</span>)<br>)<br>input_image = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br>logits = seq_modules(input_image)<br></code></pre></td></tr></table></figure></li><li><code>nn.Softmax</code> : The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class.</li></ul><h1 id="6-Automatic-Differentiation"><a href="#6-Automatic-Differentiation" class="headerlink" title="6 Automatic Differentiation"></a>6 Automatic Differentiation</h1><p><a href="https://pytorch.org/docs/stable/notes/autograd.html">Autograd mechanics</a></p><h1 id="7-Optimization-Loop"><a href="#7-Optimization-Loop" class="headerlink" title="7 Optimization Loop"></a>7 Optimization Loop</h1><p>Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters (as we saw in the previous section), and optimizes these parameters using gradient descent.<br>We define the following hyperparameters for training:</p><ul><li><strong>Number of Epochs</strong>: the number times to iterate over the dataset</li><li><strong>Batch Size</strong>: the number of data samples propagated through the network before the parameters are updated</li><li><strong>Learning Rate</strong>: how much to update models parameters at each batch&#x2F;epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training</li></ul><p>Each iteration of the optimization loop is called an epoch.And it  consists of two main parts: </p><ul><li>The Train Loop：iterate over the training dataset and try to converge to optimal parameters.</li><li>The Validation&#x2F;Test Loop：iterate over the test dataset to check if model performance is improving.</li></ul><p>Common loss functions include <code>nn.MSELoss</code> (Mean Square Error) for regression tasks, and <code>nn.NLLLoss</code> (Negative Log Likelihood) for classification. <code>nn.CrossEntropyLoss</code> combines <code>nn.LogSoftmax</code> and <code>nn.NLLLoss</code>.</p><p>Optimization is the process of adjusting model parameters to reduce model error in each training step. </p><iframe width="560" height="315" src="https://youtu.be/tIeHLnjs5U8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><h1 id="8-Save-Load-and-Use-Model"><a href="#8-Save-Load-and-Use-Model" class="headerlink" title="8 Save, Load and Use Model"></a>8 Save, Load and Use Model</h1><p>PyTorch models store the learned parameters in an internal state dictionary, called <code>state_dict</code>. These can be persisted via the <code>torch.save</code> method:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.vgg16(weights=<span class="hljs-string">&#x27;IMAGENET1K_V1&#x27;</span>)<br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;model_weights.pth&#x27;</span>)<br><br><span class="hljs-comment"># To load model weights, you need to create an instance of the same model first, and then load the parameters </span><br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;model_weights.pth&#x27;</span>))<br><span class="hljs-comment"># be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode.</span><br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure><h1 id="9-PACKAGE参考"><a href="#9-PACKAGE参考" class="headerlink" title="9 PACKAGE参考"></a>9 PACKAGE参考</h1><h2 id="9-1-torch"><a href="#9-1-torch" class="headerlink" title="9.1 torch"></a>9.1 torch</h2><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/">torch</a>包含了多维张量的数据结构以及基于其上的多种数学操作。另外，它也提供了多种工具，其中一些可以更有效地对张量和任意类型进行序列化。</p><h3 id="9-1-1-Tensor"><a href="#9-1-1-Tensor" class="headerlink" title="9.1.1 Tensor"></a>9.1.1 Tensor</h3><ul><li><code>torch.is_tensor(obj)</code></li><li><code>torch.is_storage(obj)</code></li><li><code>torch.numel(input)</code>: 返回input张量中的元素个数</li><li><code>torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)</code><ul><li>设置打印选项。</li><li>precision：浮点数输出的精度位数 (默认为8)</li><li>threshold：阈值，触发汇总显示而不是完全显示(repr)的数组元素的总数 （默认为1000）</li><li>edgeitems：汇总显示中，每维（轴）两端显示的项数（默认值为3）</li><li>linewidth：用于插入行间隔的每行字符数（默认为80）。Thresholded matricies will ignore this parameter.</li><li>profile：pretty打印的完全默认值。 可以覆盖上述所有选项 (默认为short, full)</li></ul></li></ul><h3 id="9-1-2-创建操作"><a href="#9-1-2-创建操作" class="headerlink" title="9.1.2 创建操作"></a>9.1.2 创建操作</h3><ul><li><code>torch.eye(n, m=None, out=None)</code>: 返回一个nxn的2维张量，对角线位置全1，其它位置全0</li><li><code>torch.from_numpy(ndarray)</code></li><li><code>torch.linspace(start, end, steps=100, out=None)</code>: 返回一个1维张量，包含在区间start和end上均匀间隔的steps个点。 输出1维张量的长度为steps</li><li><code>torch.logspace(start, end, steps=100, out=None)</code></li><li><code>torch.ones(*sizes, out=None)</code></li><li><code>torch.rand(*sizes, out=None)</code>: 返回一个张量，包含了从区间<code>[0,1)</code>的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义</li><li><code>torch.randn(*sizes, out=None)</code>: 返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数sizes定义</li><li><code>torch.randperm(n, out=None)</code>: 给定参数n，返回一个从0 到n -1 的随机整数排列</li><li><code>torch.arange(start, end, step=1, out=None)</code>: 包含从start到end，以step为步长的一组序列值(默认步长为1)</li><li><code>torch.zeros(*sizes, out=None)</code></li></ul><h3 id="9-1-3-索引-切片-连接-换位"><a href="#9-1-3-索引-切片-连接-换位" class="headerlink" title="9.1.3 索引,切片,连接,换位"></a>9.1.3 索引,切片,连接,换位</h3><ul><li><code>torch.cat(inputs, dimension=0)</code></li><li><code>torch.chunk(tensor, chunks, dim=0)</code>: 在给定维度(轴)上将输入张量进行分块儿</li><li><code>torch.gather(input, dim, index, out=None)</code>: 沿给定轴dim，将输入索引张量index指定位置的值进行聚合</li><li><code>torch.index_select(input, dim, index, out=None)</code>: 沿着指定维度对输入进行切片，取index中指定的相应项(index为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量_Tensor_有相同的维度(在指定轴上)。<strong>注意： 返回的张量不与原始张量共享内存空间</strong></li><li><code>torch.masked_select(input, mask, out=None)</code></li><li><code>torch.nonzero(input, out=None)</code>: 返回一个包含输入input中非零元素索引的张量</li><li><code>torch.split(tensor, split_size, dim=0)</code>: 将输入张量分割成相等形状的chunks（如果可分）</li><li><code>torch.squeeze(input, dim=None, out=None)</code>: 将输入张量形状中的1去除并返回</li><li><code>torch.stack(sequence, dim=0)</code>: 沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状</li><li><code>torch.t</code>: 输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数<code>transpose(input, 0, 1)</code>的简写函数</li><li><code>torch.transpose(input, dim0, dim1, out=None)</code>: 返回输入矩阵input的转置。交换维度dim0和dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改</li><li><code>torch.unbind</code>: 移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片</li><li><code>torch.unsqueeze(input, dim, out=None)</code>: 返回一个新的张量，对输入的制定位置插入维度 1</li></ul><h3 id="9-1-4-随机抽样"><a href="#9-1-4-随机抽样" class="headerlink" title="9.1.4 随机抽样"></a>9.1.4 随机抽样</h3><ul><li><code>torch.manual_seed(seed)</code></li><li><code>torch.initial_seed()</code></li><li><code>torch.get_rng_state()</code></li><li><code>torch.set_rng_state(new_state)</code></li><li><code>torch.default_generator = &lt;torch._C.Generator object&gt;</code></li><li><code>torch.bernoulli(input, out=None)</code>: 从伯努利分布中抽取二元随机数(0 或者 1)。输入张量须包含用于抽取上述二元随机值的概率</li><li><code>torch.multinomial(input, num_samples,replacement=False, out=None)</code>: 返回一个张量，每行包含从input相应行中定义的多项分布中抽取的num_samples个样本</li><li><code>torch.normal(means, std, out=None)</code>: 返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数</li></ul><h3 id="9-1-5-序列化"><a href="#9-1-5-序列化" class="headerlink" title="9.1.5 序列化"></a>9.1.5 序列化</h3><ul><li><code>torch.save(obj, f, pickle_module=&lt;module &#39;pickle&#39; from &#39;/home/jenkins/miniconda/lib/python3.5/pickle.py&#39;&gt;, pickle_protocol=2)</code>: 保存一个对象到一个硬盘文件上</li><li><code>torch.load</code></li></ul><h3 id="9-1-6-并行化"><a href="#9-1-6-并行化" class="headerlink" title="9.1.6 并行化"></a>9.1.6 并行化</h3><ul><li><code>torch.get_num_threads()</code>：获得用于并行化CPU操作的OpenMP线程数</li><li><code>torch.set_num_threads()</code>: 设定用于并行化CPU操作的OpenMP线程数</li></ul><h3 id="9-1-7-数学操作"><a href="#9-1-7-数学操作" class="headerlink" title="9.1.7 数学操作"></a>9.1.7 数学操作</h3><ul><li><code>torch.abs(input, out=None)</code>: 计算输入张量的每个元素绝对值</li><li><code>torch.sin(input, out=None)</code></li><li><code>torch.sinh(input, out=None)</code>: 双曲正弦</li><li><code>torch.cos(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的余弦</li><li><code>torch.cosh(input, out=None)</code>: 双曲余弦</li><li><code>torch.tan(input, out=None)</code></li><li><code>torch.tanh(input, out=None)</code>:双曲正切</li><li><code>torch.acos(input, out=None)</code>: 返回一个新张量，包含输入张量每个元素的反余弦</li><li><code>torch.asin(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的反正弦函数</li><li><code>torch.atan(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的反正切函数</li><li><code>torch.atan2(input1, input2, out=None)</code>: 返回一个新张量，包含两个输入张量input1和input2的反正切函数</li><li><code>torch.add(input, value, out=None)</code></li><li><code>torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None)</code>: 用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor</li><li><code>torch.addcmul(tensor, value=1, tensor1, tensor2, out=None)</code>: 用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor</li><li><code>torch.ceil(input, out=None)</code>: 天井函数，对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数，并返回结果到输出</li><li><code>torch.floor(input, out=None)</code>: 床函数,返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数</li><li><code>torch.round(input, out=None)</code>: 返回一个新张量，将输入input张量每个元素舍入到最近的整数</li><li><code>torch.sign(input, out=None)</code>: 符号函数：返回一个新张量，包含输入input张量每个元素的正负</li><li><code>torch.trunc(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数，其比x更接近零。简而言之，有符号数的小数部分被舍弃)</li><li><code>torch.clamp(input, min, max, out=None)</code>: 将输入input张量每个元素的夹紧到区间 [min,max]，并返回结果到一个新张量</li><li><code>torch.div(input, value, out=None)</code>: 将input逐元素除以标量值value，并返回结果到输出张量out</li><li><code>torch.exp(tensor, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的指数</li><li><code>torch.fmod(input, divisor, out=None)</code>: 计算除法余数，其结果的符号与 input 相同</li><li><code>torch.remainder(input, divisor, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的除法余数，其结果与 divisor 相同</li><li><code>torch.frac(tensor, out=None)</code>: 返回每个元素的分数部分</li><li><code>torch.lerp(start, end, weight, out=None)</code>: 对两个张量以start，end做线性插值， 将结果返回到输出张量。out &#x3D; start +weight * (end - start)</li><li><code>torch.log(input, out=None)</code>: 计算input的自然对数</li><li><code>torch.log1p(input, out=None) </code>: 计算input+1的自然对数</li><li><code>torch.mul(input, value, out=None)</code>: 用标量值value乘以输入input的每个元素，并返回一个新的结果张量</li><li><code>torch.neg(input, out=None)</code>: 返回一个新张量，包含输入input张量按元素取负</li><li><code>torch.pow(input, exponent, out=None)</code>: 对输入input的按元素求exponent次幂值，并返回结果张量</li><li><code>torch.reciprocal(input, out=None)</code>: 倒数</li><li><code>torch.rsqrt(input, out=None)</code>: 平方根倒数</li><li><code>torch.sqrt(input, out=None)</code>: 平方根</li><li><code>torch.sigmoid(input, out=None)</code></li></ul><h3 id="9-1-8-Reduction-Ops-缩减操作"><a href="#9-1-8-Reduction-Ops-缩减操作" class="headerlink" title="9.1.8 Reduction Ops(缩减操作)"></a>9.1.8 Reduction Ops(缩减操作)</h3><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/">https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/</a></p><p>！注意： 会改变tensor的函数操作会用一个下划线后缀来标示。比如，torch.FloatTensor.abs_()会在原地计算绝对值，并返回改变后的tensor，而tensor.FloatTensor.abs()将会在一个新的tensor中计算结果。</p><h2 id="9-2-torch-Tensor"><a href="#9-2-torch-Tensor" class="headerlink" title="9.2 torch.Tensor"></a>9.2 torch.Tensor</h2><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Tensor/">torch.Tensor</a>是一种包含单一数据类型元素的多维矩阵。</p><h2 id="9-3-torch-Storage"><a href="#9-3-torch-Storage" class="headerlink" title="9.3 torch.Storage"></a>9.3 torch.Storage</h2><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Storage/">torch.Storage</a>是一个单一数据类型的连续一维数组。</p><h2 id="9-4-torch-nn"><a href="#9-4-torch-nn" class="headerlink" title="9.4 torch.nn"></a>9.4 torch.nn</h2><p><a href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#_1">torch.nn</a></p>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>PyTorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Userful tools</title>
    <link href="/2024/05/22/3_Userful-tools/"/>
    <url>/2024/05/22/3_Userful-tools/</url>
    
    <content type="html"><![CDATA[<p>工欲善其事必先利其器。</p><span id="more"></span><h1 id="1-笔记管理"><a href="#1-笔记管理" class="headerlink" title="1 笔记管理"></a>1 笔记管理</h1><p><a href="https://www.notion.so/">Notion</a></p><h1 id="2-作图"><a href="#2-作图" class="headerlink" title="2 作图"></a>2 作图</h1><p>大家好像主要都是Visio，PPT为辅（我觉得PhotoShop也很方便，嘻嘻嘻感觉冥冥之中当年钻研摄影帮了我好多好多）</p><h2 id="2-1-首先是工具推荐"><a href="#2-1-首先是工具推荐" class="headerlink" title="2.1 首先是工具推荐"></a>2.1 首先是工具推荐</h2><ul><li><a href="https://app.diagrams.net/">draw.io</a></li><li><a href="https://github.com/guanyingc/latex_paper_writing_tips">Tips for Writing a Research Paper using LaTeX</a>:由陈冠英老师维护，给LaTeX初学者提供多个图表排版的例子，方便用到自己的论文当中。还有种会议poster的例子，可以参考。</li><li><a href="https://github.com/dair-ai/ml-visuals">ML Visuals</a>:里面包含100多个常用的神经网络的图，是google在线PPT的形式.<a href="https://docs.google.com/presentation/d/11mR1nkIR9fbHegFkcFq8z9oDQ5sjv8E3JJp1LfLGKuk/edit#slide=id.p">PPT直接下载</a></li><li><a href="https://www.iconfont.cn/search/index?searchType=icon&q=%E9%87%91%E5%B8%81&page=1&fromCollection=-1&fills=&tag=">阿里巴巴矢量图标素材库</a></li><li><a href="http://alexlenail.me/NN-SVG/index.html">NN SVG</a>:画神经网络结构用的。左侧是一些设置选项，可以自己设置节点，层数，还有网络类型等等，设置好以后可以把这个神经网络图直接下载下来，然后插入到visio或者PPT中就可以用了！</li><li><code>ChatHub</code>：是一个基于谷歌Chrome浏览器插件的chatbot聚合客户端，你可以用它在一个应用中使用多种AI聊天服务。</li><li><code>Code Interpreter</code>：OpenAI 的官方插件，通过设置中的Beta面板向所有ChatGPT Plus 用户提供。可以数据分析、创建图表、编辑文件、执行数学运算等。可惜要花钱，但它有以下优点：<ul><li>Code Interpreter 允许 AI 做数学题（非常复杂的数学题）和做更精确的文字工作（比如实际计算段落中的字数），因为它可以编写 Python 代码来解决大语言模型在数学和语言方面的固有弱点。</li><li>Code Interpreter 降低了幻觉和迷惑的概率。</li><li>Code Interpreter 让人工智能的用途更加广泛。</li><li>用户不必编程，因为 Code Interpreter 可以代替做所有的工作。</li><li>它给了你更多的 AI Moment。</li></ul></li><li><a href="https://github.com/OpenBMB/MiniCPM-V">视觉问答领域开源项目</a></li></ul><h2 id="2-2-简单聊聊目前对作图的认知"><a href="#2-2-简单聊聊目前对作图的认知" class="headerlink" title="2.2 简单聊聊目前对作图的认知"></a>2.2 简单聊聊目前对作图的认知</h2><p>我感觉审稿人看论文是（就连我自己看论文的时候都是）挑重点看，如果图能把你想表达的观点说清楚，文字写得不好其实问题不大，毕竟文字表达一个个磨非常耗费心力和时间。而且人的注意力天生就是喜欢看图。而且画图其实蛮享受的，它是一个把你的思路显现出来的过程，喜欢。</p><p>但无论是哪种图，配色一般是淡蓝、淡红、淡黄、淡绿四种（彩虹一共才7种颜色，要体现对比就只能是这四种颜色了，而且你需要考虑一下有人红绿色盲 … … ），然后有时会用紫色、灰色补充（紫色多用于模块，灰色多用于小区域或者大面积打底）。整个论文的配色风格需要一致，比如说后面的折线图、柱状图、散点图最好还是以前面框架的颜色一致。</p><p>经过一次投稿，CV感觉只需要两种图，框架图和数据展示图。审美相关，这里安利一下，感觉Yongming Rao老师的图好好看：<br><img src="https://pic4.zhimg.com/80/v2-917d709c464f873d5876ab9665435543_720w.webp"></p><h1 id="3-论文"><a href="#3-论文" class="headerlink" title="3 论文"></a>3 论文</h1><ul><li><a href="http://www.researchrabbitapp.com/">Research Rabbit</a>:文献检索及可视化工具，它使用 AI 帮助我们发现相关且有趣的研究文章, 这些文章是根据我们的研究方向来关联的。<a href="https://zhuanlan.zhihu.com/p/394423702">使用教程</a></li><li><a href="https://www.connectedpapers.com/">connectedpapers</a>:相似论文查找</li><li><a href="https://www.scholar-inbox.com/">scholar-inbox</a>:论文推荐</li></ul><h1 id="4-汇报"><a href="#4-汇报" class="headerlink" title="4 汇报"></a>4 汇报</h1><ul><li><a href="https://zhiwen.xfyun.cn/">讯飞智文-AI在线生成PPT、Word</a></li></ul>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tools</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Configure online demo</title>
    <link href="/2024/05/19/99_onlineDemo/"/>
    <url>/2024/05/19/99_onlineDemo/</url>
    
    <content type="html"><![CDATA[<p>这篇文章总共纪录了学习Replicate和HuggingFace在线搭demo的过程，最终我选的是HuggingFace，所以HuggingFace的内容详尽一些，Replicate只有一点点。</p><span id="more"></span><h1 id="Replicate"><a href="#Replicate" class="headerlink" title="Replicate"></a>Replicate</h1><p>参考：<a href="https://replicate.com/docs">Replicate文档</a><br>Replicate是一个云端的机器学习模型运行平台，它允许用户使用云端API（在Python或Jupyter Notebook中）直接运行模型，并在云端进行模型的部署和调优。<br>其优势在于：<br>    - 无需下载、安装或配置<br>    - 快速轻松运行机器学习模型<br>    - 提供大量的预训练模型和数据集</p><h2 id="1-1-大致流程"><a href="#1-1-大致流程" class="headerlink" title="1.1 大致流程"></a>1.1 大致流程</h2><p>Replicate的HTTP API 可与任何编程语言配合使用。使用 Python 客户端，首先需要安装Python库：<code>pip install replicate</code>。</p><p>然后去<a href="https://replicate.com/account/api-tokens">account settings</a>中找到你的API token，并把它设置（声明）到你当前的代码环境：<code>export REPLICATE_API_TOKEN=&lt;paste-your-token-here&gt;</code>。</p><p>安全一点的方式为：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># get a token: https://replicate.com/account</span><br><span class="hljs-keyword">from</span> getpass <span class="hljs-keyword">import</span> getpass<br><span class="hljs-keyword">import</span> os<br><br>REPLICATE_API_TOKEN = getpass()<br>os.environ[<span class="hljs-string">&quot;REPLICATE_API_TOKEN&quot;</span>] = REPLICATE_API_TOKEN<br><br><span class="hljs-comment">## 2.1 在Python代码中导入Replicate库，以便使用Replicate的功能</span><br><span class="hljs-keyword">import</span> replicate<br><span class="hljs-comment">### receive images as inputs. Use a file handle or URL:</span><br>image = <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;mystery.jpg&quot;</span>, <span class="hljs-string">&quot;rb&quot;</span>)<br><span class="hljs-comment">### or...</span><br>image = <span class="hljs-string">&quot;https://example.com/mystery.jpg&quot;</span><br><br><span class="hljs-comment">## 2.2 调用模型，按需求返回</span><br>replicate.run(<br>  <span class="hljs-string">&quot;replicate/resnet:dd782a3d531b61af491d1026434392e8afb40bfb53b8af35f727e80661489767&quot;</span>,<br>  <span class="hljs-built_in">input</span>=&#123;<span class="hljs-string">&quot;image&quot;</span>: image&#125;<br>)<br></code></pre></td></tr></table></figure><p><a href="https://juejin.cn/post/7298642789078974515">有个跟我很像的用了controlnet的工作在Replicate上的部署教程</a><br>大概了解了一下，感觉Replicate挺简单的，但是好像不如Huggingface的使用者多以及GPU算力要贵一点。决定临阵倒戈，还是去学HuggingFace吧，谁知道以后遇到什么bug了能不能解决。</p><p>好多人用啊，让我看看这个最大的开源平台有多牛(ง๑ •̀_•́)ง</p><h1 id="Gradio-HuggingFace"><a href="#Gradio-HuggingFace" class="headerlink" title="Gradio + HuggingFace"></a>Gradio + HuggingFace</h1><p>计算机视觉和图像处理的算法一般都具有直观的实用性。为了推广自己工作的影响力，大家会选择把自己算法的实现效果部署到网页端的UI接口供大家使用。</p><h1 id="1-界面设计库Gradio"><a href="#1-界面设计库Gradio" class="headerlink" title="1 界面设计库Gradio"></a>1 界面设计库Gradio</h1><p>Gradio是MIT的开源项目，它允许我们快速建立demo或者web application。使用时可理解为一个Python包，Prerequisite: Gradio requires Python 3.8 or higher，它的安装命令：<code>pip install gradio</code>。<br><a href="https://www.gradio.app/guides">Gradio教程</a></p><h2 id="1-1-Quickstart"><a href="#1-1-Quickstart" class="headerlink" title="1.1 Quickstart"></a>1.1 Quickstart</h2><h3 id="1-1-1-示例，实现一个本地静态交互页面"><a href="#1-1-1-示例，实现一个本地静态交互页面" class="headerlink" title="1.1.1 示例，实现一个本地静态交互页面"></a>1.1.1 示例，实现一个本地静态交互页面</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>(<span class="hljs-params">name, intensity</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Hello, &quot;</span> + name + <span class="hljs-string">&quot;!&quot;</span> * <span class="hljs-built_in">int</span>(intensity)<br><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;slider&quot;</span>],<br>    outputs=[<span class="hljs-string">&quot;text&quot;</span>],<br>)<br><br>demo.launch()<br><br></code></pre></td></tr></table></figure><p>我用的anaconda命令行运行，它显示：</p><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs lasso">Running <span class="hljs-keyword">on</span> <span class="hljs-built_in">local</span> URL:  http:<span class="hljs-comment">//127.0.0.1:7860</span><br><br><span class="hljs-keyword">To</span> create a <span class="hljs-keyword">public</span> <span class="hljs-keyword">link</span>, <span class="hljs-built_in">set</span> <span class="hljs-string">`share=True`</span> <span class="hljs-keyword">in</span> <span class="hljs-string">`launch()`</span>.<br></code></pre></td></tr></table></figure><p>然后我在本地浏览器输入<code>http://127.0.0.1:7788/</code>就看到了对应的页面。</p><h3 id="1-1-2-hot-reload-mode"><a href="#1-1-2-hot-reload-mode" class="headerlink" title="1.1.2 hot reload mode"></a>1.1.2 hot reload mode</h3><p>Automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in gradio before the name of the file instead of python. In the example:<code>gradio app.py</code>(Type this in terminal).<br><a href="https://www.gradio.app/guides/developing-faster-with-reload-mode">Hot Reloading Guide</a></p><h3 id="1-1-3-Understanding-the-Interface-Class"><a href="#1-1-3-Understanding-the-Interface-Class" class="headerlink" title="1.1.3 Understanding the Interface Class"></a>1.1.3 Understanding the Interface Class</h3><p>gradio的核心是它的<code>gr.Interface</code>函数，用来构建可视化界面。The Interface class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs.<br>它主要有三个核心属性：</p><ul><li><code>fn</code>: the function to wrap a user interface (UI) around</li><li><code>inputs</code>：the number of components should match the number of arguments in your function</li><li><code>outputs</code>：the number of components should match the number of return values from your function.</li></ul><p>所以对于任何图像处理类的ML代码来说，基本流程就是<strong>图像输入&gt;&gt;模型推理&gt;&gt;返回图片</strong>。<a href="https://www.gradio.app/main/guides/the-interface-class">building Interfaces</a></p><h3 id="1-1-4-Sharing-Your-Demo"><a href="#1-1-4-Sharing-Your-Demo" class="headerlink" title="1.1.4 Sharing Your Demo"></a>1.1.4 Sharing Your Demo</h3><p>Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set <code>share=True</code> in launch() like <code>demo.launch(share=True)</code>, and a publicly accessible URL will be created for your demo.</p><p>所以实际上他跑起来用的是我本地的算力资源，但是类似网络通信可以将输入输出在不同电脑之间进行联络。Share links expire after 72 hours. (it is also possible to <a href="https://github.com/huggingface/frp/">set up your own Share Server</a> on your own cloud server to overcome this restriction.)</p><h2 id="1-2-the-Interface-Class"><a href="#1-2-the-Interface-Class" class="headerlink" title="1.2 the Interface Class"></a>1.2 the Interface Class</h2><h3 id="1-2-1-Components"><a href="#1-2-1-Components" class="headerlink" title="1.2.1 Components"></a>1.2.1 Components</h3><p>提供了超过30种components(e.g. the gr.Image component is designed to handle input or output images, the gr.Label component displays classification labels and probabilities, the gr.Plot component displays various kinds of plots, and so on).<br>而且Gradio可以自动处理输入输出与函数fn之间的类型转换(Preprocessing and Postprocessing)。</p><h3 id="1-2-2-Components-Attributes"><a href="#1-2-2-Components-Attributes" class="headerlink" title="1.2.2 Components Attributes"></a>1.2.2 Components Attributes</h3><p>比如加一个滑轮效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 之前的</span><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;slider&quot;</span>],<br>    outputs=[<span class="hljs-string">&quot;text&quot;</span>],<br>)<br><span class="hljs-comment">######################################################################</span><br><span class="hljs-comment"># 现在</span><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, gr.Slider(value=<span class="hljs-number">2</span>, minimum=<span class="hljs-number">1</span>, maximum=<span class="hljs-number">10</span>, step=<span class="hljs-number">1</span>)],<br>    outputs=[gr.Textbox(label=<span class="hljs-string">&quot;greeting&quot;</span>, lines=<span class="hljs-number">3</span>)],<br>)<br><br></code></pre></td></tr></table></figure><p>Suppose you had a more complex function, with multiple outputs as well. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 之前的</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>(<span class="hljs-params">name, intensity</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Hello, &quot;</span> + name + <span class="hljs-string">&quot;!&quot;</span> * intensity<br><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, gr.Slider(value=<span class="hljs-number">2</span>, minimum=<span class="hljs-number">1</span>, maximum=<span class="hljs-number">10</span>, step=<span class="hljs-number">1</span>)],<br>    outputs=[gr.Textbox(label=<span class="hljs-string">&quot;greeting&quot;</span>, lines=<span class="hljs-number">3</span>)],<br>)<br><span class="hljs-comment"># lines=3 参数用于设置文本框的默认显示行数</span><br><span class="hljs-comment"># 具体来说，它控制了文本框的高度，使其在显示时能够容纳 3 行文本。</span><br><br><span class="hljs-comment">######################################################################</span><br><span class="hljs-comment"># 现在</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>(<span class="hljs-params">name, is_morning, temperature</span>):<br>    salutation = <span class="hljs-string">&quot;Good morning&quot;</span> <span class="hljs-keyword">if</span> is_morning <span class="hljs-keyword">else</span> <span class="hljs-string">&quot;Good evening&quot;</span><br>    greeting = <span class="hljs-string">f&quot;<span class="hljs-subst">&#123;salutation&#125;</span> <span class="hljs-subst">&#123;name&#125;</span>. It is <span class="hljs-subst">&#123;temperature&#125;</span> degrees today&quot;</span><br>    celsius = (temperature - <span class="hljs-number">32</span>) * <span class="hljs-number">5</span> / <span class="hljs-number">9</span><br>    <span class="hljs-keyword">return</span> greeting, <span class="hljs-built_in">round</span>(celsius, <span class="hljs-number">2</span>)<br><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;checkbox&quot;</span>, gr.Slider(<span class="hljs-number">0</span>, <span class="hljs-number">100</span>)],<br>    outputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;number&quot;</span>],<br>)<br></code></pre></td></tr></table></figure><h3 id="1-2-3-Reactive-Interfaces"><a href="#1-2-3-Reactive-Interfaces" class="headerlink" title="1.2.3 Reactive Interfaces"></a>1.2.3 Reactive Interfaces</h3><p>提供Live Interfaces模式，不需要submit button，会自动计算提交结果。暂时我不太需要这个功能，之后遇到再学。</p><h3 id="1-2-4-The-4-Kinds-of-Gradio-Interfaces"><a href="#1-2-4-The-4-Kinds-of-Gradio-Interfaces" class="headerlink" title="1.2.4 The 4 Kinds of Gradio Interfaces"></a>1.2.4 The 4 Kinds of Gradio Interfaces</h3><ol><li>Standard demos: which have both separate inputs and outputs</li><li>Output-only demos: which don’t take any input but produce on output</li><li>Input-only demos: which don’t produce any output but do take in some sort of input</li><li>Unified demos: which have both input and output components, but the input and output components are the same. This means that the output produced overrides the input (e.g. a text autocomplete model)</li></ol><h3 id="1-2-5-Queuing"><a href="#1-2-5-Queuing" class="headerlink" title="1.2.5 Queuing"></a>1.2.5 Queuing</h3><p>Every Gradio app comes with a built-in queuing system that can scale to thousands of concurrent users. You can configure the queue by using <code>queue()</code> method. 例如通过设置queue()的<code>default_concurrency_limit</code>参数来控制单次处理的请求数(默认是1):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">demo = gr.Interface(...).queue(default_concurrency_limit=<span class="hljs-number">5</span>)<br>demo.launch()<br></code></pre></td></tr></table></figure><h3 id="1-2-6-Streaming-outputs"><a href="#1-2-6-Streaming-outputs" class="headerlink" title="1.2.6 Streaming outputs"></a>1.2.6 Streaming outputs</h3><p>In some cases, you may want to stream a sequence of outputs rather than show a single output at once.官网说的场景是“图像生成，显示到结果图像中的每一时刻图像”，感觉我暂时不需要，之后再学。</p><h3 id="1-2-7-Alerts"><a href="#1-2-7-Alerts" class="headerlink" title="1.2.7 Alerts"></a>1.2.7 Alerts</h3><p>跟用户显示警告提醒他们某些输入输出的错误。主要使用<code>gr.Error(&quot;custom message&quot;)</code>、<code>gr.Warning(&quot;custom message&quot;)</code>和<code>gr.Info(&quot;custom message&quot;) </code>。The only difference between gr.Info() and gr.Warning() is the color of the alert.</p><h3 id="1-2-8-Styling"><a href="#1-2-8-Styling" class="headerlink" title="1.2.8 Styling"></a>1.2.8 Styling</h3><p>可以直接调用别人目前设计的主题模板，也可以自己上传CSS文件自定义。只需要在代码中写上这一句<code>demo = gr.Interface(..., theme=gr.themes.Monochrome())</code><br><a href="https://www.gradio.app/guides/theming-guide">关于Theming的介绍</a></p><h3 id="1-2-9-Progress-bars"><a href="#1-2-9-Progress-bars" class="headerlink" title="1.2.9 Progress bars"></a>1.2.9 Progress bars</h3><p>挺人性化的设计的，到哪都是进度条是吧( ´◔︎ ‸◔︎&#96;)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><span class="hljs-keyword">import</span> time<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">slowly_reverse</span>(<span class="hljs-params">word, progress=gr.Progress(<span class="hljs-params"></span>)</span>):<br>    progress(<span class="hljs-number">0</span>, desc=<span class="hljs-string">&quot;Starting&quot;</span>)<br>    time.sleep(<span class="hljs-number">1</span>)<br>    progress(<span class="hljs-number">0.05</span>)<br>    new_string = <span class="hljs-string">&quot;&quot;</span><br>    <span class="hljs-keyword">for</span> letter <span class="hljs-keyword">in</span> progress.tqdm(word, desc=<span class="hljs-string">&quot;Reversing&quot;</span>):<br>        time.sleep(<span class="hljs-number">0.25</span>)<br>        new_string = letter + new_string<br>    <span class="hljs-keyword">return</span> new_string<br><br>demo = gr.Interface(slowly_reverse, gr.Text(), gr.Text())<br><br>demo.launch()<br><br></code></pre></td></tr></table></figure><p> you can even report progress updates automatically from any tqdm.tqdm that already exists within your function by setting the default argument as gr.Progress(track_tqdm&#x3D;True)!</p><h1 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h1><p> add example<br>我们可以在页面下方添加供用户选择的测试样例。比如做了一个图像去噪算法，但是用户手头并没有躁点照片，example能让他更快的体验到效果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> gradio <span class="hljs-keyword">as</span> gr<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">greet</span>(<span class="hljs-params">name, intensity</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;Hello, &quot;</span> + name + <span class="hljs-string">&quot;!&quot;</span> * <span class="hljs-built_in">int</span>(intensity)<br><br>demo = gr.Interface(<br>    fn=greet,<br>    inputs=[<span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;slider&quot;</span>],<br>    outputs=[<span class="hljs-string">&quot;text&quot;</span>],<br>)<br><br>demo.launch()<br><br></code></pre></td></tr></table></figure><h1 id="2-在线托管平台-HuggingFace"><a href="#2-在线托管平台-HuggingFace" class="headerlink" title="2 在线托管平台 HuggingFace"></a>2 在线托管平台 HuggingFace</h1><p><a href="https://www.gradio.app/guides/sharing-your-app">Hosting on HF Spaces</a><br>在本地测试完成后，接下来就是考虑如何将程序发布到在线平台的问题了。HuggingFace提供了一个项目托管平台，而且能免费提供如Google Colab的在线计算资源。使用服务前，先注册账号（官网界面右上角”Sign Up”）。</p><h2 id="2-1-在线计算空间-Space"><a href="#2-1-在线计算空间-Space" class="headerlink" title="2.1 在线计算空间:Space"></a>2.1 在线计算空间:Space</h2><p>在HuggingFace官网登录账号后，切换到Spaces创建一个新的空间。记得选中”Gradio”和”Public”，以生成一个可公开使用的Gradio在线应用。每个项目空间免费配备8个CPU核和16GB 运行内存，GPU资源需要单独付费。更多关于Spaces的介绍可参考<a href="https://huggingface.co/docs/hub/spaces">官方文档</a>。<br>创建完Space之后，我们需要把本地项目文件（UI构建文件必须得命名成app.py且位于根目录）上传到该空间。具体方法与Github项目的上传和版本维护方式完全一样:</p><ul><li>克隆space项目到本地：<code>git clone https://huggingface.co/spaces/your_account/proj_name/tree/main</code></li><li>将本地已跑通的项目文件复制到刚才克隆的space项目文件夹</li><li>新建描述运行环境依赖的文件：<code>requirements.txt</code></li><li>指定Python依赖的包:<code>packages.txt</code></li><li>指定特殊的系统依赖配置</li><li><a href="https://huggingface.co/docs/hub/spaces-dependencies">详情参考</a></li><li>将此更新同步到远程仓库：<ul><li>git add -A .</li><li>git commit -m “add project files”</li><li>git push</li></ul></li></ul><p>完成以上步骤后（等待1~2分钟系统刷新），进入Space项目的App选项卡即可查看部署到web端的应用。</p><h2 id="2-2-模型托管仓库-Models"><a href="#2-2-模型托管仓库-Models" class="headerlink" title="2.2 模型托管仓库:Models"></a>2.2 模型托管仓库:Models</h2><p>如果我们运行的程序是AI模型，那么一般需要提供一个训练好的checkpoint（一般上百兆）供在线加载。这时，我们可以在HuggingFace的Models页面创建一个与Space项目同名的模型仓库，用于存储需要的checkpoint等文件。</p><ul><li>上传文件：通过上文提到的git方式，或者直接点击已创建的模型页面的Add file</li><li>获取文件路径：例如上传到模型仓库的文件路径是：<code>https://huggingface.co/menghanxia/disco/tree/main/model.pth.tar</code>，其对应的下载路径则需要将tree修改为resolve，即<code>https://huggingface.co/menghanxia/disco/resolve/main/disco-beta.pth.tar</code></li><li>在Space项目的app.py文件中调用文件下载命令:<code>os.system(&quot;wget https://huggingface.co/menghanxia/disco/resolve/main/disco-beta.pth.rar&quot;)</code></li></ul><p><a href="https://huggingface.co/docs/hub/spaces">Spaces文档</a></p>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>Tools</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Tools</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>PyTorch_《Deep learning with PyTorch》</title>
    <link href="/2024/04/22/2_Pytorch/"/>
    <url>/2024/04/22/2_Pytorch/</url>
    
    <content type="html"><![CDATA[<p>路漫漫其修远兮。本篇章主要记录的是清华翻译的《Deep learning with PyTorch》这本书中的重要内容，是Pytorch探索之旅的第1篇章！</p><span id="more"></span><p>Reference materials include:</p><ul><li><a href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch官方文档(中文版)</a>: 我感觉这个文档的好处在于非常详尽的给出相关函数和模块的细致化定义介绍</li><li><a href="https://pytorch.org/tutorials/">Tutorials</a>：感觉官网的这个文档是从Pytorch的使用框架去简要介绍</li><li>Andrew W. Traska撰写的<a href="https://www.manning.com/books/grokking-deep-learning">《Grokking Deep Learning》</a>是开发强大模型和理解深度神经网络基础机制的重要资源</li><li>Ian Goodfellow, Yoshua Bengio和Aaron Courville的<a href="https://www.deeplearningbook.org/">《Deep Learning》</a></li><li>清华翻译的<a href="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/">《Deep learning with PyTorch》</a></li></ul><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><strong>PyTorch是一个使用Python格式实现深度学习模型的库</strong>。它的核心数据结构Tensor，是一个多维数组。Pytorch就像是一个能在GPU上运行并且自带自动求导功能的Numpy数组。它配备了高性能的C++运行引擎使得他不必依赖Python的运行机制：</p><ul><li><strong>Pytorch</strong>的很大部分使用C++和CUDA（NVIDIA提供的类似C++的语言）编写。</li><li><strong>PyTorch</strong>核心是提供多维数组（tensor）的库， torch模块提供了对其进行扩展操作的库。</li><li>同时<strong>PyTorch</strong>的第二个核心功能是允许Tensor跟踪对其所执行的操作，并通过反向传播来计算输出相对于其任何输入的导数。</li></ul><p><strong>PyTorch</strong>最先实现了深度学习架构，深度学习模型强大的原因是因为它可以自动的学习样本输出与所期望输出之间的关系。</p><h2 id="1-1-PyTorch提供了构建和训练神经网络所需的所有模块："><a href="#1-1-PyTorch提供了构建和训练神经网络所需的所有模块：" class="headerlink" title="1.1 PyTorch提供了构建和训练神经网络所需的所有模块："></a>1.1 PyTorch提供了构建和训练神经网络所需的所有模块：</h2><ul><li>构建神经网络的核心模块位于<code>troch.nn</code>中。包括全连接层、卷积层、激活函数和损失函数。</li><li><code>torch.util.data</code>能找到适用于数据加载和处理的工具，相关的两个主要的类为Dataset和DataLoader。<br>  -<code>Dataset</code>承担了自定义的数据格式与标准的Pytorch张量之间的转换任务<br>  -<code>DataLoader</code>可以在后台生成子进程来从Dataset中加载数据，使数据准备就绪并在循环可以使用后立即等待训练循环<br>  -除此之外还可以使用专用的硬件（多个GPU）来训练模型，在这些情况下，可以通过<code>torch.nn.DataParallel</code>和<code>torch.distributed</code>来使用其他的可用硬件</li></ul><p>Pytorch使用Caffe2作为后端，增加了对ONNX的支持（定义了一种与深度学习库无关的模型描述和转换格式），增加了称为<strong>TorchScript</strong>的延迟执行图模式运行引擎（这个模块避开了Python解释器所带来的成本，我们可以将这个模型看作是具有针对张量操作的有限指令集的虚拟机，它的调用不会增加Python的开销，还能使PyTorch可以实时地将已知序列转换为更有效的混合操作）。默认的运行方式是即使执行（eager mode）。</p><p><strong>关于PyTorch的安装，windows建议使用Anaconda，Linux建议使用Pip。</strong></p><h1 id="2-从张量开始"><a href="#2-从张量开始" class="headerlink" title="2. 从张量开始"></a>2. 从张量开始</h1><p><strong>- Tensor是Pytorch最基本的数据结构</strong></p><p>深度学习的应用往往是将某种形式获取的数据（图像或文本）转换为另一种形式的数据（标签、数字或文本），因此从这个角度看，深度学习就像是构建一个将数据从一种表示转换为另一种表示的系统，这种转换是通过从一系列样本中提取共性来驱动的额，这些共性能够反映期望的映射关系。<br>为了实现上述过程，首先需要让网路理解输入数据，因此输入需要被转换为浮点数的集合。这些浮点数的集合及其操作是现代AI的核心。而网络层次之间的数据被视为中间表示（intermediate representation）。中间表示是将输入与前一层神经元权重相结合的结果，每个中间表示对于之前的输入都是唯一的。<br>为此，PyTorch引入了一个基本的数据结构：张量（tensor）。张量是指将向量（vector）和矩阵（matrix）推广到任意维度，与张量相同概念的另一个名称是多维数组（multidimensional array）。</p><h2 id="2-1-张量基础"><a href="#2-1-张量基础" class="headerlink" title="2.1 张量基础"></a>2.1 张量基础</h2><p>Python列表或数字元组（tuple）是在内存中单独分配的Python对象的集合；而PyTorch张量或NumPy数组（通常）是连续内存块上的视图（view），这些内存块存有未封装（unboxed）的C数值类型。<br><strong>获取一个张量的形状：tensor.shape</strong></p><h2 id="2-2-张量与存储"><a href="#2-2-张量与存储" class="headerlink" title="2.2 张量与存储"></a>2.2 张量与存储</h2><p>基础内存只分配一次，由torch.Storage实例管理，<strong>Storage是一个一维的数值数据数组</strong>，例如一块包含了指定类型（可能是float或int32）数字的连续内存块。<strong>而tensor可以看作是某个Storage实例的视图。</strong>因此多个tensor可以索引到同一Storage。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br>points = torch.tensor([[<span class="hljs-number">1.0</span>, <span class="hljs-number">4.0</span>], [<span class="hljs-number">2.0</span>, <span class="hljs-number">1.0</span>], [<span class="hljs-number">3.0</span>, <span class="hljs-number">5.0</span>]])<br>points.storage()<br><br><span class="hljs-comment"># 输出</span><br> <span class="hljs-number">1.0</span><br> <span class="hljs-number">4.0</span><br> <span class="hljs-number">2.0</span><br> <span class="hljs-number">1.0</span><br> <span class="hljs-number">3.0</span><br> <span class="hljs-number">5.0</span><br>[torch.FloatStorage of size <span class="hljs-number">6</span>]<br></code></pre></td></tr></table></figure><p> <strong>无法使用两个索引来索引二维tensor的存储，因为Storage始终是一维的，与引用它的任何张量的维数无关。</strong><br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter2/2.4.png" alt="张量是一个存储实例的视图"></p><h2 id="2-3-尺寸、存储偏移与步长"><a href="#2-3-尺寸、存储偏移与步长" class="headerlink" title="2.3 尺寸、存储偏移与步长"></a>2.3 尺寸、存储偏移与步长</h2><p><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter2/2.5.png" alt="张量的尺寸、偏移与步长之间的关系"><br>tensor可以看作是某个Storage实例的视图。为了索引Storage，张量依赖于几条明确定义它们的信息：尺寸（size）、存储偏移（storage offset）和步长（stride）<br>    - 尺寸是一个元组，表示tensor每个维度上有多少个元素。<br>    - 存储偏移是Storage中与张量中的第一个元素相对应的索引。<br>        - 步长是在Storage中为了沿每个维度获取下一个元素而需要跳过的元素数量。<br>        - 步长是一个元组，表示当索引在每个维度上增加1时必须跳过的存储中元素的数量。<br>    - shape是属性，size()是类，这俩包含的信息相同。</p><p>用下标<code>i</code>和<code>j</code>访问二维张量等价于访问存储中的<code>storage_offset + stride[0] * i + stride[1] * j</code>元素。<br>张量Tensor和和存储Storage之间的这种间接操作会使某些操作（例如转置或提取子张量）的代价很小，因为<strong>它们不会导致内存重新分配</strong>；相反，它们（仅仅）分配一个新的张量对象，该对象具有不同的尺寸、存储偏移或步长。更改子张量同时也会对原始张量产生影响。所以我们可以克隆子张量得到新的张量（以避免这种影响）：<code>tensor.clone()</code>.Tensor的转置只改变尺寸和步长。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">some_tensor = torch.ones(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>)<br>some_tensor.shape, some_tensor.stride()<br><span class="hljs-comment"># 输出</span><br>(torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]), (<span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-number">1</span>))<br><br>some_tensor_t = some_tensor.transpose(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br>some_tensor_t.shape, some_tensor_t.stride()<br><span class="hljs-comment"># 输出</span><br>(torch.Size([<span class="hljs-number">5</span>, <span class="hljs-number">4</span>, <span class="hljs-number">3</span>]), (<span class="hljs-number">1</span>, <span class="hljs-number">5</span>, <span class="hljs-number">20</span>))<br><br></code></pre></td></tr></table></figure><h2 id="2-4-数据类型"><a href="#2-4-数据类型" class="headerlink" title="2.4 数据类型"></a>2.4 数据类型</h2><ul><li>torch.float32&#x2F;torch.float —— 32位浮点数:torch.FloatTensor &#x3D; torch.Tensor</li><li>torch.float64&#x2F;torch.double —— 64位双精度浮点数:torch.DoubleTensor</li><li>torch.float16&#x2F;torch.half —— 16位半精度浮点数</li><li>torch.int8 —— 带符号8位整数:torch.CharTensor</li><li>torch.uint8 —— 无符号8位整数:torch.ByteTensor</li><li>torch.int16&#x2F;torch.short —— 带符号16位整数</li><li>torch.int32&#x2F;torch.int —— 带符号32位整数</li><li>torch.int64&#x2F;torch.long —— 带符号64位整数</li></ul><p>通过访问<code>dtype</code>属性来获得张量的数据类型。而数据类型之间的转换可以通过<code>type</code>和<code>to</code>实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">short_points = torch.ones(<span class="hljs-number">10</span>, <span class="hljs-number">2</span>).short()<br>short_points = torch.ones(<span class="hljs-number">10</span>, <span class="hljs-number">2</span>).to(dtype=torch.short)<br></code></pre></td></tr></table></figure><h2 id="2-5-索引张量"><a href="#2-5-索引张量" class="headerlink" title="2.5 索引张量"></a>2.5 索引张量</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">some_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-number">6</span>))<br>some_list[:]     <span class="hljs-comment"># 所有元素</span><br>some_list[<span class="hljs-number">1</span>:<span class="hljs-number">4</span>]   <span class="hljs-comment"># 第1（含）到第4（不含）个元素</span><br>some_list[<span class="hljs-number">1</span>:]    <span class="hljs-comment"># 第1（含）个之后所有元素</span><br>some_list[:<span class="hljs-number">4</span>]    <span class="hljs-comment"># 第4（不含）个之前所有元素</span><br>some_list[:-<span class="hljs-number">1</span>]   <span class="hljs-comment"># 最末尾（不含）元素之前所有元素</span><br>some_list[<span class="hljs-number">1</span>:<span class="hljs-number">4</span>:<span class="hljs-number">2</span>] <span class="hljs-comment"># 范围1（含）到4（不含），步长为2的元素</span><br><br>points[<span class="hljs-number">1</span>:]    <span class="hljs-comment"># 第1行及之后所有行，（默认）所有列</span><br>points[<span class="hljs-number">1</span>:, :] <span class="hljs-comment"># 第1行及之后所有行，所有列</span><br>points[<span class="hljs-number">1</span>:, <span class="hljs-number">0</span>] <span class="hljs-comment"># 第1行及之后所有行，仅第0列</span><br><br></code></pre></td></tr></table></figure><h2 id="2-6-Pytorch与NumPy的互通性"><a href="#2-6-Pytorch与NumPy的互通性" class="headerlink" title="2.6 Pytorch与NumPy的互通性"></a>2.6 Pytorch与NumPy的互通性</h2><p>从PyTorch张量创建NumPy数组：<code>points_np = points.numpy()</code><br>从NumPy数组创建PyTorch张量：<code>points = torch.from_numpy(points_np)</code></p><h2 id="2-7-序列化张量"><a href="#2-7-序列化张量" class="headerlink" title="2.7 序列化张量"></a>2.7 序列化张量</h2><p>PyTorch内部使用<code>pickle</code>来序列化张量对象和实现用于存储的专用序列化代码。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 将points张量保存到ourpoints.t文件中</span><br>torch.save(points, <span class="hljs-string">&#x27;../../data/chapter2/ourpoints.t&#x27;</span>)<br><span class="hljs-comment"># 将points加载回来</span><br>points = torch.load(<span class="hljs-string">&#x27;../../data/chapter2/ourpoints.t&#x27;</span>)<br><br><span class="hljs-comment">## 上述例子可让你快速保存张量，但这个文件格式本身是不互通的，你只能用PyTorch读取它</span><br><span class="hljs-comment">## 对于需要（互通）的情况，你可以使用HDF5格式和库</span><br><span class="hljs-comment">## HDF5是一种可移植的、广泛支持的格式，用于表示以嵌套键值字典形式组织的序列化多维数组。</span><br><span class="hljs-comment">## Python通过h5py库支持HDF5，该库以NumPy数组的形式接收和返回数据。</span><br><br><span class="hljs-keyword">import</span> h5py<br><br>f = h5py.File(<span class="hljs-string">&#x27;../../data/chapter2/ourpoints.hdf5&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>dset = f.create_dataset(<span class="hljs-string">&#x27;coords&#x27;</span>, data=points.numpy())<br>f.close()<br><span class="hljs-comment"># coords是传入HDF5文件的键值</span><br><span class="hljs-comment"># 你可以索引在磁盘的数据并且仅访问你感兴趣的元素，例如你只想加载数据集中的最后两个点数据：</span><br>f = h5py.File(<span class="hljs-string">&#x27;../../data/chapter2/ourpoints.hdf5&#x27;</span>, <span class="hljs-string">&#x27;r&#x27;</span>)<br>dset = f[<span class="hljs-string">&#x27;coords&#x27;</span>]<br>last_points = dset[<span class="hljs-number">1</span>:]<br><br></code></pre></td></tr></table></figure><h2 id="2-8-将张量转移到GPU上运行"><a href="#2-8-将张量转移到GPU上运行" class="headerlink" title="2.8 将张量转移到GPU上运行"></a>2.8 将张量转移到GPU上运行</h2><p>PyTorch张量还具有设备（device）的概念，这是在设置计算机上放张量（tensor）数据的位置。</p><ul><li>通过为构造函数指定相应的参数，可以在GPU上创建张量：<code>points_gpu = torch.tensor([[1.0, 4.0], [2.0, 1.0], [3.0, 4.0]], device=&#39;cuda&#39;)</code>；同时可以通过提供device和dtype参数来同时更改位置和数据类型</li><li>也可以使用<code>to</code>方法将在CPU上创建的张量（tensor）复制到GPU：<code>points_gpu = points.to(device=&#39;cuda&#39;)</code></li><li>使用速记方法<code>cpu</code>和<code>cuda</code>代替to方法来实现相同的目标：<code>points_gpu = points.cuda(0)</code>和<code>points_cpu = points_gpu.cpu()</code></li></ul><h1 id="3-使用张量表示真实数据"><a href="#3-使用张量表示真实数据" class="headerlink" title="3 使用张量表示真实数据"></a>3 使用张量表示真实数据</h1><p>将异构的现实世界数据编码成浮点数张量以供神经网络使用。</p><ul><li>表格数据：表格中的每一行都独立于其他行，他们的顺序页没有任何关系</li><li>时间序列：存在严格的排序其他类型的数据。比如文本和音频。</li><li>文本数据：将文本信息编码为张量形式的技术为独热编码。<br>每个字符将由一个长度等于编码中字符数的向量表示。该向量除了有一个元素是1外其他全为0，这个1的索引对应该字符在字符集中的位置。接下来，在编码中建立单词到索引的映射，一般单词作为键，而整数作为值。独热编码时，你将使用此词典来有效地找到单词的索引。但由于独热编码不支持长篇文本，后续出现了嵌入的方式处理文本。<br><strong>【关于文本嵌入】</strong><br>用于同一上下文的单词映射到嵌入空间的邻近区域。生成的嵌入的一个有趣的方面是，相似的词不仅会聚在一起，还会与其他词保持一致的空间关系。如果你要使用“苹果”的嵌入向量，并加上和减去其他词的嵌入向量，就可以进行类比，例如<code>苹果 - 红色 - 甜 + 酸</code>，最后可能得到一个类似<code>柠檬</code>的向量。</li><li>图像数据：图像表示为按规则网格排列的标量集合。PyTorch模块处理图像数据需要将张量设置为C x H x W（分别为通道、高度和宽度）</li><li>体积数据：在CT（Computed Tomography）扫描等医学成像应用程序的情况下，通常需要处理从头到脚方向堆叠的图像序列，每个序列对应于整个身体的横截面。在CT扫描中，强度代表身体不同部位的密度：肺、脂肪、水、肌肉、骨骼，以密度递增的顺序排列，当在临床工作站上显示CT扫描时，会从暗到亮映射。根据穿过人体后到达检测器的X射线量计算每个点的密度，并使用一些复杂的数学运算将原始传感器数据反卷积（deconvolve）为完整体积数据。CT具有单个的强度通道，这类似于灰度图像。通过将单个2D切片堆叠到3D张量中，你可以构建表示对象的3D解剖结构的体积数据。</li></ul><h1 id="4-学习机制"><a href="#4-学习机制" class="headerlink" title="4 学习机制"></a>4 学习机制</h1><p>数据科学的流程</p><ul><li>得到很多好的数据</li><li>试图将这些数据可视化</li><li>选择有可能拟合数据的最简单的模型</li><li>划分数据，以便处理部分数据并保留独立的数据集用来验证</li><li>从试探性的偏心率和大小开始，然后进行迭代直到模型拟合观察结果为止</li><li>根据独立的数据集验证他的模型</li></ul><p><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter4/4.2.png" alt="模型的学习过程"></p><h2 id="4-1-一个反向传播的简单示例"><a href="#4-1-一个反向传播的简单示例" class="headerlink" title="4.1 一个反向传播的简单示例"></a>4.1 一个反向传播的简单示例</h2><p>通过链式法则向后传播导数，可以计算复合函数（模型函数和损失函数）相对于它们的最内层参数<code>w</code>和<code>b</code>的梯度。基本的要求是涉及到的函数都是可微分的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 定义模型</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">model</span>(<span class="hljs-params">t_u, w, b</span>):<br>    <span class="hljs-keyword">return</span> w * t_u + b<br><br><span class="hljs-comment"># 定义损失函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">loss_fn</span>(<span class="hljs-params">t_p, t_c</span>):<br>    squared_diffs = (t_p - t_c)**<span class="hljs-number">2</span><br>    <span class="hljs-keyword">return</span> squared_diffs.mean()<br><br><span class="hljs-comment"># 导数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dmodel_dw</span>(<span class="hljs-params">t_u, w, b</span>):<br>    <span class="hljs-keyword">return</span> t_u<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">dmodel_db</span>(<span class="hljs-params">t_u, w, b</span>):<br>    <span class="hljs-keyword">return</span> <span class="hljs-number">1.0</span><br><br><span class="hljs-comment"># 返回相对于 w 和 b 的梯度的函数</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">grad_fn</span>(<span class="hljs-params">t_u, t_c, t_p, w, b</span>):<br>    dloss_dw = dloss_fn(t_p, t_c) * dmodel_dw(t_u, w, b)<br>    dloss_db = dloss_fn(t_p, t_c) * dmodel_db(t_u, w, b)<br>    <span class="hljs-keyword">return</span> torch.stack([dloss_dw.mean(), dloss_db.mean()])<br><br><span class="hljs-comment"># 循环训练</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">training_loop</span>(<span class="hljs-params">n_epochs, learning_rate, params, t_u, t_c, </span><br><span class="hljs-params">                    print_params = <span class="hljs-literal">True</span>, verbose=<span class="hljs-number">1</span></span>):<br>    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, n_epochs + <span class="hljs-number">1</span>):<br>        w, b = params<br><br>        t_p = model(t_u, w, b) <span class="hljs-comment"># 前向传播</span><br>        loss = loss_fn(t_p, t_c)<br>        grad = grad_fn(t_u, t_c, t_p, w, b) <span class="hljs-comment"># 反向传播</span><br><br>        params = params - learning_rate * grad<br><br>        <span class="hljs-keyword">if</span> epoch % verbose == <span class="hljs-number">0</span>:<br>            <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Epoch %d, Loss %f&#x27;</span> % (epoch, <span class="hljs-built_in">float</span>(loss)))<br>            <span class="hljs-keyword">if</span> print_params:<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;    Params: &#x27;</span>, params)<br>                <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;    Grad  : &#x27;</span>, grad)<br>    <span class="hljs-keyword">return</span> params<br><br></code></pre></td></tr></table></figure><h2 id="4-2-Pytorch自动求导"><a href="#4-2-Pytorch自动求导" class="headerlink" title="4.2 Pytorch自动求导"></a>4.2 Pytorch自动求导</h2><p>一般来讲，所有PyTorch张量都有一个初始为空的名为<code>grad</code>的属性:<code>params.grad is None # True</code>。你可以将包含任意数量的张量的<code>require_grad</code>设置为<code>True</code>以及组合任何函数。在这种情况下，PyTorch会在沿着整个函数链（即计算图）计算损失的导数，并在这些张量（即计算图的叶节点）的grad属性中将这些导数值累积（accumulate）起来。<br><strong>!!!!【Attention】</strong><br>重复调用backward会导致导数在叶节点处累积。因此，如果提前调用了backward，然后再次计算损失并再次调用backward（如在训练循环中一样），那么在每个叶节点上的梯度会被累积（即求和）在前一次迭代计算出的那个叶节点上，导致梯度值不正确。为防止这种情况发生，你需要在每次迭代时将梯度显式清零:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">if</span> params.grad <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>    params.grad.zero_()<br></code></pre></td></tr></table></figure><p>torch模块有一个optim子模块，你可以在其中找到实现不同优化算法的类。这里有一个简短的清单：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim<br><span class="hljs-built_in">dir</span>(optim)<br><br><span class="hljs-comment"># 输出</span><br>[<span class="hljs-string">&#x27;ASGD&#x27;</span>,<br> <span class="hljs-string">&#x27;Adadelta&#x27;</span>,<br> <span class="hljs-string">&#x27;Adagrad&#x27;</span>,<br> <span class="hljs-string">&#x27;Adam&#x27;</span>,<br> <span class="hljs-string">&#x27;AdamW&#x27;</span>,<br> <span class="hljs-string">&#x27;Adamax&#x27;</span>,<br> <span class="hljs-string">&#x27;LBFGS&#x27;</span>,<br> <span class="hljs-string">&#x27;Optimizer&#x27;</span>,<br> <span class="hljs-string">&#x27;RMSprop&#x27;</span>,<br> <span class="hljs-string">&#x27;Rprop&#x27;</span>,<br> <span class="hljs-string">&#x27;SGD&#x27;</span>,<br> <span class="hljs-string">&#x27;SparseAdam&#x27;</span>,<br> ...<br>]<br></code></pre></td></tr></table></figure><p>深度神经网络可以近似复杂的函数，前提是神经元的数量（即参数量）足够高。参数越少，网络能够近似的函数越简单。因此，这里有一条规律：如果训练损失没有减少，则该模型对于数据来说太简单了。另一种可能性是训练数据中不包含有意义的信息以用于预测输出。<br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter4/4.12.png" alt="关于过拟合的举例"></p><h1 id="5-使用神经网络拟合数据"><a href="#5-使用神经网络拟合数据" class="headerlink" title="5 使用神经网络拟合数据"></a>5 使用神经网络拟合数据</h1><p>不管具体模型是什么，参数的更新方式都是一样的：反向传播误差然后通过计算损失关于参数的梯度来更新这些参数。神经网络具有非凸误差曲面主要是因为激活函数。组合神经元来逼近各种复杂函数的能力取决于每个神经元固有的线性和非线性行为的组合。深度神经网络可让你近似高度非线性的过程，而无需为它们建立明确的模型。</p><h2 id="5-1-神经元"><a href="#5-1-神经元" class="headerlink" title="5.1 神经元"></a>5.1 神经元</h2><p>深度学习的核心是神经网络，即能够通过简单函数的组合来表示复杂函数的数学实体。这些复杂函数的基本组成单元是神经元，如图5.2所示。从本质上讲，神经元不过是输入的线性变换（例如，输入乘以一个数[weight，权重]，再加上一个常数[偏置，bias]），然后再经过一个固定的非线性函数（称为激活函数）。<br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter5/5.2.png" alt="神经元：线性变换后再经过一个非线性函数"><br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter5/5.3.png" alt="一个三层的神经网络"></p><h2 id="5-2-激活函数"><a href="#5-2-激活函数" class="headerlink" title="5.2 激活函数"></a>5.2 激活函数</h2><p>激活函数的作用是将先前线性运算的输出聚集到给定范围内。一系列线性变换紧跟可微激活函数中可以构建出能近似高度非线性过程的模型，且可以通过梯度下降很好地估计出其参数。<br>激活函数的要求有：</p><ul><li>非线性</li><li>可微<br><img src="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/img/chapter5/5.5.png" alt="常用以及不是很常用的激活函数"></li></ul><h2 id="5-3-PyTorch的nn模块"><a href="#5-3-PyTorch的nn模块" class="headerlink" title="5.3 PyTorch的nn模块"></a>5.3 PyTorch的nn模块</h2><p>PyTorch有一个专门用于神经网络的完整子模块：<code>torch.nn</code>。该子模块包含创建各种神经网络体系结构所需的构建块。这些构建块在PyTorch术语中称为module（模块），在其他框架中称为layer（层）。nn中的任何模块都被编写成同时产生一个批次（即多个输入）的输出。这样进行批处理的主要原因是希望可以充分利用执行计算的计算资源。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. nn提供了一种通过nn.Sequential容器串联模块的简单方法</span><br>seq_model = nn.Sequential(<br>            nn.Linear(<span class="hljs-number">1</span>, <span class="hljs-number">13</span>),<br>            nn.Tanh(),<br>            nn.Linear(<span class="hljs-number">13</span>, <span class="hljs-number">1</span>))<br>seq_model<br><span class="hljs-comment">## 输出</span><br>Sequential(<br>  (<span class="hljs-number">0</span>): Linear(in_features=<span class="hljs-number">1</span>, out_features=<span class="hljs-number">13</span>, bias=<span class="hljs-literal">True</span>)<br>  (<span class="hljs-number">1</span>): Tanh()<br>  (<span class="hljs-number">2</span>): Linear(in_features=<span class="hljs-number">13</span>, out_features=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>)<br>)<br><span class="hljs-comment"># ——————————————————————————————————————————————————————————————————</span><br><br><span class="hljs-comment">## 1.1 Sequential中每个模块的名称都是该模块在参数中出现的顺序。</span><br><span class="hljs-comment">##      有趣的是，Sequential还可以接受OrderedDict作为参数</span><br><span class="hljs-comment">#       这样就可以给Sequential的每个模块命名</span><br><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> OrderedDict<br><br>seq_model = nn.Sequential(OrderedDict([<br>    (<span class="hljs-string">&#x27;hidden_linear&#x27;</span>, nn.Linear(<span class="hljs-number">1</span>, <span class="hljs-number">8</span>)),<br>    (<span class="hljs-string">&#x27;hidden_activation&#x27;</span>, nn.Tanh()),<br>    (<span class="hljs-string">&#x27;output_linear&#x27;</span>, nn.Linear(<span class="hljs-number">8</span>, <span class="hljs-number">1</span>))<br>]))<br><br>seq_model<br><span class="hljs-comment">## 输出</span><br>Sequential(<br>  (hidden_linear): Linear(in_features=<span class="hljs-number">1</span>, out_features=<span class="hljs-number">8</span>, bias=<span class="hljs-literal">True</span>)<br>  (hidden_activation): Tanh()<br>  (output_linear): Linear(in_features=<span class="hljs-number">8</span>, out_features=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">True</span>)<br>)<br><span class="hljs-comment"># ——————————————————————————————————————————————————————————————————</span><br><br><br><span class="hljs-comment"># 2. 调用model.parameters()可以得到第一线性模块和第二线性模块中的权重和偏差。</span><br><span class="hljs-comment">## 在本例中，我们可以通过打印形状来检查参数：</span><br>[param.shape <span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> seq_model.parameters()]<br><br><span class="hljs-comment">## 输出</span><br>[torch.Size([<span class="hljs-number">13</span>, <span class="hljs-number">1</span>]), torch.Size([<span class="hljs-number">13</span>]), torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">13</span>]), torch.Size([<span class="hljs-number">1</span>])]<br><span class="hljs-comment"># ——————————————————————————————————————————————————————————————————</span><br><br><span class="hljs-comment"># 3. 当你检查由几个子模块组成的模型的参数时，可以方便地通过其名称识别参数。这个方法叫做named_parameters</span><br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> seq_model.named_parameters():<br>    <span class="hljs-built_in">print</span>(name, param.shape)<br><br><span class="hljs-comment">##  输出</span><br><span class="hljs-number">0.</span>weight torch.Size([<span class="hljs-number">13</span>, <span class="hljs-number">1</span>])<br><span class="hljs-number">0.</span>bias torch.Size([<span class="hljs-number">13</span>])<br><span class="hljs-number">2.</span>weight torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">13</span>])<br><span class="hljs-number">2.</span>bias torch.Size([<span class="hljs-number">1</span>])<br><br></code></pre></td></tr></table></figure>]]></content>
    
    
    <categories>
      
      <category>Tutorial</category>
      
      <category>PyTorch</category>
      
    </categories>
    
    
    <tags>
      
      <tag>PyTorch</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
