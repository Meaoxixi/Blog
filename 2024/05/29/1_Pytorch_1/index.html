

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/icon.png">
  <link rel="icon" href="/img/icon.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Xi Wang">
  <meta name="keywords" content="">
  
    <meta name="description" content="本篇章主要记录的是官网提供的Tutorials和PyTorch官方文档(中文版)的重要内容，是Pytorch探索之旅的第2篇章啦！">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch_《Tutorials》">
<meta property="og:url" content="http://example.com/2024/05/29/1_Pytorch_1/index.html">
<meta property="og:site_name" content="Xi Wang">
<meta property="og:description" content="本篇章主要记录的是官网提供的Tutorials和PyTorch官方文档(中文版)的重要内容，是Pytorch探索之旅的第2篇章啦！">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2024-05-29T03:19:25.000Z">
<meta property="article:modified_time" content="2024-06-04T00:41:59.559Z">
<meta property="article:author" content="Xi Wang">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary_large_image">
  
  
  
  <title>PyTorch_《Tutorials》 - Xi Wang</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.2.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>Meao&#39;s Blog</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>Home</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-books"></i>
                <span>Archives</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>Categories</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>Tags</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>About</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/5.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="PyTorch_《Tutorials》"></span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2024-05-29 11:19" pubdate>
          May 29, 2024 am
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.1k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          35 mins
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">PyTorch_《Tutorials》</h1>
            
            
              <div class="markdown-body">
                
                <p>本篇章主要记录的是官网提供的<a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/">Tutorials</a>和<a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch官方文档(中文版)</a>的重要内容，是Pytorch探索之旅的第2篇章啦！</p>
<span id="more"></span>

<p>Reference materials include:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/">PyTorch官方文档(中文版)</a>: 我感觉这个文档的好处在于非常详尽的给出相关函数和模块的细致化定义介绍</li>
<li><a target="_blank" rel="noopener" href="https://pytorch.org/tutorials/">Tutorials</a>：感觉官网的这个文档是从Pytorch的使用框架去简要介绍</li>
<li>Andrew W. Traska撰写的<a target="_blank" rel="noopener" href="https://www.manning.com/books/grokking-deep-learning">《Grokking Deep Learning》</a>是开发强大模型和理解深度神经网络基础机制的重要资源</li>
<li>Ian Goodfellow, Yoshua Bengio和Aaron Courville的<a target="_blank" rel="noopener" href="https://www.deeplearningbook.org/">《Deep Learning》</a></li>
<li>清华翻译的<a target="_blank" rel="noopener" href="https://tangshusen.me/Deep-Learning-with-PyTorch-Chinese/">《Deep learning with PyTorch》</a></li>
</ul>
<h1 id="1-Quickstart"><a href="#1-Quickstart" class="headerlink" title="1 Quickstart"></a>1 Quickstart</h1><h2 id="1-1-Working-with-data"><a href="#1-1-Working-with-data" class="headerlink" title="1.1 Working with data"></a>1.1 Working with data</h2><p>PyTorch has two primitives to work with data: <code>torch.utils.data.DataLoader</code> and <code>torch.utils.data.Dataset</code>. <strong>Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset.</strong> PyTorch offers domain-specific libraries such as TorchText, TorchVision, and TorchAudio, all of which include datasets. The torchvision.datasets module contains Dataset objects for many real-world vision data like CIFAR, COCO (<a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/datasets.html">full list here</a>).<br>Every TorchVision Dataset includes two arguments: <code>transform</code> and <code>target_transform</code> to modify the samples and labels respectively.</p>
<h2 id="1-2-Creating-Models"><a href="#1-2-Creating-Models" class="headerlink" title="1.2 Creating Models"></a>1.2 Creating Models</h2><p>To define a neural network in PyTorch, we create a class that inherits from nn.Module. </p>
<ul>
<li>We define the layers of the network in the <code>__init__</code> function</li>
<li>Specify how data will pass through the network in the <code>forward</code> function</li>
<li>To accelerate operations in the neural network, we move it to the GPU or MPS if available.</li>
</ul>
<h2 id="1-3-Optimizing-the-Model-Parameters"><a href="#1-3-Optimizing-the-Model-Parameters" class="headerlink" title="1.3 Optimizing the Model Parameters"></a>1.3 Optimizing the Model Parameters</h2><p>To train a model, we need a loss function and an optimizer.</p>
<h2 id="1-4-Saving-Models"><a href="#1-4-Saving-Models" class="headerlink" title="1.4 Saving Models"></a>1.4 Saving Models</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">torch.save(model.state_dict(), <span class="hljs-string">&quot;model.pth&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Saved PyTorch Model State to model.pth&quot;</span>)<br></code></pre></td></tr></table></figure>

<h2 id="1-5-Loading-Models"><a href="#1-5-Loading-Models" class="headerlink" title="1.5 Loading Models"></a>1.5 Loading Models</h2><p>The process for loading a model includes re-creating the model structure and loading the state dictionary into it.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = NeuralNetwork().to(device)<br>model.load_state_dict(torch.load(<span class="hljs-string">&quot;model.pth&quot;</span>))<br></code></pre></td></tr></table></figure>
<h1 id="2-Tensors"><a href="#2-Tensors" class="headerlink" title="2 Tensors"></a>2 Tensors</h1><p>Tensors are similar to NumPy’s ndarrays, except that tensors can run on GPUs or other hardware accelerators.</p>
<h2 id="2-1-Initializing-a-Tensor"><a href="#2-1-Initializing-a-Tensor" class="headerlink" title="2.1 Initializing a Tensor"></a>2.1 Initializing a Tensor</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 1. Directly from data</span><br>data = [[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],[<span class="hljs-number">3</span>, <span class="hljs-number">4</span>]]<br>x_data = torch.tensor(data)<br><br><span class="hljs-comment"># 2. From a NumPy array</span><br>np_array = np.array(data)<br>x_np = torch.from_numpy(np_array)<br><br><span class="hljs-comment"># 3. From another tensor</span><br>x_ones = torch.ones_like(x_data) <span class="hljs-comment"># retains the properties of x_data</span><br>x_rand = torch.rand_like(x_data, dtype=torch.<span class="hljs-built_in">float</span>) <span class="hljs-comment"># overrides the datatype of x_data</span><br><br><span class="hljs-comment"># 4. With random or constant values</span><br>shape = (<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,)<br>rand_tensor = torch.rand(shape)<br>ones_tensor = torch.ones(shape)<br>zeros_tensor = torch.zeros(shape)<br></code></pre></td></tr></table></figure>
<h2 id="2-2-Attributes-of-a-Tensor"><a href="#2-2-Attributes-of-a-Tensor" class="headerlink" title="2.2 Attributes of a Tensor"></a>2.2 Attributes of a Tensor</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs pythohn">print(f&quot;Shape of tensor: &#123;tensor.shape&#125;&quot;)<br>print(f&quot;Datatype of tensor: &#123;tensor.dtype&#125;&quot;)<br>print(f&quot;Device tensor is stored on: &#123;tensor.device&#125;&quot;)<br></code></pre></td></tr></table></figure>

<h2 id="2-3-Operations-on-Tensor"><a href="#2-3-Operations-on-Tensor" class="headerlink" title="2.3 Operations on Tensor"></a>2.3 Operations on Tensor</h2><p>Over 100 tensor operations, including arithmetic, linear algebra, matrix manipulation (transposing, indexing, slicing), sampling and more are comprehensively described <a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/torch.html">here</a>.<br>By default, tensors are created on the CPU. We need to explicitly move tensors to the GPU using .to method (after checking for GPU availability). Keep in mind that copying large tensors across devices can be expensive in terms of time and memory!</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># We move our tensor to the GPU if available</span><br><span class="hljs-keyword">if</span> torch.cuda.is_available():<br>    tensor = tensor.to(<span class="hljs-string">&quot;cuda&quot;</span>)<br><br><span class="hljs-comment"># 1 Standard numpy-like indexing and slicing</span><br>tensor = torch.ones(<span class="hljs-number">4</span>, <span class="hljs-number">4</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First row: <span class="hljs-subst">&#123;tensor[<span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;First column: <span class="hljs-subst">&#123;tensor[:, <span class="hljs-number">0</span>]&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Last column: <span class="hljs-subst">&#123;tensor[..., -<span class="hljs-number">1</span>]&#125;</span>&quot;</span>)<br>tensor[:,<span class="hljs-number">1</span>] = <span class="hljs-number">0</span><br><span class="hljs-built_in">print</span>(tensor)<br><span class="hljs-comment">## 输出</span><br>First row: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>First column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>Last column: tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])<br>tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],<br>        [<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])<br><br><span class="hljs-comment"># 2 Joining tensors</span><br><span class="hljs-comment"># - `torch.cat`: Concatenates the given sequence along an existing dimension.</span><br><span class="hljs-comment"># - `torch.stack`: Concatenates a sequence of tensors along a new dimension.</span><br><br><span class="hljs-comment"># 3 Single-element tensors </span><br><span class="hljs-comment"># If you have a one-element tensor, for example by aggregating all values of a tensor into one value, you can convert it to a Python numerical value using `item()`</span><br></code></pre></td></tr></table></figure>
<h2 id="2-4-Bridge-with-NumPy"><a href="#2-4-Bridge-with-NumPy" class="headerlink" title="2.4 Bridge with NumPy"></a>2.4 Bridge with NumPy</h2><p>Tensors on the CPU and NumPy arrays can share their underlying memory locations, and changing one will change the other.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Tensor to NumPy array</span><br>n = t.numpy()<br><span class="hljs-comment"># NumPy array to Tensor</span><br>t = torch.from_numpy(n)<br><br><span class="hljs-comment">## Changes in the NumPy array reflects in the tensor.</span><br>n = np.ones(<span class="hljs-number">5</span>)<br>t = torch.from_numpy(n)<br>np.add(n, <span class="hljs-number">1</span>, out=n)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;t: <span class="hljs-subst">&#123;t&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;n: <span class="hljs-subst">&#123;n&#125;</span>&quot;</span>)<br><span class="hljs-comment">### 输出</span><br>t: tensor([<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>], dtype=torch.float64)<br>n: [<span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span> <span class="hljs-number">2.</span>]<br></code></pre></td></tr></table></figure>

<h1 id="3-Datasets-and-DataLoaders"><a href="#3-Datasets-and-DataLoaders" class="headerlink" title="3 Datasets and DataLoaders"></a>3 Datasets and DataLoaders</h1><h2 id="3-1-Loading-a-Dataset"><a href="#3-1-Loading-a-Dataset" class="headerlink" title="3.1 Loading a Dataset"></a>3.1 Loading a Dataset</h2><p>We can load the FashionMNIST Dataset with the following parameters:</p>
<ul>
<li>root is the path where the train&#x2F;test data is stored,</li>
<li>train specifies training or test dataset,</li>
<li>download&#x3D;True downloads the data from the internet if it’s not available at root.</li>
<li>transform and target_transform specify the feature and label transformations<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> Dataset<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><br>training_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br><br>test_data = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">False</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor()<br>)<br></code></pre></td></tr></table></figure></li>
</ul>
<h2 id="3-2-Iterating-and-Visualizing-the-Dataset"><a href="#3-2-Iterating-and-Visualizing-the-Dataset" class="headerlink" title="3.2 Iterating and Visualizing the Dataset"></a>3.2 Iterating and Visualizing the Dataset</h2><p>We can index Datasets manually like a list: training_data[index]. We use matplotlib to visualize some samples in our training data.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python">labels_map = &#123;<br>    <span class="hljs-number">0</span>: <span class="hljs-string">&quot;T-Shirt&quot;</span>,<br>    <span class="hljs-number">1</span>: <span class="hljs-string">&quot;Trouser&quot;</span>,<br>    <span class="hljs-number">2</span>: <span class="hljs-string">&quot;Pullover&quot;</span>,<br>    <span class="hljs-number">3</span>: <span class="hljs-string">&quot;Dress&quot;</span>,<br>    <span class="hljs-number">4</span>: <span class="hljs-string">&quot;Coat&quot;</span>,<br>    <span class="hljs-number">5</span>: <span class="hljs-string">&quot;Sandal&quot;</span>,<br>    <span class="hljs-number">6</span>: <span class="hljs-string">&quot;Shirt&quot;</span>,<br>    <span class="hljs-number">7</span>: <span class="hljs-string">&quot;Sneaker&quot;</span>,<br>    <span class="hljs-number">8</span>: <span class="hljs-string">&quot;Bag&quot;</span>,<br>    <span class="hljs-number">9</span>: <span class="hljs-string">&quot;Ankle Boot&quot;</span>,<br>&#125;<br>figure = plt.figure(figsize=(<span class="hljs-number">8</span>, <span class="hljs-number">8</span>))<br>cols, rows = <span class="hljs-number">3</span>, <span class="hljs-number">3</span><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, cols * rows + <span class="hljs-number">1</span>):<br>    sample_idx = torch.randint(<span class="hljs-built_in">len</span>(training_data), size=(<span class="hljs-number">1</span>,)).item()<br>    img, label = training_data[sample_idx]<br>    figure.add_subplot(rows, cols, i)<br>    plt.title(labels_map[label])<br>    plt.axis(<span class="hljs-string">&quot;off&quot;</span>)<br>    plt.imshow(img.squeeze(), cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
<h2 id="3-3-Creating-a-Custom-Dataset-for-your-files"><a href="#3-3-Creating-a-Custom-Dataset-for-your-files" class="headerlink" title="3.3 Creating a Custom Dataset for your files"></a>3.3 Creating a Custom Dataset for your files</h2><p>A custom Dataset class must implement three functions: <code>__init__</code>, <code>__len__</code>, and <code>__getitem__</code>.<br>The FashionMNIST images are stored in a directory <code>img_dir</code>, and their labels are stored separately in a CSV file <code>annotations_file</code>.<br>The labels.csv file looks like:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">tshirt1.jpg, <span class="hljs-number">0</span><br>tshirt2.jpg, <span class="hljs-number">0</span><br>......<br>ankleboot999.jpg, <span class="hljs-number">9</span><br></code></pre></td></tr></table></figure>
<p>Code as fellow:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd<br><span class="hljs-keyword">from</span> torchvision.io <span class="hljs-keyword">import</span> read_image<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">CustomImageDataset</span>(<span class="hljs-title class_ inherited__">Dataset</span>):<br>    <span class="hljs-comment"># The __init__ function is run once when instantiating the Dataset object.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, annotations_file, img_dir, transform=<span class="hljs-literal">None</span>, target_transform=<span class="hljs-literal">None</span></span>):<br>        self.img_labels = pd.read_csv(annotations_file)<br>        self.img_dir = img_dir<br>        self.transform = transform<br>        self.target_transform = target_transform<br><br>    <span class="hljs-comment"># The __len__ function returns the number of samples in our dataset.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__len__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">len</span>(self.img_labels)<br><br>    <span class="hljs-comment"># The __getitem__ function loads and returns a sample from the dataset at the given index idx.</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">self, idx</span>):<br>        <span class="hljs-comment"># Based on the index, it identifies the image’s location on disk</span><br>        <span class="hljs-comment"># converts that to a tensor using read_image</span><br>        <span class="hljs-comment"># retrieves the corresponding label from the csv data in self.img_labels</span><br>        <span class="hljs-comment"># calls the transform functions on them (if applicable)</span><br>        <span class="hljs-comment"># and returns the tensor image and corresponding label in a tuple.</span><br>        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, <span class="hljs-number">0</span>])<br>        image = read_image(img_path)<br>        label = self.img_labels.iloc[idx, <span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">if</span> self.transform:<br>            image = self.transform(image)<br>        <span class="hljs-keyword">if</span> self.target_transform:<br>            label = self.target_transform(label)<br>        <span class="hljs-keyword">return</span> image, label<br></code></pre></td></tr></table></figure>
<h2 id="3-4-Preparing-your-data-for-training-with-DataLoaders"><a href="#3-4-Preparing-your-data-for-training-with-DataLoaders" class="headerlink" title="3.4 Preparing your data for training with DataLoaders"></a>3.4 Preparing your data for training with DataLoaders</h2><p>The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, and use Python’s multiprocessing to speed up data retrieval.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader<br><br>train_dataloader = DataLoader(training_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br>test_dataloader = DataLoader(test_data, batch_size=<span class="hljs-number">64</span>, shuffle=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure>
<h2 id="3-5-Iterate-through-the-DataLoader"><a href="#3-5-Iterate-through-the-DataLoader" class="headerlink" title="3.5 Iterate through the DataLoader"></a>3.5 Iterate through the DataLoader</h2><p>We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns <strong>a batch of train_features and train_labels</strong> (containing batch_size&#x3D;64 features and labels respectively). Because we specified shuffle&#x3D;True, after we iterate over all batches the data is shuffled (for finer-grained control over the data loading order, take a look at Samplers).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Display image and label.</span><br>train_features, train_labels = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">iter</span>(train_dataloader))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Feature batch shape: <span class="hljs-subst">&#123;train_features.size()&#125;</span>&quot;</span>)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Labels batch shape: <span class="hljs-subst">&#123;train_labels.size()&#125;</span>&quot;</span>)<br>img = train_features[<span class="hljs-number">0</span>].squeeze()<br>label = train_labels[<span class="hljs-number">0</span>]<br>plt.imshow(img, cmap=<span class="hljs-string">&quot;gray&quot;</span>)<br>plt.show()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;Label: <span class="hljs-subst">&#123;label&#125;</span>&quot;</span>)<br><br><span class="hljs-comment">## 输出</span><br><span class="hljs-comment">## 一张图示</span><br>Feature batch shape: torch.Size([<span class="hljs-number">64</span>, <span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Labels batch shape: torch.Size([<span class="hljs-number">64</span>])<br>Label: <span class="hljs-number">5</span><br></code></pre></td></tr></table></figure>

<h1 id="4-Transformers"><a href="#4-Transformers" class="headerlink" title="4 Transformers"></a>4 Transformers</h1><p>We use transforms to perform some manipulation of the data and make it suitable for training. All TorchVision datasets have two parameters <code>transform</code> to modify the features and <code>target_transform</code> to modify the labels that accept callables containing the transformation logic. <a target="_blank" rel="noopener" href="https://pytorch.org/vision/stable/transforms.html">More resource in torchvision.transforms module </a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torchvision <span class="hljs-keyword">import</span> datasets<br><span class="hljs-keyword">from</span> torchvision.transforms <span class="hljs-keyword">import</span> ToTensor, Lambda<br><br>ds = datasets.FashionMNIST(<br>    root=<span class="hljs-string">&quot;data&quot;</span>,<br>    train=<span class="hljs-literal">True</span>,<br>    download=<span class="hljs-literal">True</span>,<br>    transform=ToTensor(),<br>    <span class="hljs-comment"># Lambda transforms apply any user-defined lambda function. </span><br>    target_transform=Lambda(<span class="hljs-keyword">lambda</span> y: torch.zeros(<span class="hljs-number">10</span>, dtype=torch.<span class="hljs-built_in">float</span>).scatter_(<span class="hljs-number">0</span>, torch.tensor(y), value=<span class="hljs-number">1</span>))<br>)<br></code></pre></td></tr></table></figure>

<h1 id="5-Build-Model"><a href="#5-Build-Model" class="headerlink" title="5 Build Model"></a>5 Build Model</h1><ul>
<li>The <code>nn.Flatten</code> layer converts each 2D 28x28 image into a contiguous array of 784 pixel values.</li>
<li>The <code>nn.Linear</code> layer is a module that applies a linear transformation on the input using its stored weights and biases.</li>
<li>The <code>nn.ReLU</code> creates the complex mappings between the model’s inputs and outputs.</li>
<li><code>nn.Sequential</code> is an ordered container of modules.<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">seq_modules = nn.Sequential(<br>    flatten,<br>    layer1,<br>    nn.ReLU(),<br>    nn.Linear(<span class="hljs-number">20</span>, <span class="hljs-number">10</span>)<br>)<br>input_image = torch.rand(<span class="hljs-number">3</span>,<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)<br>logits = seq_modules(input_image)<br></code></pre></td></tr></table></figure></li>
<li><code>nn.Softmax</code> : The logits are scaled to values [0, 1] representing the model’s predicted probabilities for each class.</li>
</ul>
<h1 id="6-Automatic-Differentiation"><a href="#6-Automatic-Differentiation" class="headerlink" title="6 Automatic Differentiation"></a>6 Automatic Differentiation</h1><p><a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/notes/autograd.html">Autograd mechanics</a></p>
<h1 id="7-Optimization-Loop"><a href="#7-Optimization-Loop" class="headerlink" title="7 Optimization Loop"></a>7 Optimization Loop</h1><p>Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (loss), collects the derivatives of the error with respect to its parameters (as we saw in the previous section), and optimizes these parameters using gradient descent.<br>We define the following hyperparameters for training:</p>
<ul>
<li><strong>Number of Epochs</strong>: the number times to iterate over the dataset</li>
<li><strong>Batch Size</strong>: the number of data samples propagated through the network before the parameters are updated</li>
<li><strong>Learning Rate</strong>: how much to update models parameters at each batch&#x2F;epoch. Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training</li>
</ul>
<p>Each iteration of the optimization loop is called an epoch.And it  consists of two main parts: </p>
<ul>
<li>The Train Loop：iterate over the training dataset and try to converge to optimal parameters.</li>
<li>The Validation&#x2F;Test Loop：iterate over the test dataset to check if model performance is improving.</li>
</ul>
<p>Common loss functions include <code>nn.MSELoss</code> (Mean Square Error) for regression tasks, and <code>nn.NLLLoss</code> (Negative Log Likelihood) for classification. <code>nn.CrossEntropyLoss</code> combines <code>nn.LogSoftmax</code> and <code>nn.NLLLoss</code>.</p>
<p>Optimization is the process of adjusting model parameters to reduce model error in each training step. </p>
<iframe width="560" height="315" src="https://youtu.be/tIeHLnjs5U8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<h1 id="8-Save-Load-and-Use-Model"><a href="#8-Save-Load-and-Use-Model" class="headerlink" title="8 Save, Load and Use Model"></a>8 Save, Load and Use Model</h1><p>PyTorch models store the learned parameters in an internal state dictionary, called <code>state_dict</code>. These can be persisted via the <code>torch.save</code> method:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">model = models.vgg16(weights=<span class="hljs-string">&#x27;IMAGENET1K_V1&#x27;</span>)<br>torch.save(model.state_dict(), <span class="hljs-string">&#x27;model_weights.pth&#x27;</span>)<br><br><span class="hljs-comment"># To load model weights, you need to create an instance of the same model first, and then load the parameters </span><br>model.load_state_dict(torch.load(<span class="hljs-string">&#x27;model_weights.pth&#x27;</span>))<br><span class="hljs-comment"># be sure to call model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode.</span><br>model.<span class="hljs-built_in">eval</span>()<br></code></pre></td></tr></table></figure>

<h1 id="9-PACKAGE参考"><a href="#9-PACKAGE参考" class="headerlink" title="9 PACKAGE参考"></a>9 PACKAGE参考</h1><h2 id="9-1-torch"><a href="#9-1-torch" class="headerlink" title="9.1 torch"></a>9.1 torch</h2><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch/">torch</a>包含了多维张量的数据结构以及基于其上的多种数学操作。另外，它也提供了多种工具，其中一些可以更有效地对张量和任意类型进行序列化。</p>
<h3 id="9-1-1-Tensor"><a href="#9-1-1-Tensor" class="headerlink" title="9.1.1 Tensor"></a>9.1.1 Tensor</h3><ul>
<li><code>torch.is_tensor(obj)</code></li>
<li><code>torch.is_storage(obj)</code></li>
<li><code>torch.numel(input)</code>: 返回input张量中的元素个数</li>
<li><code>torch.set_printoptions(precision=None, threshold=None, edgeitems=None, linewidth=None, profile=None)</code><ul>
<li>设置打印选项。</li>
<li>precision：浮点数输出的精度位数 (默认为8)</li>
<li>threshold：阈值，触发汇总显示而不是完全显示(repr)的数组元素的总数 （默认为1000）</li>
<li>edgeitems：汇总显示中，每维（轴）两端显示的项数（默认值为3）</li>
<li>linewidth：用于插入行间隔的每行字符数（默认为80）。Thresholded matricies will ignore this parameter.</li>
<li>profile：pretty打印的完全默认值。 可以覆盖上述所有选项 (默认为short, full)</li>
</ul>
</li>
</ul>
<h3 id="9-1-2-创建操作"><a href="#9-1-2-创建操作" class="headerlink" title="9.1.2 创建操作"></a>9.1.2 创建操作</h3><ul>
<li><code>torch.eye(n, m=None, out=None)</code>: 返回一个nxn的2维张量，对角线位置全1，其它位置全0</li>
<li><code>torch.from_numpy(ndarray)</code></li>
<li><code>torch.linspace(start, end, steps=100, out=None)</code>: 返回一个1维张量，包含在区间start和end上均匀间隔的steps个点。 输出1维张量的长度为steps</li>
<li><code>torch.logspace(start, end, steps=100, out=None)</code></li>
<li><code>torch.ones(*sizes, out=None)</code></li>
<li><code>torch.rand(*sizes, out=None)</code>: 返回一个张量，包含了从区间<code>[0,1)</code>的均匀分布中抽取的一组随机数，形状由可变参数sizes 定义</li>
<li><code>torch.randn(*sizes, out=None)</code>: 返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数，形状由可变参数sizes定义</li>
<li><code>torch.randperm(n, out=None)</code>: 给定参数n，返回一个从0 到n -1 的随机整数排列</li>
<li><code>torch.arange(start, end, step=1, out=None)</code>: 包含从start到end，以step为步长的一组序列值(默认步长为1)</li>
<li><code>torch.zeros(*sizes, out=None)</code></li>
</ul>
<h3 id="9-1-3-索引-切片-连接-换位"><a href="#9-1-3-索引-切片-连接-换位" class="headerlink" title="9.1.3 索引,切片,连接,换位"></a>9.1.3 索引,切片,连接,换位</h3><ul>
<li><code>torch.cat(inputs, dimension=0)</code></li>
<li><code>torch.chunk(tensor, chunks, dim=0)</code>: 在给定维度(轴)上将输入张量进行分块儿</li>
<li><code>torch.gather(input, dim, index, out=None)</code>: 沿给定轴dim，将输入索引张量index指定位置的值进行聚合</li>
<li><code>torch.index_select(input, dim, index, out=None)</code>: 沿着指定维度对输入进行切片，取index中指定的相应项(index为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量_Tensor_有相同的维度(在指定轴上)。<strong>注意： 返回的张量不与原始张量共享内存空间</strong></li>
<li><code>torch.masked_select(input, mask, out=None)</code></li>
<li><code>torch.nonzero(input, out=None)</code>: 返回一个包含输入input中非零元素索引的张量</li>
<li><code>torch.split(tensor, split_size, dim=0)</code>: 将输入张量分割成相等形状的chunks（如果可分）</li>
<li><code>torch.squeeze(input, dim=None, out=None)</code>: 将输入张量形状中的1去除并返回</li>
<li><code>torch.stack(sequence, dim=0)</code>: 沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状</li>
<li><code>torch.t</code>: 输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数<code>transpose(input, 0, 1)</code>的简写函数</li>
<li><code>torch.transpose(input, dim0, dim1, out=None)</code>: 返回输入矩阵input的转置。交换维度dim0和dim1。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改</li>
<li><code>torch.unbind</code>: 移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片</li>
<li><code>torch.unsqueeze(input, dim, out=None)</code>: 返回一个新的张量，对输入的制定位置插入维度 1</li>
</ul>
<h3 id="9-1-4-随机抽样"><a href="#9-1-4-随机抽样" class="headerlink" title="9.1.4 随机抽样"></a>9.1.4 随机抽样</h3><ul>
<li><code>torch.manual_seed(seed)</code></li>
<li><code>torch.initial_seed()</code></li>
<li><code>torch.get_rng_state()</code></li>
<li><code>torch.set_rng_state(new_state)</code></li>
<li><code>torch.default_generator = &lt;torch._C.Generator object&gt;</code></li>
<li><code>torch.bernoulli(input, out=None)</code>: 从伯努利分布中抽取二元随机数(0 或者 1)。输入张量须包含用于抽取上述二元随机值的概率</li>
<li><code>torch.multinomial(input, num_samples,replacement=False, out=None)</code>: 返回一个张量，每行包含从input相应行中定义的多项分布中抽取的num_samples个样本</li>
<li><code>torch.normal(means, std, out=None)</code>: 返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数</li>
</ul>
<h3 id="9-1-5-序列化"><a href="#9-1-5-序列化" class="headerlink" title="9.1.5 序列化"></a>9.1.5 序列化</h3><ul>
<li><code>torch.save(obj, f, pickle_module=&lt;module &#39;pickle&#39; from &#39;/home/jenkins/miniconda/lib/python3.5/pickle.py&#39;&gt;, pickle_protocol=2)</code>: 保存一个对象到一个硬盘文件上</li>
<li><code>torch.load</code></li>
</ul>
<h3 id="9-1-6-并行化"><a href="#9-1-6-并行化" class="headerlink" title="9.1.6 并行化"></a>9.1.6 并行化</h3><ul>
<li><code>torch.get_num_threads()</code>：获得用于并行化CPU操作的OpenMP线程数</li>
<li><code>torch.set_num_threads()</code>: 设定用于并行化CPU操作的OpenMP线程数</li>
</ul>
<h3 id="9-1-7-数学操作"><a href="#9-1-7-数学操作" class="headerlink" title="9.1.7 数学操作"></a>9.1.7 数学操作</h3><ul>
<li><code>torch.abs(input, out=None)</code>: 计算输入张量的每个元素绝对值</li>
<li><code>torch.sin(input, out=None)</code></li>
<li><code>torch.sinh(input, out=None)</code>: 双曲正弦</li>
<li><code>torch.cos(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的余弦</li>
<li><code>torch.cosh(input, out=None)</code>: 双曲余弦</li>
<li><code>torch.tan(input, out=None)</code></li>
<li><code>torch.tanh(input, out=None)</code>:双曲正切</li>
<li><code>torch.acos(input, out=None)</code>: 返回一个新张量，包含输入张量每个元素的反余弦</li>
<li><code>torch.asin(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的反正弦函数</li>
<li><code>torch.atan(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的反正切函数</li>
<li><code>torch.atan2(input1, input2, out=None)</code>: 返回一个新张量，包含两个输入张量input1和input2的反正切函数</li>
<li><code>torch.add(input, value, out=None)</code></li>
<li><code>torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None)</code>: 用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor</li>
<li><code>torch.addcmul(tensor, value=1, tensor1, tensor2, out=None)</code>: 用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensor</li>
<li><code>torch.ceil(input, out=None)</code>: 天井函数，对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数，并返回结果到输出</li>
<li><code>torch.floor(input, out=None)</code>: 床函数,返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数</li>
<li><code>torch.round(input, out=None)</code>: 返回一个新张量，将输入input张量每个元素舍入到最近的整数</li>
<li><code>torch.sign(input, out=None)</code>: 符号函数：返回一个新张量，包含输入input张量每个元素的正负</li>
<li><code>torch.trunc(input, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数，其比x更接近零。简而言之，有符号数的小数部分被舍弃)</li>
<li><code>torch.clamp(input, min, max, out=None)</code>: 将输入input张量每个元素的夹紧到区间 [min,max]，并返回结果到一个新张量</li>
<li><code>torch.div(input, value, out=None)</code>: 将input逐元素除以标量值value，并返回结果到输出张量out</li>
<li><code>torch.exp(tensor, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的指数</li>
<li><code>torch.fmod(input, divisor, out=None)</code>: 计算除法余数，其结果的符号与 input 相同</li>
<li><code>torch.remainder(input, divisor, out=None)</code>: 返回一个新张量，包含输入input张量每个元素的除法余数，其结果与 divisor 相同</li>
<li><code>torch.frac(tensor, out=None)</code>: 返回每个元素的分数部分</li>
<li><code>torch.lerp(start, end, weight, out=None)</code>: 对两个张量以start，end做线性插值， 将结果返回到输出张量。out &#x3D; start +weight * (end - start)</li>
<li><code>torch.log(input, out=None)</code>: 计算input的自然对数</li>
<li><code>torch.log1p(input, out=None) </code>: 计算input+1的自然对数</li>
<li><code>torch.mul(input, value, out=None)</code>: 用标量值value乘以输入input的每个元素，并返回一个新的结果张量</li>
<li><code>torch.neg(input, out=None)</code>: 返回一个新张量，包含输入input张量按元素取负</li>
<li><code>torch.pow(input, exponent, out=None)</code>: 对输入input的按元素求exponent次幂值，并返回结果张量</li>
<li><code>torch.reciprocal(input, out=None)</code>: 倒数</li>
<li><code>torch.rsqrt(input, out=None)</code>: 平方根倒数</li>
<li><code>torch.sqrt(input, out=None)</code>: 平方根</li>
<li><code>torch.sigmoid(input, out=None)</code></li>
</ul>
<h3 id="9-1-8-Reduction-Ops-缩减操作"><a href="#9-1-8-Reduction-Ops-缩减操作" class="headerlink" title="9.1.8 Reduction Ops(缩减操作)"></a>9.1.8 Reduction Ops(缩减操作)</h3><ul>
<li><code>torch.cumprod(input, dim, out=None)</code>: 返回输入沿指定维度的累积</li>
<li><code>torch.prod(input)</code>: 返回输入张量input 所有元素的积</li>
<li><code>torch.cumsum(input, dim, out=None)</code>: 返回输入沿指定维度的累和</li>
<li><code>torch.sum(input)</code>: 返回输入张量input 所有元素的和</li>
<li><code>torch.dist(input, other, p=2, out=None)</code>: 返回 (input - other) 的 p范数</li>
<li><code>torch.mean(input)</code></li>
<li><code>torch.median(input, dim=-1, values=None, indices=None)</code>: 返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的LongTensor</li>
<li><code>torch.mode(input, dim=-1, values=None, indices=None)</code>: 返回给定维dim上，每行的众数值。 同时返回一个LongTensor，包含众数职的索引</li>
<li><code>torch.norm(input, p=2)</code>: 返回输入张量input 的p 范数</li>
<li><code>torch.std(input)</code>: 返回输入张量input 所有元素的标准差</li>
<li><code>torch.var(input)</code>: 返回输入张量所有元素的方差</li>
</ul>
<h3 id="9-1-8-Comparison-Ops-比较操作"><a href="#9-1-8-Comparison-Ops-比较操作" class="headerlink" title="9.1.8 Comparison Ops(比较操作)"></a>9.1.8 Comparison Ops(比较操作)</h3><ul>
<li><code>torch.eq(input, other, out=None)</code>：比较元素相等性, 返回一个<code>torch.ByteTensor</code>张量，包含了每个位置的比较结果(相等为1，不等为0 )</li>
<li><code>torch.equal(tensor1, tensor2)</code>: 如果两个张量有相同的形状和元素值，则返回True ，否则 False</li>
<li><code>torch.ge(input, other, out=None)</code>: 逐元素比较input和other，即是否 input&gt;&#x3D;other. 返回一个 torch.ByteTensor 张量，包含了每个位置的比较结果(是否 input &gt;&#x3D; other )</li>
<li><code>torch.gt(input, other, out=None)</code>: 逐元素比较input和other ， 即是否input&gt;other. 如果两个张量有相同的形状和元素值，则返回True ，否则 False</li>
<li><code>torch.le(input, other, out=None)</code>: 逐元素比较input和other ， 即是否input&lt;&#x3D;other</li>
<li><code>torch.lt(input, other, out=None)</code>: 逐元素比较input和other ， 即是否 input &lt; other</li>
<li><code>torch.ne(input, other, out=None)</code>: 逐元素比较input和other ， 即是否 input!&#x3D;other</li>
<li><code>torch.kthvalue(input, k, dim=None, out=None)</code>: 取输入张量input指定维上第k 个最小值</li>
<li><code>torch.max()</code></li>
<li><code>torch.min(input)</code></li>
<li><code>torch.sort(input, dim=None, descending=False, out=None)</code>: 对输入张量input沿着指定维按升序排序</li>
<li><code>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None)</code>: 沿给定dim维度返回输入张量input中 k 个最大值</li>
</ul>
<h3 id="9-1-9-Other-Operations-其他操作"><a href="#9-1-9-Other-Operations-其他操作" class="headerlink" title="9.1.9 Other Operations(其他操作)"></a>9.1.9 Other Operations(其他操作)</h3><ul>
<li><code>torch.cross(input, other, dim=-1, out=None)</code>: 返回沿着维度dim上，两个张量input和other的向量积（叉积）</li>
<li><code>torch.diag(input, diagonal=0, out=None)</code><ul>
<li>如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵</li>
<li>如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量</li>
</ul>
</li>
<li><code>torch.histc(input, bins=100, min=0, max=0, out=None)</code>: 计算输入张量的直方图. 如果min和max都为0, 则利用数据中的最大最小值作为边界</li>
<li><code>torch.renorm(input, p, dim, maxnorm, out=None)</code>: 返回一个张量，包含规范化后的各个子张量，使得沿着dim维划分的各子张量的p范数小于maxnorm</li>
<li><code>torch.trace(input)</code>: 返回输入2维矩阵对角线元素的和</li>
<li><code>torch.tril(input, k=0, out=None)</code>: 返回一个张量out，包含输入矩阵(2D张量)的下三角部分，out其余部分被设为0</li>
<li><code>torch.triu(input, k=0, out=None)</code>: 返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为0</li>
</ul>
<h3 id="9-1-10-BLAS-and-LAPACK-Operations"><a href="#9-1-10-BLAS-and-LAPACK-Operations" class="headerlink" title="9.1.10 BLAS and LAPACK Operations"></a>9.1.10 BLAS and LAPACK Operations</h3><ul>
<li><code>torch.addbmm(beta=1, mat, alpha=1, batch1, batch2, out=None)</code>: 执行mat &#x3D; beta * mat + alpha * batch1.bmm(batch2) 操作。<ul>
<li>mat：二维张量（n, p）</li>
<li>batch1：三维张量（batch_size, n, m）</li>
<li>batch2：三维张量（batch_size, m, p）</li>
</ul>
</li>
<li><code>torch.baddbmm(beta=1, mat, alpha=1, batch1, batch2, out=None)</code>: 执行 mat &#x3D; beta * mat + alpha * torch.bmm(batch1, batch2) 操作<ul>
<li>mat：三维张量（batch_size, n, p）</li>
<li>batch1：三维张量（batch_size, n, m）</li>
<li>batch2：三维张量（batch_size, m, p）</li>
</ul>
</li>
<li><code>torch.bmm(batch1, batch2, out=None)</code>: 对存储在两个批batch1和batch2内的矩阵进行批矩阵乘操作<ul>
<li>batch1：三维张量（batch_size, n, m）</li>
<li>batch2：三维张量（batch_size, m, p）</li>
</ul>
</li>
<li><code>torch.mm(mat1, mat2, out=None)</code>: 执行二维矩阵乘法</li>
<li><code>torch.addmm(beta=1, mat, alpha=1, mat1, mat2, out=None)</code>: 适用于将批量矩阵乘法的结果与一个二维矩阵按比例相加</li>
<li><code>torch.mv(mat, vec, out=None)</code>: 对矩阵mat和向量vec进行相乘</li>
<li><code>torch.addmv(beta=1, tensor, alpha=1, mat, vec, out=None)</code>: 向量相乘</li>
<li><code>torch.addr(beta=1, mat, alpha=1, vec1, vec2, out=None)</code>：张量积</li>
<li><code>torch.btrifact(A, info=None)</code>: 返回一个元组，包含LU 分解和pivots</li>
<li><code>torch.btrisolve(b, LU_data, LU_pivots)</code>: 返回线性方程组Ax&#x3D;b的LU解</li>
<li><code>torch.dot(tensor1, tensor2)</code></li>
<li><code>torch.eig(a, eigenvectors=False, out=None)</code>: 计算<strong>实方阵a</strong>的特征值和特征向量</li>
<li><code>torch.gels(B, A, out=None)</code>: 对形如m×n的满秩矩阵a计算其最小二乘和最小范数问题的解</li>
<li><code>torch.geqrf(input, out=None)</code>: 直接调用LAPACK的底层函数</li>
<li><code>torch.ger(vec1, vec2, out=None)</code>: 计算两向量vec1,vec2的张量积</li>
<li><code>torch.gesv(B, A, out=None)</code>: 返回线性方程组AX&#x3D;B的解</li>
<li><code>torch.inverse(input, out=None)</code>: 对方阵输入input取逆</li>
<li><code>torch.svd(input, some=True, out=None)</code>:  返回对形如<code>n×m</code>的实矩阵<code>A</code>进行奇异值分解的结果</li>
<li><code>torch.symeig(input, eigenvectors=False, upper=True, out=None)</code>: 返回<strong>实对称矩阵</strong>input的特征值和特征向量</li>
</ul>
<h2 id="9-2-torch-Tensor"><a href="#9-2-torch-Tensor" class="headerlink" title="9.2 torch.Tensor"></a>9.2 torch.Tensor</h2><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Tensor/">torch.Tensor</a>是一种包含单一数据类型元素的多维矩阵。</p>
<p>！注意： 会改变tensor的函数操作会用一个下划线后缀来标示。比如，torch.FloatTensor.abs_()会在原地计算绝对值，并返回改变后的tensor，而tensor.FloatTensor.abs()将会在一个新的tensor中计算结果。</p>
<h2 id="9-3-torch-Storage"><a href="#9-3-torch-Storage" class="headerlink" title="9.3 torch.Storage"></a>9.3 torch.Storage</h2><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/Storage/">torch.Storage</a>是一个单一数据类型的连续一维数组。</p>
<h2 id="9-4-torch-nn"><a href="#9-4-torch-nn" class="headerlink" title="9.4 torch.nn"></a>9.4 torch.nn</h2><p><a target="_blank" rel="noopener" href="https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-nn/#_1">torch.nn</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/Tutorial/" class="category-chain-item">Tutorial</a>
  
  
    <span>></span>
    
  <a href="/categories/Tutorial/PyTorch/" class="category-chain-item">PyTorch</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/PyTorch/" class="print-no-link">#PyTorch</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>PyTorch_《Tutorials》</div>
      <div>http://example.com/2024/05/29/1_Pytorch_1/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Xi Wang</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>May 29, 2024</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2024/06/01/4_Linear-Algebra/" title="Linear Algebra">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Linear Algebra</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2024/05/22/3_Userful-tools/" title="Userful tools">
                        <span class="hidden-mobile">Userful tools</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>Table of Contents</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/tororo.model.json"},"display":{"position":"right","width":200,"height":400,"hOffset":20,"vOffset":0},"mobile":{"show":false},"react":{"opacityDefault":1,"opacityOnHover":1},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body>
</html>
